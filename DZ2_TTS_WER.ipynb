{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ WER –¥–ª—è TTS –º–æ–¥–µ–ª–∏\n",
        "\n",
        "**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** WER (Word Error Rate) –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ ASR –º–æ–¥–µ–ª–µ–π, –Ω–æ –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é –∑–∞–¥–∞–Ω–∏—è –º—ã –≤—ã—á–∏—Å–ª—è–µ–º WER –¥–ª—è TTS –º–æ–¥–µ–ª–∏ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
        "1. –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º –∞—É–¥–∏–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ TTS –º–æ–¥–µ–ª—å\n",
        "2. –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ ASR –º–æ–¥–µ–ª—å (Whisper)\n",
        "3. –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º —á–µ—Ä–µ–∑ WER\n",
        "\n",
        "–≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ –∫–æ—Å–≤–µ–Ω–Ω–æ - –µ—Å–ª–∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç—Å—è —Å –Ω–∏–∑–∫–∏–º WER, –∑–Ω–∞—á–∏—Ç —Å–∏–Ω—Ç–µ–∑ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import wave\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"‚úì –ë–∞–∑–æ–≤—ã–µ –∏–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Whisper –¥–ª—è ASR (–µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)\n",
        "print(\"=\"*60)\n",
        "print(\"–£–°–¢–ê–ù–û–í–ö–ê –ò –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
        "except ImportError:\n",
        "    print(\"–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Whisper...\")\n",
        "    %pip install openai-whisper -q\n",
        "    import whisper\n",
        "    print(\"‚úì Whisper —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
        "\n",
        "try:\n",
        "    from piper import PiperVoice\n",
        "    print(\"‚úì Piper —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
        "except ImportError:\n",
        "    print(\"–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Piper...\")\n",
        "    %pip install piper-tts -q\n",
        "    from piper import PiperVoice\n",
        "    print(\"‚úì Piper —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
        "\n",
        "try:\n",
        "    from jiwer import wer\n",
        "    print(\"‚úì jiwer —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
        "except ImportError:\n",
        "    print(\"–£—Å—Ç–∞–Ω–æ–≤–∫–∞ jiwer...\")\n",
        "    %pip install jiwer -q\n",
        "    from jiwer import wer\n",
        "    print(\"‚úì jiwer —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
        "\n",
        "print(\"\\n‚úì –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≥–æ—Ç–æ–≤—ã\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ü–æ–∏—Å–∫ TTS –º–æ–¥–µ–ª–∏ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"–ü–û–ò–°–ö TTS –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é (–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —ç—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫)\n",
        "# –í Jupyter –Ω–æ—É—Ç–±—É–∫–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é —Ä–∞–±–æ—á—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é\n",
        "NOTEBOOK_DIR = os.getcwd()\n",
        "\n",
        "print(f\"–ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {NOTEBOOK_DIR}\")\n",
        "\n",
        "# –í–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Å—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "# –ü–†–ò–û–†–ò–¢–ï–¢: —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ tts_data (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å), –ø–æ—Ç–æ–º –≤ examples (–ø—Ä–∏–º–µ—Ä—ã –æ—Ç –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è)\n",
        "possible_model_paths = [\n",
        "    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ tts_data (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å)\n",
        "    os.path.join(NOTEBOOK_DIR, \"tts_data\", \"final_model\", \"final_model.onnx\"),\n",
        "    os.path.join(NOTEBOOK_DIR, \"tts_data\", \"**\", \"*.onnx\"),\n",
        "    # –ü–æ—Ç–æ–º –≤ examples (–ø—Ä–∏–º–µ—Ä—ã –æ—Ç –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è)\n",
        "    os.path.join(NOTEBOOK_DIR, \"examples\", \"nataha_ruslan.onnx\"),\n",
        "    os.path.join(NOTEBOOK_DIR, \"examples\", \"*.onnx\"),\n",
        "]\n",
        "\n",
        "# –ò—â–µ–º ONNX –º–æ–¥–µ–ª—å\n",
        "onnx_model_path = None\n",
        "onnx_config_path = None\n",
        "\n",
        "for path_pattern in possible_model_paths:\n",
        "    if '*' in path_pattern:\n",
        "        # –ì–ª–æ–± –ø–∞—Ç—Ç–µ—Ä–Ω\n",
        "        matches = glob.glob(path_pattern, recursive=True)\n",
        "        if matches:\n",
        "            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π\n",
        "            onnx_model_path = matches[0]\n",
        "            print(f\"‚úì –ù–∞–π–¥–µ–Ω–∞ –º–æ–¥–µ–ª—å (glob): {onnx_model_path}\")\n",
        "            break\n",
        "    else:\n",
        "        # –ü—Ä—è–º–æ–π –ø—É—Ç—å\n",
        "        if os.path.exists(path_pattern):\n",
        "            onnx_model_path = path_pattern\n",
        "            print(f\"‚úì –ù–∞–π–¥–µ–Ω–∞ –º–æ–¥–µ–ª—å: {onnx_model_path}\")\n",
        "            break\n",
        "\n",
        "if not onnx_model_path:\n",
        "    print(\"‚ö†Ô∏è  ONNX –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\")\n",
        "    print(\"\\n–ü—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ –ø—É—Ç–∏:\")\n",
        "    for path in possible_model_paths:\n",
        "        exists = \"‚úì\" if os.path.exists(path) else \"‚úó\"\n",
        "        print(f\"  {exists} {path}\")\n",
        "    print(\"\\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –≤—Ä—É—á–Ω—É—é –≤ —Å–ª–µ–¥—É—é—â–µ–π —è—á–µ–π–∫–µ\")\n",
        "else:\n",
        "    # –ò—â–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª\n",
        "    # –ü–†–ò–û–†–ò–¢–ï–¢: —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Ä—è–¥–æ–º —Å –º–æ–¥–µ–ª—å—é, –ø–æ—Ç–æ–º –≤ tts_data, –ø–æ—Ç–æ–º –≤ examples\n",
        "    config_candidates = [\n",
        "        onnx_model_path + \".json\",  # –†—è–¥–æ–º —Å –º–æ–¥–µ–ª—å—é\n",
        "        os.path.join(os.path.dirname(onnx_model_path), \"nata_config.json\"),\n",
        "        os.path.join(NOTEBOOK_DIR, \"tts_data\", \"nata_config.json\"),  # –í tts_data\n",
        "        os.path.join(NOTEBOOK_DIR, \"examples\", \"nata_config.json\"),  # –í examples (–ø—Ä–∏–º–µ—Ä—ã)\n",
        "    ]\n",
        "    \n",
        "    for config_path in config_candidates:\n",
        "        if os.path.exists(config_path):\n",
        "            onnx_config_path = config_path\n",
        "            print(f\"‚úì –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ñ–∏–≥: {onnx_config_path}\")\n",
        "            break\n",
        "    \n",
        "    if not onnx_config_path:\n",
        "        print(\"‚ö†Ô∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è –±–µ–∑ –Ω–µ–≥–æ\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"–ó–ê–ì–†–£–ó–ö–ê TTS –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if not onnx_model_path:\n",
        "    print(\"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –≤—Ä—É—á–Ω—É—é:\")\n",
        "    print(\"   onnx_model_path = '/path/to/model.onnx'\")\n",
        "    print(\"   onnx_config_path = '/path/to/model.onnx.json'  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ\")\n",
        "    voice = None\n",
        "else:\n",
        "    try:\n",
        "        print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {onnx_model_path}\")\n",
        "        if onnx_config_path:\n",
        "            print(f\"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥–∞: {onnx_config_path}\")\n",
        "            voice = PiperVoice.load(onnx_model_path, config_path=onnx_config_path, use_cuda=True)\n",
        "        else:\n",
        "            print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –±–µ–∑ –∫–æ–Ω—Ñ–∏–≥–∞...\")\n",
        "            voice = PiperVoice.load(onnx_model_path, use_cuda=True)\n",
        "        print(\"‚úì TTS –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        voice = None\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ASR –º–æ–¥–µ–ª–∏ (Whisper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"–ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ASR –ú–û–î–ï–õ–ò (WHISPER)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è...\")\n",
        "try:\n",
        "    asr_model = whisper.load_model(\"base\")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º base –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞\n",
        "    print(\"‚úì –ú–æ–¥–µ–ª—å Whisper –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Whisper: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    asr_model = None\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∞—É–¥–∏–æ\n",
        "def transcribe_audio_with_whisper(audio_path: str) -> str:\n",
        "    \"\"\"–†–∞—Å–ø–æ–∑–Ω–∞–µ—Ç –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ Whisper.\"\"\"\n",
        "    if asr_model is None:\n",
        "        return \"\"\n",
        "    \n",
        "    try:\n",
        "        result = asr_model.transcribe(audio_path, language=\"ru\")\n",
        "        return result[\"text\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ {audio_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"\\n‚úì ASR –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é\")\n",
        "print(f\"\\n{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"–ü–û–î–ì–û–¢–û–í–ö–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è WER\n",
        "def calculate_wer(true_text: str, predicted_text: str) -> float:\n",
        "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç Word Error Rate –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏.\"\"\"\n",
        "    try:\n",
        "        from jiwer import wer\n",
        "        return wer(true_text, predicted_text)\n",
        "    except:\n",
        "        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è, –µ—Å–ª–∏ jiwer –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç\n",
        "        true_words = true_text.lower().split()\n",
        "        pred_words = predicted_text.lower().split()\n",
        "        \n",
        "        if len(true_words) == 0:\n",
        "            return 1.0 if len(pred_words) > 0 else 0.0\n",
        "        \n",
        "        errors = sum(1 for t, p in zip(true_words, pred_words) if t != p)\n",
        "        errors += abs(len(true_words) - len(pred_words))\n",
        "        return errors / len(true_words)\n",
        "\n",
        "# –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞\n",
        "# –ú–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–æ–∏\n",
        "test_texts = [\n",
        "    \"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\",\n",
        "    \"–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏ —Å–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è –æ–±–ª–∞—Å—Ç—å.\",\n",
        "    \"–°–µ–≥–æ–¥–Ω—è —Ö–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞.\",\n",
        "    \"–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ–º–æ–≥–∞–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —É–º–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã.\",\n",
        "    \"–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.\",\n",
        "    \"–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ.\",\n",
        "    \"–ì–æ–ª–æ—Å–æ–≤—ã–µ –ø–æ–º–æ—â–Ω–∏–∫–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤—Å–µ –±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏.\",\n",
        "    \"–°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –≥–æ–≤–æ—Ä–∏—Ç—å –∫–∞–∫ –ª—é–¥–∏.\",\n",
        "    \"–ö–∞—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —É–ª—É—á—à–∞–µ—Ç—Å—è.\",\n",
        "    \"–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—á–µ–Ω—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—É—é —Ä–µ—á—å.\",\n",
        "]\n",
        "\n",
        "# –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV —Ñ–∞–π–ª–æ–≤\n",
        "possible_csv_paths = [\n",
        "    os.path.join(NOTEBOOK_DIR, \"tts_data\", \"valid.csv\"),\n",
        "    os.path.join(NOTEBOOK_DIR, \"tts_data\", \"test.csv\"),\n",
        "    os.path.join(NOTEBOOK_DIR, \"**\", \"valid.csv\"),\n",
        "    os.path.join(NOTEBOOK_DIR, \"**\", \"test.csv\"),\n",
        "]\n",
        "\n",
        "csv_data = []\n",
        "csv_path = None\n",
        "\n",
        "for path_pattern in possible_csv_paths:\n",
        "    if '*' in path_pattern:\n",
        "        matches = glob.glob(path_pattern, recursive=True)\n",
        "        if matches:\n",
        "            csv_path = matches[0]\n",
        "            break\n",
        "    else:\n",
        "        if os.path.exists(path_pattern):\n",
        "            csv_path = path_pattern\n",
        "            break\n",
        "\n",
        "if csv_path:\n",
        "    print(f\"‚úì –ù–∞–π–¥–µ–Ω CSV —Ñ–∞–π–ª: {csv_path}\")\n",
        "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV...\")\n",
        "    try:\n",
        "        with open(csv_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if '|' in line:\n",
        "                    # –§–æ—Ä–º–∞—Ç: audio_file|text\n",
        "                    parts = line.split('|', 1)\n",
        "                    if len(parts) == 2:\n",
        "                        csv_data.append(parts[1])  # –¢–µ–∫—Å—Ç\n",
        "        print(f\"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(csv_data)} —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ CSV\")\n",
        "        if len(csv_data) > 0:\n",
        "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 50 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "            test_texts = csv_data[:50]\n",
        "            print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(test_texts)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ CSV\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV: {e}\")\n",
        "        print(\"–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã\")\n",
        "    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {len(test_texts)}\")\n",
        "\n",
        "print(f\"\\n–ò—Ç–æ–≥–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(test_texts)}\")\n",
        "print(f\"\\n{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"–í–´–ß–ò–°–õ–ï–ù–ò–ï WER –î–õ–Ø TTS –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
        "if not voice:\n",
        "    print(\"‚ö†Ô∏è  TTS –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å WER.\")\n",
        "    print(\"   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —è—á–µ–π–∫–∞—Ö\")\n",
        "elif asr_model is None:\n",
        "    print(\"‚ö†Ô∏è  ASR –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å WER.\")\n",
        "    print(\"   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Whisper –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —è—á–µ–π–∫–∞—Ö\")\n",
        "else:\n",
        "    print(\"\\n‚úì –û–±–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
        "    print(f\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(test_texts)} –ø—Ä–∏–º–µ—Ä–æ–≤...\")\n",
        "    \n",
        "    wers = []\n",
        "    original_texts = []\n",
        "    recognized_texts = []\n",
        "    processed_count = 0\n",
        "    errors_count = 0\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞—É–¥–∏–æ\n",
        "    temp_dir = os.path.join(NOTEBOOK_DIR, \"temp_wer_audio\")\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "    \n",
        "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã\n",
        "    for i, original_text in enumerate(tqdm(test_texts, desc=\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ WER\", unit=\"–ø—Ä–∏–º–µ—Ä\")):\n",
        "        try:\n",
        "            if not original_text or not original_text.strip():\n",
        "                errors_count += 1\n",
        "                continue\n",
        "            \n",
        "            # –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º –∞—É–¥–∏–æ\n",
        "            temp_wav = os.path.join(temp_dir, f\"synth_{i:03d}.wav\")\n",
        "            try:\n",
        "                with wave.open(temp_wav, \"wb\") as wav_file:\n",
        "                    voice.synthesize_wav(original_text, wav_file)\n",
        "                \n",
        "                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω –∏ –Ω–µ –ø—É—Å—Ç–æ–π\n",
        "                if not os.path.exists(temp_wav) or os.path.getsize(temp_wav) == 0:\n",
        "                    errors_count += 1\n",
        "                    if errors_count <= 3:\n",
        "                        print(f\"‚ö†Ô∏è  –ü—Ä–∏–º–µ—Ä {i}: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏–æ\")\n",
        "                    continue\n",
        "            except Exception as synth_error:\n",
        "                errors_count += 1\n",
        "                if errors_count <= 3:\n",
        "                    print(f\"‚ö†Ô∏è  –ü—Ä–∏–º–µ—Ä {i}: –æ—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞: {synth_error}\")\n",
        "                continue\n",
        "            \n",
        "            # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ\n",
        "            recognized_text = transcribe_audio_with_whisper(temp_wav)\n",
        "            \n",
        "            if not recognized_text:\n",
        "                errors_count += 1\n",
        "                if errors_count <= 3:\n",
        "                    print(f\"‚ö†Ô∏è  –ü—Ä–∏–º–µ—Ä {i}: –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –∞—É–¥–∏–æ\")\n",
        "                continue\n",
        "            \n",
        "            # –í—ã—á–∏—Å–ª—è–µ–º WER\n",
        "            wer_value = calculate_wer(original_text, recognized_text)\n",
        "            wers.append(wer_value)\n",
        "            original_texts.append(original_text)\n",
        "            recognized_texts.append(recognized_text)\n",
        "            processed_count += 1\n",
        "            \n",
        "            # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "            if processed_count <= 5:\n",
        "                print(f\"\\n–ü—Ä–∏–º–µ—Ä {processed_count}:\")\n",
        "                print(f\"  –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: {original_text[:80]}...\")\n",
        "                print(f\"  –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {recognized_text[:80]}...\")\n",
        "                print(f\"  WER: {wer_value:.4f}\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            errors_count += 1\n",
        "            if errors_count <= 3:\n",
        "                import traceback\n",
        "                print(f\"‚ö†Ô∏è  –ü—Ä–∏–º–µ—Ä {i}: –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}\")\n",
        "                traceback.print_exc()\n",
        "    \n",
        "    # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã\n",
        "    if os.path.exists(temp_dir):\n",
        "        try:\n",
        "            shutil.rmtree(temp_dir)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–´–ß–ò–°–õ–ï–ù–ò–Ø WER –î–õ–Ø TTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:\")\n",
        "    print(f\"  –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(test_texts)}\")\n",
        "    print(f\"  –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}\")\n",
        "    print(f\"  –û—à–∏–±–æ–∫: {errors_count}\")\n",
        "    \n",
        "    if wers:\n",
        "        print(f\"\\nWER (Word Error Rate) - —á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ:\")\n",
        "        print(f\"  –í—ã—á–∏—Å–ª–µ–Ω–æ –º–µ—Ç—Ä–∏–∫: {len(wers)}\")\n",
        "        print(f\"  –°—Ä–µ–¥–Ω–∏–π WER: {np.mean(wers):.4f}\")\n",
        "        print(f\"  –ú–µ–¥–∏–∞–Ω–Ω—ã–π WER: {np.median(wers):.4f}\")\n",
        "        print(f\"  –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π WER: {np.min(wers):.4f}\")\n",
        "        print(f\"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π WER: {np.max(wers):.4f}\")\n",
        "        print(f\"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(wers):.4f}\")\n",
        "        \n",
        "        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "        print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ WER:\")\n",
        "        print(f\"  WER < 0.1 (–æ—Ç–ª–∏—á–Ω–æ): {sum(1 for w in wers if w < 0.1)} ({100*sum(1 for w in wers if w < 0.1)/len(wers):.1f}%)\")\n",
        "        print(f\"  WER < 0.2 (—Ö–æ—Ä–æ—à–æ): {sum(1 for w in wers if w < 0.2)} ({100*sum(1 for w in wers if w < 0.2)/len(wers):.1f}%)\")\n",
        "        print(f\"  WER < 0.3 (—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ): {sum(1 for w in wers if w < 0.3)} ({100*sum(1 for w in wers if w < 0.3)/len(wers):.1f}%)\")\n",
        "        print(f\"  WER >= 0.3 (—Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è): {sum(1 for w in wers if w >= 0.3)} ({100*sum(1 for w in wers if w >= 0.3)/len(wers):.1f}%)\")\n",
        "        \n",
        "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Å –ª—É—á—à–∏–º –∏ —Ö—É–¥—à–∏–º WER\n",
        "        if len(wers) > 0:\n",
        "            best_idx = np.argmin(wers)\n",
        "            worst_idx = np.argmax(wers)\n",
        "            \n",
        "            print(f\"\\n–õ—É—á—à–∏–π –ø—Ä–∏–º–µ—Ä (WER = {wers[best_idx]:.4f}):\")\n",
        "            print(f\"  –û—Ä–∏–≥–∏–Ω–∞–ª: {original_texts[best_idx][:100]}...\")\n",
        "            print(f\"  –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {recognized_texts[best_idx][:100]}...\")\n",
        "            \n",
        "            print(f\"\\n–•—É–¥—à–∏–π –ø—Ä–∏–º–µ—Ä (WER = {wers[worst_idx]:.4f}):\")\n",
        "            print(f\"  –û—Ä–∏–≥–∏–Ω–∞–ª: {original_texts[worst_idx][:100]}...\")\n",
        "            print(f\"  –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {recognized_texts[worst_idx][:100]}...\")\n",
        "        \n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª\n",
        "        results_file = os.path.join(NOTEBOOK_DIR, \"wer_results.txt\")\n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–´–ß–ò–°–õ–ï–ù–ò–Ø WER –î–õ–Ø TTS –ú–û–î–ï–õ–ò\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\\n\")\n",
        "            f.write(f\"–ú–æ–¥–µ–ª—å: {onnx_model_path}\\n\")\n",
        "            f.write(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {processed_count}\\n\")\n",
        "            f.write(f\"–°—Ä–µ–¥–Ω–∏–π WER: {np.mean(wers):.4f}\\n\")\n",
        "            f.write(f\"–ú–µ–¥–∏–∞–Ω–Ω—ã–π WER: {np.median(wers):.4f}\\n\")\n",
        "            f.write(f\"–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π WER: {np.min(wers):.4f}\\n\")\n",
        "            f.write(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π WER: {np.max(wers):.4f}\\n\")\n",
        "            f.write(f\"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(wers):.4f}\\n\\n\")\n",
        "            f.write(\"–î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\\n\")\n",
        "            f.write(\"-\"*60 + \"\\n\")\n",
        "            for i, (orig, rec, wer_val) in enumerate(zip(original_texts, recognized_texts, wers)):\n",
        "                f.write(f\"\\n–ü—Ä–∏–º–µ—Ä {i+1} (WER = {wer_val:.4f}):\\n\")\n",
        "                f.write(f\"  –û—Ä–∏–≥–∏–Ω–∞–ª: {orig}\\n\")\n",
        "                f.write(f\"  –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {rec}\\n\")\n",
        "        \n",
        "        print(f\"\\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_file}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å WER –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞.\")\n",
        "        print(f\"   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\")\n",
        "        print(f\"   - –û—à–∏–±–∫–∏ –ø—Ä–∏ —Å–∏–Ω—Ç–µ–∑–µ –∞—É–¥–∏–æ\")\n",
        "        print(f\"   - –û—à–∏–±–∫–∏ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –∞—É–¥–∏–æ\")\n",
        "        print(f\"   - –ü—Ä–æ–±–ª–µ–º—ã —Å ASR –º–æ–¥–µ–ª—å—é (Whisper)\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é –∏–∑ examples\n",
        "\n",
        "–î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å—Ä–∞–≤–Ω–∏–º –µ—ë —Å –∏—Å—Ö–æ–¥–Ω–æ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é –∏–∑ examples (–ø—Ä–∏–º–µ—Ä—ã –æ—Ç –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"–ó–ê–ì–†–£–ó–ö–ê –ë–ê–ó–û–í–û–ô –ú–û–î–ï–õ–ò –ò–ó EXAMPLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# –ò—â–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ examples\n",
        "base_model_path = os.path.join(NOTEBOOK_DIR, \"examples\", \"nataha_ruslan.onnx\")\n",
        "base_config_path = os.path.join(NOTEBOOK_DIR, \"examples\", \"nataha_ruslan.onnx.json\")\n",
        "\n",
        "# –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—â–µ–º –ª—é–±—É—é ONNX –º–æ–¥–µ–ª—å –≤ examples\n",
        "if not os.path.exists(base_model_path):\n",
        "    examples_onnx = glob.glob(os.path.join(NOTEBOOK_DIR, \"examples\", \"*.onnx\"))\n",
        "    if examples_onnx:\n",
        "        base_model_path = examples_onnx[0]\n",
        "        base_config_path = base_model_path + \".json\"\n",
        "        print(f\"‚úì –ù–∞–π–¥–µ–Ω–∞ –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_model_path}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ examples\")\n",
        "        base_model_path = None\n",
        "        base_config_path = None\n",
        "else:\n",
        "    print(f\"‚úì –ù–∞–π–¥–µ–Ω–∞ –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_model_path}\")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å\n",
        "base_voice = None\n",
        "if base_model_path:\n",
        "    try:\n",
        "        if os.path.exists(base_config_path):\n",
        "            print(f\"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥–∞: {base_config_path}\")\n",
        "            base_voice = PiperVoice.load(base_model_path, config_path=base_config_path, use_cuda=True)\n",
        "        else:\n",
        "            print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –±–µ–∑ –∫–æ–Ω—Ñ–∏–≥–∞...\")\n",
        "            base_voice = PiperVoice.load(base_model_path, use_cuda=True)\n",
        "        print(\"‚úì –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        base_voice = None\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"–í–´–ß–ò–°–õ–ï–ù–ò–ï WER –î–õ–Ø –ë–ê–ó–û–í–û–ô –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –µ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "if not base_voice:\n",
        "    print(\"‚ö†Ô∏è  –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ.\")\n",
        "    base_wers = []\n",
        "    base_original_texts = []\n",
        "    base_recognized_texts = []\n",
        "elif 'test_texts' not in globals() or len(test_texts) == 0:\n",
        "    print(\"‚ö†Ô∏è  –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ.\")\n",
        "    base_wers = []\n",
        "    base_original_texts = []\n",
        "    base_recognized_texts = []\n",
        "else:\n",
        "    print(f\"\\n‚úì –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "    print(f\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(test_texts)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...\")\n",
        "    \n",
        "    base_wers = []\n",
        "    base_original_texts = []\n",
        "    base_recognized_texts = []\n",
        "    base_processed_count = 0\n",
        "    base_errors_count = 0\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞—É–¥–∏–æ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
        "    temp_dir_base = os.path.join(NOTEBOOK_DIR, \"temp_wer_audio_base\")\n",
        "    os.makedirs(temp_dir_base, exist_ok=True)\n",
        "    \n",
        "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
        "    for i, original_text in enumerate(tqdm(test_texts, desc=\"WER –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å\", unit=\"–ø—Ä–∏–º–µ—Ä\")):\n",
        "        try:\n",
        "            if not original_text or not original_text.strip():\n",
        "                base_errors_count += 1\n",
        "                continue\n",
        "            \n",
        "            # –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º –∞—É–¥–∏–æ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é\n",
        "            temp_wav = os.path.join(temp_dir_base, f\"synth_base_{i:03d}.wav\")\n",
        "            try:\n",
        "                with wave.open(temp_wav, \"wb\") as wav_file:\n",
        "                    base_voice.synthesize_wav(original_text, wav_file)\n",
        "                \n",
        "                if not os.path.exists(temp_wav) or os.path.getsize(temp_wav) == 0:\n",
        "                    base_errors_count += 1\n",
        "                    continue\n",
        "            except Exception as synth_error:\n",
        "                base_errors_count += 1\n",
        "                continue\n",
        "            \n",
        "            # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ\n",
        "            recognized_text = transcribe_audio_with_whisper(temp_wav)\n",
        "            \n",
        "            if not recognized_text:\n",
        "                base_errors_count += 1\n",
        "                continue\n",
        "            \n",
        "            # –í—ã—á–∏—Å–ª—è–µ–º WER\n",
        "            wer_value = calculate_wer(original_text, recognized_text)\n",
        "            base_wers.append(wer_value)\n",
        "            base_original_texts.append(original_text)\n",
        "            base_recognized_texts.append(recognized_text)\n",
        "            base_processed_count += 1\n",
        "        \n",
        "        except Exception as e:\n",
        "            base_errors_count += 1\n",
        "    \n",
        "    # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã\n",
        "    if os.path.exists(temp_dir_base):\n",
        "        try:\n",
        "            shutil.rmtree(temp_dir_base)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ WER –î–õ–Ø –ë–ê–ó–û–í–û–ô –ú–û–î–ï–õ–ò\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:\")\n",
        "    print(f\"  –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(test_texts)}\")\n",
        "    print(f\"  –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {base_processed_count}\")\n",
        "    print(f\"  –û—à–∏–±–æ–∫: {base_errors_count}\")\n",
        "    \n",
        "    if base_wers:\n",
        "        print(f\"\\nWER –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏:\")\n",
        "        print(f\"  –í—ã—á–∏—Å–ª–µ–Ω–æ –º–µ—Ç—Ä–∏–∫: {len(base_wers)}\")\n",
        "        print(f\"  –°—Ä–µ–¥–Ω–∏–π WER: {np.mean(base_wers):.4f}\")\n",
        "        print(f\"  –ú–µ–¥–∏–∞–Ω–Ω—ã–π WER: {np.median(base_wers):.4f}\")\n",
        "        print(f\"  –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π WER: {np.min(base_wers):.4f}\")\n",
        "        print(f\"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π WER: {np.max(base_wers):.4f}\")\n",
        "        print(f\"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(base_wers):.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"–°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π\n",
        "if 'wers' in globals() and len(wers) > 0 and len(base_wers) > 0:\n",
        "    print(\"\\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é:\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "    min_len = min(len(wers), len(base_wers))\n",
        "    if min_len > 0:\n",
        "        trained_wers = wers[:min_len]\n",
        "        base_wers_trimmed = base_wers[:min_len]\n",
        "        \n",
        "        trained_mean = np.mean(trained_wers)\n",
        "        base_mean = np.mean(base_wers_trimmed)\n",
        "        difference = trained_mean - base_mean\n",
        "        improvement_pct = (base_mean - trained_mean) / base_mean * 100 if base_mean > 0 else 0\n",
        "        \n",
        "        print(f\"–°—Ä–µ–¥–Ω–∏–π WER:\")\n",
        "        print(f\"  –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {trained_mean:.4f}\")\n",
        "        print(f\"  –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å:   {base_mean:.4f}\")\n",
        "        print(f\"  –†–∞–∑–Ω–∏—Ü–∞:          {difference:+.4f}\")\n",
        "        \n",
        "        if difference < 0:\n",
        "            print(f\"  ‚úì –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –õ–£–ß–®–ï –Ω–∞ {abs(improvement_pct):.2f}%\")\n",
        "        elif difference > 0:\n",
        "            print(f\"  ‚úó –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –•–£–ñ–ï –Ω–∞ {abs(improvement_pct):.2f}%\")\n",
        "        else:\n",
        "            print(f\"  = –ú–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\")\n",
        "        \n",
        "        print(f\"\\n–ú–µ–¥–∏–∞–Ω–Ω—ã–π WER:\")\n",
        "        trained_median = np.median(trained_wers)\n",
        "        base_median = np.median(base_wers_trimmed)\n",
        "        print(f\"  –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {trained_median:.4f}\")\n",
        "        print(f\"  –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å:   {base_median:.4f}\")\n",
        "        print(f\"  –†–∞–∑–Ω–∏—Ü–∞:          {trained_median - base_median:+.4f}\")\n",
        "        \n",
        "        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –ø—Ä–∏–º–µ—Ä–∞–º\n",
        "        print(f\"\\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –ø—Ä–∏–º–µ—Ä–∞–º (–∏–∑ {min_len} –æ–±—â–∏—Ö):\")\n",
        "        better_count = sum(1 for t, b in zip(trained_wers, base_wers_trimmed) if t < b)\n",
        "        worse_count = sum(1 for t, b in zip(trained_wers, base_wers_trimmed) if t > b)\n",
        "        equal_count = sum(1 for t, b in zip(trained_wers, base_wers_trimmed) if t == b)\n",
        "        \n",
        "        print(f\"  –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–µ: {better_count} ({100*better_count/min_len:.1f}%)\")\n",
        "        print(f\"  –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Ö—É–∂–µ:  {worse_count} ({100*worse_count/min_len:.1f}%)\")\n",
        "        print(f\"  –û–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:   {equal_count} ({100*equal_count/min_len:.1f}%)\")\n",
        "        \n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "        comparison_file = os.path.join(NOTEBOOK_DIR, \"wer_comparison.txt\")\n",
        "        with open(comparison_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"–°–†–ê–í–ù–ï–ù–ò–ï –û–ë–£–ß–ï–ù–ù–û–ô –ò –ë–ê–ó–û–í–û–ô –ú–û–î–ï–õ–ï–ô\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\\n\")\n",
        "            f.write(f\"–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {onnx_model_path}\\n\")\n",
        "            f.write(f\"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_model_path}\\n\\n\")\n",
        "            f.write(f\"–°—Ä–µ–¥–Ω–∏–π WER:\\n\")\n",
        "            f.write(f\"  –û–±—É—á–µ–Ω–Ω–∞—è: {trained_mean:.4f}\\n\")\n",
        "            f.write(f\"  –ë–∞–∑–æ–≤–∞—è:   {base_mean:.4f}\\n\")\n",
        "            f.write(f\"  –†–∞–∑–Ω–∏—Ü–∞:   {difference:+.4f} ({improvement_pct:+.2f}%)\\n\\n\")\n",
        "            f.write(f\"–ú–µ–¥–∏–∞–Ω–Ω—ã–π WER:\\n\")\n",
        "            f.write(f\"  –û–±—É—á–µ–Ω–Ω–∞—è: {trained_median:.4f}\\n\")\n",
        "            f.write(f\"  –ë–∞–∑–æ–≤–∞—è:   {base_median:.4f}\\n\")\n",
        "            f.write(f\"  –†–∞–∑–Ω–∏—Ü–∞:   {trained_median - base_median:+.4f}\\n\\n\")\n",
        "            f.write(f\"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –ø—Ä–∏–º–µ—Ä–∞–º:\\n\")\n",
        "            f.write(f\"  –û–±—É—á–µ–Ω–Ω–∞—è –ª—É—á—à–µ: {better_count} ({100*better_count/min_len:.1f}%)\\n\")\n",
        "            f.write(f\"  –û–±—É—á–µ–Ω–Ω–∞—è —Ö—É–∂–µ:  {worse_count} ({100*worse_count/min_len:.1f}%)\\n\")\n",
        "            f.write(f\"  –û–¥–∏–Ω–∞–∫–æ–≤–æ:       {equal_count} ({100*equal_count/min_len:.1f}%)\\n\\n\")\n",
        "            f.write(\"–î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –ø—Ä–∏–º–µ—Ä–∞–º:\\n\")\n",
        "            f.write(\"-\"*60 + \"\\n\")\n",
        "            for i, (orig, trained_wer, base_wer) in enumerate(zip(original_texts[:min_len], trained_wers, base_wers_trimmed)):\n",
        "                diff = trained_wer - base_wer\n",
        "                status = \"–ª—É—á—à–µ\" if diff < 0 else \"—Ö—É–∂–µ\" if diff > 0 else \"—Ä–∞–≤–Ω–æ\"\n",
        "                f.write(f\"\\n–ü—Ä–∏–º–µ—Ä {i+1} (–æ–±—É—á–µ–Ω–Ω–∞—è {status} –Ω–∞ {abs(diff):.4f}):\\n\")\n",
        "                f.write(f\"  –¢–µ–∫—Å—Ç: {orig}\\n\")\n",
        "                f.write(f\"  WER –æ–±—É—á–µ–Ω–Ω–æ–π: {trained_wer:.4f}\\n\")\n",
        "                f.write(f\"  WER –±–∞–∑–æ–≤–æ–π:   {base_wer:.4f}\\n\")\n",
        "        \n",
        "        print(f\"\\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {comparison_file}\")\n",
        "        \n",
        "elif 'wers' not in globals() or len(wers) == 0:\n",
        "    print(\"‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç—ã WER –¥–ª—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\")\n",
        "    print(\"   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫—É '–í—ã—á–∏—Å–ª–µ–Ω–∏–µ WER'\")\n",
        "elif len(base_wers) == 0:\n",
        "    print(\"‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç—ã WER –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\")\n",
        "    print(\"   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫—É '–í—ã—á–∏—Å–ª–µ–Ω–∏–µ WER –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏'\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"–ü–†–û–°–õ–£–®–ò–í–ê–ù–ò–ï –ü–†–ò–ú–ï–†–û–í\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "if 'wers' in globals() and len(wers) > 0 and voice:\n",
        "    from IPython.display import Audio, display\n",
        "    \n",
        "    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π –∏ —Ö—É–¥—à–∏–π –ø—Ä–∏–º–µ—Ä—ã\n",
        "    best_idx = np.argmin(wers)\n",
        "    worst_idx = np.argmax(wers)\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∞—É–¥–∏–æ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "    examples_audio_dir = os.path.join(NOTEBOOK_DIR, \"wer_examples_audio\")\n",
        "    os.makedirs(examples_audio_dir, exist_ok=True)\n",
        "    \n",
        "    print(f\"\\n–°–∏–Ω—Ç–µ–∑ –∞—É–¥–∏–æ –¥–ª—è –ø—Ä–∏–º–µ—Ä–æ–≤...\")\n",
        "    \n",
        "    # –õ—É—á—à–∏–π –ø—Ä–∏–º–µ—Ä\n",
        "    best_text = original_texts[best_idx]\n",
        "    best_audio_path = os.path.join(examples_audio_dir, \"best_example.wav\")\n",
        "    \n",
        "    try:\n",
        "        with wave.open(best_audio_path, \"wb\") as wav_file:\n",
        "            voice.synthesize_wav(best_text, wav_file)\n",
        "        print(f\"‚úì –õ—É—á—à–∏–π –ø—Ä–∏–º–µ—Ä —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω: {best_audio_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∏–Ω—Ç–µ–∑–µ –ª—É—á—à–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞: {e}\")\n",
        "        best_audio_path = None\n",
        "    \n",
        "    # –•—É–¥—à–∏–π –ø—Ä–∏–º–µ—Ä\n",
        "    worst_text = original_texts[worst_idx]\n",
        "    worst_audio_path = os.path.join(examples_audio_dir, \"worst_example.wav\")\n",
        "    \n",
        "    try:\n",
        "        with wave.open(worst_audio_path, \"wb\") as wav_file:\n",
        "            voice.synthesize_wav(worst_text, wav_file)\n",
        "        print(f\"‚úì –•—É–¥—à–∏–π –ø—Ä–∏–º–µ—Ä —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω: {worst_audio_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∏–Ω—Ç–µ–∑–µ —Ö—É–¥—à–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞: {e}\")\n",
        "        worst_audio_path = None\n",
        "    \n",
        "    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"–õ–£–ß–®–ò–ô –ü–†–ò–ú–ï–† (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π WER)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"WER: {wers[best_idx]:.4f}\")\n",
        "    print(f\"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: {best_text}\")\n",
        "    print(f\"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {recognized_texts[best_idx]}\")\n",
        "    \n",
        "    if best_audio_path and os.path.exists(best_audio_path):\n",
        "        print(f\"\\nüéµ –ü—Ä–æ—Å–ª—É—à–∞—Ç—å –ª—É—á—à–∏–π –ø—Ä–∏–º–µ—Ä:\")\n",
        "        display(Audio(filename=best_audio_path, autoplay=False))\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  –ê—É–¥–∏–æ —Ñ–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω\")\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"–•–£–î–®–ò–ô –ü–†–ò–ú–ï–† (–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π WER)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"WER: {wers[worst_idx]:.4f}\")\n",
        "    print(f\"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: {worst_text}\")\n",
        "    print(f\"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {recognized_texts[worst_idx]}\")\n",
        "    \n",
        "    if worst_audio_path and os.path.exists(worst_audio_path):\n",
        "        print(f\"\\nüéµ –ü—Ä–æ—Å–ª—É—à–∞—Ç—å —Ö—É–¥—à–∏–π –ø—Ä–∏–º–µ—Ä:\")\n",
        "        display(Audio(filename=worst_audio_path, autoplay=False))\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  –ê—É–¥–∏–æ —Ñ–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω\")\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"–ê—É–¥–∏–æ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {examples_audio_dir}\")\n",
        "    print(f\"  - –õ—É—á—à–∏–π –ø—Ä–∏–º–µ—Ä: {best_audio_path}\")\n",
        "    print(f\"  - –•—É–¥—à–∏–π –ø—Ä–∏–º–µ—Ä: {worst_audio_path}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "elif 'wers' not in globals() or len(wers) == 0:\n",
        "    print(\"‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç—ã WER –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\")\n",
        "    print(\"   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫—É '–í—ã—á–∏—Å–ª–µ–Ω–∏–µ WER'\")\n",
        "elif not voice:\n",
        "    print(\"‚ö†Ô∏è  TTS –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.\")\n",
        "    print(\"   –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —è—á–µ–π–∫–∞—Ö\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
