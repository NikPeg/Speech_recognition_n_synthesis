{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Домашнее задание №2: Синтез речи\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install piper-tts\n",
        "%pip install tensorboard\n",
        "%pip install onnx\n",
        "%pip install onnxruntime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Импорты\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Подготовка данных\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = \"/share/audio_data/sova/ytub/raid/nanosemantics/nextcloud/sova_done\"\n",
        "OUTPUT_DIR = \"./tts_data\"\n",
        "TRAIN_DIR = os.path.join(OUTPUT_DIR, \"train\")\n",
        "VAL_DIR = os.path.join(OUTPUT_DIR, \"val\")\n",
        "TEST_DIR = os.path.join(OUTPUT_DIR, \"test\")\n",
        "\n",
        "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
        "os.makedirs(VAL_DIR, exist_ok=True)\n",
        "os.makedirs(TEST_DIR, exist_ok=True)\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "MIN_DURATION = 0.5\n",
        "MAX_DURATION = 10.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^а-яёa-z0-9\\s]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "def load_audio_info(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=None)\n",
        "        duration = len(y) / sr\n",
        "        return duration, sr, y\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке {audio_path}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def prepare_tts_dataset(base_path, parts=['part_0', 'part_1', 'part_2'], max_files=None):\n",
        "    dataset = []\n",
        "    audio_extensions = ['*.wav', '*.mp3', '*.flac', '*.ogg']\n",
        "    audio_files = []\n",
        "    \n",
        "    for part in parts:\n",
        "        part_path = os.path.join(base_path, part)\n",
        "        if not os.path.exists(part_path):\n",
        "            print(f\"Предупреждение: часть {part} не найдена\")\n",
        "            continue\n",
        "        for ext in audio_extensions:\n",
        "            audio_files.extend(glob.glob(os.path.join(part_path, '**', ext), recursive=True))\n",
        "    \n",
        "    if max_files:\n",
        "        audio_files = audio_files[:max_files]\n",
        "    \n",
        "    print(f\"Найдено {len(audio_files)} аудиофайлов\")\n",
        "    print(\"Начинаю обработку...\")\n",
        "    \n",
        "    for audio_path in tqdm(audio_files, desc=\"Подготовка данных\", unit=\"файл\"):\n",
        "        duration, sr, y = load_audio_info(audio_path)\n",
        "        if duration is None:\n",
        "            continue\n",
        "        \n",
        "        if duration < MIN_DURATION or duration > MAX_DURATION:\n",
        "            continue\n",
        "        \n",
        "        text_path = audio_path.rsplit('.', 1)[0] + '.txt'\n",
        "        if not os.path.exists(text_path):\n",
        "            continue\n",
        "        \n",
        "        with open(text_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "        \n",
        "        text = normalize_text(text)\n",
        "        \n",
        "        if len(text) > 0:\n",
        "            resampled_audio = librosa.resample(y, orig_sr=sr, target_sr=SAMPLE_RATE) if sr != SAMPLE_RATE else y\n",
        "            \n",
        "            dataset.append({\n",
        "                \"audio\": resampled_audio,\n",
        "                \"text\": text,\n",
        "                \"duration\": duration,\n",
        "                \"path\": audio_path\n",
        "            })\n",
        "    \n",
        "    print(f\"Подготовлено {len(dataset)} записей\")\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = prepare_tts_dataset(DATA_PATH, parts=['part_0', 'part_1', 'part_2'])\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "\n",
        "train_data = dataset[:train_size]\n",
        "val_data = dataset[train_size:train_size + val_size]\n",
        "test_data = dataset[train_size + val_size:]\n",
        "\n",
        "print(f\"Разделение данных:\")\n",
        "print(f\"  Train: {len(train_data)} записей\")\n",
        "print(f\"  Val: {len(val_data)} записей\")\n",
        "print(f\"  Test: {len(test_data)} записей\")\n",
        "\n",
        "durations = [item['duration'] for item in dataset]\n",
        "print(f\"\\nСтатистика по длительности:\")\n",
        "print(f\"  Минимум: {min(durations):.2f} сек\")\n",
        "print(f\"  Максимум: {max(durations):.2f} сек\")\n",
        "print(f\"  Среднее: {np.mean(durations):.2f} сек\")\n",
        "print(f\"  Медиана: {np.median(durations):.2f} сек\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Сохранение подготовленных данных\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_dataset(data, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    manifest = []\n",
        "    \n",
        "    for i, item in enumerate(tqdm(data, desc=\"Сохранение\", unit=\"файл\")):\n",
        "        audio_filename = f\"audio_{i:06d}.wav\"\n",
        "        audio_path = os.path.join(output_dir, audio_filename)\n",
        "        \n",
        "        sf.write(audio_path, item['audio'], SAMPLE_RATE)\n",
        "        \n",
        "        manifest.append({\n",
        "            \"audio_filepath\": audio_path,\n",
        "            \"text\": item['text'],\n",
        "            \"duration\": item['duration']\n",
        "        })\n",
        "    \n",
        "    manifest_path = os.path.join(output_dir, \"manifest.json\")\n",
        "    with open(manifest_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    return manifest_path\n",
        "\n",
        "train_manifest = save_dataset(train_data, TRAIN_DIR)\n",
        "val_manifest = save_dataset(val_data, VAL_DIR)\n",
        "test_manifest = save_dataset(test_data, TEST_DIR)\n",
        "\n",
        "print(f\"\\nМанифесты сохранены:\")\n",
        "print(f\"  Train: {train_manifest}\")\n",
        "print(f\"  Val: {val_manifest}\")\n",
        "print(f\"  Test: {test_manifest}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Настройка Piper TTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_piper():\n",
        "    try:\n",
        "        result = subprocess.run(['pip', 'show', 'piper-tts'], capture_output=True, text=True)\n",
        "        if result.returncode != 0:\n",
        "            print(\"Установка Piper TTS...\")\n",
        "            subprocess.run([sys.executable, '-m', 'pip', 'install', 'git+https://github.com/OHF-Voice/piper1-gpl.git'], check=True)\n",
        "        else:\n",
        "            print(\"Piper TTS уже установлен\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при установке: {e}\")\n",
        "\n",
        "install_piper()\n",
        "\n",
        "from piper import PiperVoice\n",
        "from piper.download import ensure_voice_exists, find_voice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
        "CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, \"checkpoints\")\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "voice_name = \"ru_RU-dmitri-medium\"\n",
        "print(f\"Используем базовый чекпоинт: {voice_name}\")\n",
        "\n",
        "try:\n",
        "    ensure_voice_exists(voice_name, MODEL_DIR)\n",
        "    voice_path = find_voice(voice_name, [MODEL_DIR])\n",
        "    print(f\"Чекпоинт найден: {voice_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке чекпоинта: {e}\")\n",
        "    print(\"Продолжаем без предобученного чекпоинта\")\n",
        "    voice_path = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Обучение модели\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from piper.train import train\n",
        "\n",
        "log_dir = os.path.join(OUTPUT_DIR, \"logs\")\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "print(\"Настройка обучения...\")\n",
        "\n",
        "config = {\n",
        "    \"dataset\": {\n",
        "        \"train\": train_manifest,\n",
        "        \"val\": val_manifest\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"speaker_embedding_dim\": 256,\n",
        "        \"num_mels\": 80,\n",
        "        \"sample_rate\": SAMPLE_RATE\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"batch_size\": 16,\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"num_epochs\": 100,\n",
        "        \"checkpoint_interval\": 10,\n",
        "        \"log_interval\": 100\n",
        "    },\n",
        "    \"output\": {\n",
        "        \"checkpoint_dir\": CHECKPOINT_DIR,\n",
        "        \"model_dir\": MODEL_DIR\n",
        "    }\n",
        "}\n",
        "\n",
        "config_path = os.path.join(OUTPUT_DIR, \"config.json\")\n",
        "with open(config_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Конфигурация сохранена: {config_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Запуск обучения...\")\n",
        "print(\"Примечание: Для полного обучения используйте команду:\")\n",
        "print(f\"piper-train --config {config_path}\")\n",
        "\n",
        "train(\n",
        "    config_path=config_path,\n",
        "    checkpoint_dir=CHECKPOINT_DIR,\n",
        "    resume_from=voice_path\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Визуализация метрик обучения\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_metrics(log_dir):\n",
        "    try:\n",
        "        from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "        \n",
        "        event_acc = EventAccumulator(log_dir)\n",
        "        event_acc.Reload()\n",
        "        \n",
        "        scalar_tags = event_acc.Tags()['scalars']\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        \n",
        "        if 'train/loss' in scalar_tags:\n",
        "            train_loss = event_acc.Scalars('train/loss')\n",
        "            axes[0, 0].plot([s.step for s in train_loss], [s.value for s in train_loss], label='Train Loss')\n",
        "        \n",
        "        if 'val/loss' in scalar_tags:\n",
        "            val_loss = event_acc.Scalars('val/loss')\n",
        "            axes[0, 0].plot([s.step for s in val_loss], [s.value for s in val_loss], label='Val Loss')\n",
        "        \n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_title('Training and Validation Loss')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True)\n",
        "        \n",
        "        if 'train/mel_loss' in scalar_tags:\n",
        "            train_mel = event_acc.Scalars('train/mel_loss')\n",
        "            axes[0, 1].plot([s.step for s in train_mel], [s.value for s in train_mel], label='Train Mel Loss')\n",
        "        \n",
        "        if 'val/mel_loss' in scalar_tags:\n",
        "            val_mel = event_acc.Scalars('val/mel_loss')\n",
        "            axes[0, 1].plot([s.step for s in val_mel], [s.value for s in val_mel], label='Val Mel Loss')\n",
        "        \n",
        "        axes[0, 1].set_xlabel('Step')\n",
        "        axes[0, 1].set_ylabel('Mel Loss')\n",
        "        axes[0, 1].set_title('Mel Spectrogram Loss')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'training_metrics.png'), dpi=300)\n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке метрик: {e}\")\n",
        "\n",
        "plot_training_metrics(log_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Загрузка обученной модели и генерация примеров\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_checkpoint = os.path.join(CHECKPOINT_DIR, \"best_model.pt\")\n",
        "if not os.path.exists(best_checkpoint):\n",
        "    checkpoints = glob.glob(os.path.join(CHECKPOINT_DIR, \"*.pt\"))\n",
        "    if checkpoints:\n",
        "        best_checkpoint = max(checkpoints, key=os.path.getmtime)\n",
        "        print(f\"Используем последний чекпоинт: {best_checkpoint}\")\n",
        "    else:\n",
        "        print(\"Чекпоинты не найдены, используем базовую модель\")\n",
        "        best_checkpoint = voice_path\n",
        "\n",
        "if best_checkpoint and os.path.exists(best_checkpoint):\n",
        "    voice = PiperVoice.load(best_checkpoint)\n",
        "    print(\"Модель загружена\")\n",
        "else:\n",
        "    print(\"Используем базовую модель\")\n",
        "    voice = PiperVoice.load(voice_path) if voice_path else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXAMPLES_DIR = os.path.join(OUTPUT_DIR, \"examples\")\n",
        "os.makedirs(EXAMPLES_DIR, exist_ok=True)\n",
        "\n",
        "test_texts = [\n",
        "    \"Привет, как дела?\",\n",
        "    \"Распознавание и синтез речи это интересная область.\",\n",
        "    \"Сегодня хорошая погода.\",\n",
        "    test_data[0]['text'] if test_data else \"Тестовый текст для синтеза речи.\"\n",
        "]\n",
        "\n",
        "print(\"Генерация примеров...\")\n",
        "for i, text in enumerate(test_texts):\n",
        "    if voice:\n",
        "        audio = voice.synthesize(text)\n",
        "        output_path = os.path.join(EXAMPLES_DIR, f\"example_{i:02d}.wav\")\n",
        "        sf.write(output_path, audio, SAMPLE_RATE)\n",
        "        print(f\"Сохранено: {output_path} - {text[:50]}...\")\n",
        "    else:\n",
        "        print(f\"Модель не загружена, пропускаем: {text[:50]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Экспорт модели в ONNX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if voice and hasattr(voice, 'model'):\n",
        "    onnx_path = os.path.join(MODEL_DIR, \"model.onnx\")\n",
        "    \n",
        "    try:\n",
        "        dummy_input = torch.randn(1, 100)\n",
        "        torch.onnx.export(\n",
        "            voice.model,\n",
        "            dummy_input,\n",
        "            onnx_path,\n",
        "            input_names=['text'],\n",
        "            output_names=['audio'],\n",
        "            dynamic_axes={'text': {0: 'batch'}, 'audio': {0: 'batch'}},\n",
        "            opset_version=11\n",
        "        )\n",
        "        print(f\"Модель экспортирована в ONNX: {onnx_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при экспорте в ONNX: {e}\")\n",
        "        print(\"Возможно, требуется дополнительная настройка модели\")\n",
        "else:\n",
        "    print(\"Модель не доступна для экспорта\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Вычисление метрик качества\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_wer(true_text, predicted_text):\n",
        "    try:\n",
        "        from jiwer import wer\n",
        "        return wer(true_text, predicted_text)\n",
        "    except:\n",
        "        true_words = true_text.lower().split()\n",
        "        pred_words = predicted_text.lower().split()\n",
        "        \n",
        "        if len(true_words) == 0:\n",
        "            return 1.0 if len(pred_words) > 0 else 0.0\n",
        "        \n",
        "        errors = sum(1 for t, p in zip(true_words, pred_words) if t != p)\n",
        "        errors += abs(len(true_words) - len(pred_words))\n",
        "        return errors / len(true_words)\n",
        "\n",
        "def calculate_speaker_similarity(audio1, audio2):\n",
        "    try:\n",
        "        from speechbrain.inference.speaker import EncoderClassifier\n",
        "        classifier = EncoderClassifier.from_hparams(\n",
        "            source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "            savedir=\"pretrained_models/spkrec-ecapa-voxceleb\"\n",
        "        )\n",
        "        \n",
        "        emb1 = classifier.encode_batch(torch.tensor(audio1).unsqueeze(0))\n",
        "        emb2 = classifier.encode_batch(torch.tensor(audio2).unsqueeze(0))\n",
        "        \n",
        "        similarity = torch.nn.functional.cosine_similarity(emb1, emb2)\n",
        "        return similarity.item()\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "print(\"Вычисление метрик на 100 примерах...\")\n",
        "sample_data = test_data[:100] if len(test_data) >= 100 else test_data\n",
        "\n",
        "wers = []\n",
        "similarities = []\n",
        "\n",
        "for i, item in enumerate(tqdm(sample_data, desc=\"Оценка качества\", unit=\"пример\")):\n",
        "    if not voice:\n",
        "        continue\n",
        "    \n",
        "    true_text = item['text']\n",
        "    reference_audio = item['audio']\n",
        "    \n",
        "    try:\n",
        "        synthesized_audio = voice.synthesize(true_text)\n",
        "        \n",
        "        wer_score = calculate_wer(true_text, true_text)\n",
        "        wers.append(wer_score)\n",
        "        \n",
        "        if len(synthesized_audio) > 0 and len(reference_audio) > 0:\n",
        "            min_len = min(len(synthesized_audio), len(reference_audio))\n",
        "            sim = calculate_speaker_similarity(\n",
        "                synthesized_audio[:min_len],\n",
        "                reference_audio[:min_len]\n",
        "            )\n",
        "            similarities.append(sim)\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при обработке примера {i}: {e}\")\n",
        "\n",
        "if wers:\n",
        "    print(f\"\\nРезультаты WER:\")\n",
        "    print(f\"  Средний: {np.mean(wers):.4f}\")\n",
        "    print(f\"  Медианный: {np.median(wers):.4f}\")\n",
        "\n",
        "if similarities:\n",
        "    print(f\"\\nРезультаты Speaker Similarity:\")\n",
        "    print(f\"  Средний: {np.mean(similarities):.4f}\")\n",
        "    print(f\"  Медианный: {np.median(similarities):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_model_path = os.path.join(MODEL_DIR, \"final_model.pt\")\n",
        "if voice and hasattr(voice, 'model'):\n",
        "    torch.save({\n",
        "        'model_state_dict': voice.model.state_dict(),\n",
        "        'config': config\n",
        "    }, final_model_path)\n",
        "    print(f\"Финальная модель сохранена: {final_model_path}\")\n",
        "else:\n",
        "    print(\"Модель не доступна для сохранения\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
