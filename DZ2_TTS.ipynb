{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Домашнее задание №2: Синтез речи\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Установка зависимостей для сборки\n",
        "# setuptools нужен для distutils (Python 3.12+)\n",
        "%pip install setuptools wheel build Cython\n",
        "\n",
        "# Импорты для работы с путями и процессами\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Установка piper из локальной папки\n",
        "# ВАЖНО: После установки может потребоваться перезапуск ядра Jupyter\n",
        "LOCAL_PIPER_PATH = \"/tf/piper1-gpl\"\n",
        "\n",
        "print(\"Установка piper из локальной папки...\")\n",
        "print(\"Это может занять несколько минут из-за компиляции C-расширений...\")\n",
        "\n",
        "# Установка piper\n",
        "%pip install -e /tf/piper1-gpl\n",
        "\n",
        "# Проверяем, установился ли espeakbridge\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from piper import espeakbridge\n",
        "    print(\"✓ espeakbridge успешно установлен\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ espeakbridge не найден: {e}\")\n",
        "    print(\"\\nПопытка автоматической сборки espeakbridge...\")\n",
        "    \n",
        "    # Пробуем собрать espeakbridge\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, \"setup.py\", \"build_ext\", \"--inplace\"],\n",
        "            cwd=\"/tf/piper1-gpl\",\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=300\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(\"✓ Сборка завершена, переустанавливаем...\")\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \"/tf/piper1-gpl\"], check=False)\n",
        "            \n",
        "            # Проверяем снова\n",
        "            try:\n",
        "                from piper import espeakbridge\n",
        "                print(\"✓ espeakbridge успешно собран и установлен\")\n",
        "            except ImportError:\n",
        "                print(\"✗ espeakbridge все еще не найден после сборки\")\n",
        "                print(\"\\n⚠️  Выполните в терминале:\")\n",
        "                print(\"  cd /tf/piper1-gpl\")\n",
        "                print(\"  python setup.py build_ext --inplace\")\n",
        "                print(\"  pip install -e .\")\n",
        "        else:\n",
        "            print(f\"✗ Ошибка сборки: {result.stderr}\")\n",
        "            print(\"\\n⚠️  Возможно, нужны системные зависимости:\")\n",
        "            print(\"  sudo apt-get install espeak-ng libespeak-ng-dev\")\n",
        "    except Exception as build_error:\n",
        "        print(f\"✗ Не удалось собрать автоматически: {build_error}\")\n",
        "        print(\"\\n⚠️  Выполните в терминале:\")\n",
        "        print(\"  cd /tf/piper1-gpl\")\n",
        "        print(\"  python setup.py build_ext --inplace\")\n",
        "        print(\"  pip install -e .\")\n",
        "\n",
        "# Проверяем и собираем monotonic_align (критически важно для обучения)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ПРОВЕРКА И СБОРКА MONOTONIC_ALIGN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    from piper.train.vits.monotonic_align import monotonic_align\n",
        "    from piper.train.vits.monotonic_align.monotonic_align.core import maximum_path_c\n",
        "    print(\"✓ monotonic_align успешно установлен\")\n",
        "except (ImportError, ModuleNotFoundError) as e:\n",
        "    print(f\"✗ monotonic_align не найден: {e}\")\n",
        "    print(\"\\nПопытка автоматической сборки monotonic_align...\")\n",
        "    \n",
        "    # Проверяем наличие скрипта сборки\n",
        "    build_script = os.path.join(LOCAL_PIPER_PATH, \"build_monotonic_align.sh\")\n",
        "    monotonic_dir = os.path.join(LOCAL_PIPER_PATH, \"src\", \"piper\", \"train\", \"vits\", \"monotonic_align\")\n",
        "    \n",
        "    if os.path.exists(build_script):\n",
        "        print(f\"  Найден скрипт сборки: {build_script}\")\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"bash\", build_script],\n",
        "                cwd=LOCAL_PIPER_PATH,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=600\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"✓ Сборка monotonic_align завершена\")\n",
        "                # Переустанавливаем piper\n",
        "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", LOCAL_PIPER_PATH], check=False)\n",
        "            else:\n",
        "                print(f\"✗ Ошибка сборки скриптом: {result.stderr[:500] if result.stderr else 'Неизвестная ошибка'}\")\n",
        "                print(f\"   Вывод: {result.stdout[:500] if result.stdout else 'Нет вывода'}\")\n",
        "        except Exception as build_error:\n",
        "            print(f\"✗ Ошибка при запуске скрипта: {build_error}\")\n",
        "    \n",
        "    # Пробуем собрать через setup.py\n",
        "    if os.path.exists(monotonic_dir):\n",
        "        print(f\"\\n  Пробуем собрать через setup.py...\")\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [sys.executable, \"setup.py\", \"build_ext\", \"--inplace\"],\n",
        "                cwd=LOCAL_PIPER_PATH,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=600\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"✓ Сборка через setup.py завершена, переустанавливаем...\")\n",
        "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", LOCAL_PIPER_PATH], check=False)\n",
        "            else:\n",
        "                print(f\"✗ Ошибка сборки через setup.py: {result.stderr[:500] if result.stderr else 'Неизвестная ошибка'}\")\n",
        "        except Exception as build_error:\n",
        "            print(f\"✗ Ошибка сборки: {build_error}\")\n",
        "    \n",
        "    # Проверяем снова\n",
        "    try:\n",
        "        from piper.train.vits.monotonic_align import monotonic_align\n",
        "        from piper.train.vits.monotonic_align.monotonic_align.core import maximum_path_c\n",
        "        print(\"\\n✓ monotonic_align успешно собран и установлен\")\n",
        "    except (ImportError, ModuleNotFoundError):\n",
        "        print(\"\\n✗ monotonic_align все еще не найден после сборки\")\n",
        "        print(\"\\n⚠️  Выполните в терминале:\")\n",
        "        print(\"  cd /tf/piper1-gpl\")\n",
        "        print(\"  pip install setuptools Cython\")\n",
        "        print(\"  bash build_monotonic_align.sh\")\n",
        "        print(\"  # или\")\n",
        "        print(\"  python setup.py build_ext --inplace\")\n",
        "        print(\"  pip install -e .\")\n",
        "        print(\"\\n⚠️  Возможно, нужны системные зависимости:\")\n",
        "        print(\"  sudo apt-get install build-essential python3-dev\")\n",
        "\n",
        "# Установка nvidia-ml-py для замены устаревшего pynvml\n",
        "%pip install nvidia-ml-py\n",
        "\n",
        "# Проверка установки\n",
        "try:\n",
        "    import pynvml\n",
        "    print(\"⚠️  pynvml все еще установлен - он используется torch\")\n",
        "    print(\"   Предупреждение о pynvml исходит из torch, не из нашего кода\")\n",
        "    print(\"   nvidia-ml-py установлен для будущего использования\")\n",
        "except ImportError:\n",
        "    print(\"✓ pynvml не установлен\")\n",
        "\n",
        "try:\n",
        "    import nvidia_ml_py3 as nvml\n",
        "    print(\"✓ nvidia-ml-py успешно установлен\")\n",
        "except ImportError:\n",
        "    print(\"⚠️  nvidia-ml-py не установлен, но это не критично\")\n",
        "\n",
        "%pip install tensorboard\n",
        "%pip install onnx\n",
        "%pip install onnxruntime\n",
        "\n",
        "# Проверка версии torch\n",
        "import torch\n",
        "print(f\"\\nВерсия PyTorch: {torch.__version__}\")\n",
        "print(\"⚠️  Предупреждение о weight_norm исходит из кода piper,\")\n",
        "print(\"   его можно исправить только в исходниках piper\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Импорты\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# Устанавливаем seed для всех генераторов случайных чисел\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Для воспроизводимости на GPU\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Устанавливаем seed для Lightning через переменную окружения\n",
        "import os\n",
        "os.environ['PL_GLOBAL_SEED'] = str(SEED)\n",
        "\n",
        "print(f\"Seed установлен: {SEED}\")\n",
        "print(f\"PL_GLOBAL_SEED: {os.environ.get('PL_GLOBAL_SEED')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Подготовка данных\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = \"/share/audio_data/sova/ytub/raid/nanosemantics/nextcloud/sova_done\"\n",
        "\n",
        "# OUTPUT_DIR делаем абсолютным, чтобы избежать проблем с дублированием путей\n",
        "OUTPUT_DIR_REL = \"./tts_data\"\n",
        "OUTPUT_DIR = os.path.abspath(OUTPUT_DIR_REL)\n",
        "# Нормализуем путь (убираем лишние компоненты типа ./)\n",
        "OUTPUT_DIR = os.path.normpath(OUTPUT_DIR)\n",
        "\n",
        "NATAHA_DIR = \"/tf/nspeganov/nataha\"\n",
        "\n",
        "print(f\"OUTPUT_DIR установлен: {OUTPUT_DIR}\")\n",
        "print(f\"Текущая рабочая директория: {os.getcwd()}\")\n",
        "\n",
        "# Проверяем наличие готовых данных (сначала локальная папка, потом share)\n",
        "LOCAL_TRAIN_CSV = os.path.join(NATAHA_DIR, \"train.csv\")\n",
        "LOCAL_VAL_CSV = os.path.join(NATAHA_DIR, \"valid.csv\")\n",
        "LOCAL_TRAIN_DIR = os.path.join(NATAHA_DIR, \"train\")\n",
        "LOCAL_VAL_DIR = os.path.join(NATAHA_DIR, \"valid\")\n",
        "\n",
        "SHARE_NATAHA_DIR = \"/share/tts_data/nataha\"\n",
        "SHARE_TRAIN_CSV = os.path.join(SHARE_NATAHA_DIR, \"train.csv\")\n",
        "SHARE_VAL_CSV = os.path.join(SHARE_NATAHA_DIR, \"valid.csv\")\n",
        "SHARE_TRAIN_DIR = os.path.join(SHARE_NATAHA_DIR, \"train\")\n",
        "SHARE_VAL_DIR = os.path.join(SHARE_NATAHA_DIR, \"valid\")\n",
        "\n",
        "# Определяем, какие данные использовать (приоритет локальной папке)\n",
        "if os.path.exists(LOCAL_TRAIN_CSV) and os.path.exists(LOCAL_VAL_CSV):\n",
        "    print(f\"Найдены готовые данные в локальной папке: {NATAHA_DIR}\")\n",
        "    TRAIN_CSV = LOCAL_TRAIN_CSV\n",
        "    VAL_CSV = LOCAL_VAL_CSV\n",
        "    TRAIN_DIR = LOCAL_TRAIN_DIR\n",
        "    VAL_DIR = LOCAL_VAL_DIR\n",
        "    USE_PREPARED_DATA = True\n",
        "elif os.path.exists(SHARE_TRAIN_CSV) and os.path.exists(SHARE_VAL_CSV):\n",
        "    print(f\"Найдены готовые данные в share: {SHARE_NATAHA_DIR}\")\n",
        "    TRAIN_CSV = SHARE_TRAIN_CSV\n",
        "    VAL_CSV = SHARE_VAL_CSV\n",
        "    TRAIN_DIR = SHARE_TRAIN_DIR\n",
        "    VAL_DIR = SHARE_VAL_DIR\n",
        "    USE_PREPARED_DATA = True\n",
        "else:\n",
        "    print(\"Готовые данные не найдены, будем обрабатывать из SOVA\")\n",
        "    TRAIN_DIR = LOCAL_TRAIN_DIR\n",
        "    VAL_DIR = LOCAL_VAL_DIR\n",
        "    USE_PREPARED_DATA = False\n",
        "\n",
        "TEST_DIR = os.path.join(OUTPUT_DIR, \"test\")\n",
        "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
        "os.makedirs(VAL_DIR, exist_ok=True)\n",
        "os.makedirs(TEST_DIR, exist_ok=True)\n",
        "\n",
        "SAMPLE_RATE = 22050\n",
        "MIN_DURATION = 0.5\n",
        "MAX_DURATION = 10.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def trim_silence(audio, sr, threshold_db=-40):\n",
        "    try:\n",
        "        trimmed, _ = librosa.effects.trim(audio, top_db=abs(threshold_db))\n",
        "        return trimmed\n",
        "    except:\n",
        "        return audio\n",
        "\n",
        "def load_audio_info(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=None)\n",
        "        duration = len(y) / sr\n",
        "        return duration, sr, y\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке {audio_path}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def prepare_tts_dataset(base_path, parts=['part_0', 'part_1', 'part_2'], max_files=None, trim_silence_audio=True):\n",
        "    import time\n",
        "    dataset = []\n",
        "    audio_extensions = ['.wav', '.mp3', '.flac', '.ogg']\n",
        "    audio_files = []\n",
        "    \n",
        "    print(\"Поиск аудиофайлов...\")\n",
        "    search_start = time.time()\n",
        "    \n",
        "    for part in parts:\n",
        "        part_path = os.path.join(base_path, part)\n",
        "        if not os.path.exists(part_path):\n",
        "            print(f\"Предупреждение: часть {part} не найдена\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\nОбработка {part}...\")\n",
        "        part_start = time.time()\n",
        "        \n",
        "        # Сначала собираем все поддиректории для оценки прогресса\n",
        "        all_dirs = []\n",
        "        print(\"  Сканирование структуры директорий...\")\n",
        "        for root, dirs, files in os.walk(part_path):\n",
        "            all_dirs.append(root)\n",
        "        \n",
        "        print(f\"  Найдено {len(all_dirs)} поддиректорий\")\n",
        "        \n",
        "        # Поиск файлов с прогресс-баром\n",
        "        part_files = []\n",
        "        ext_counts = {ext: 0 for ext in audio_extensions}\n",
        "        \n",
        "        pbar_search = tqdm(all_dirs, desc=f\"Поиск в {part}\", unit=\"дир\", ncols=100, leave=False)\n",
        "        for root in pbar_search:\n",
        "            try:\n",
        "                for file in os.listdir(root):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    if os.path.isfile(file_path):\n",
        "                        file_ext = os.path.splitext(file)[1].lower()\n",
        "                        if file_ext in audio_extensions:\n",
        "                            part_files.append(file_path)\n",
        "                            ext_counts[file_ext] += 1\n",
        "                            pbar_search.set_postfix({\n",
        "                                'найдено': len(part_files),\n",
        "                                'wav': ext_counts['.wav'],\n",
        "                                'mp3': ext_counts['.mp3']\n",
        "                            })\n",
        "            except (PermissionError, OSError) as e:\n",
        "                continue\n",
        "        \n",
        "        audio_files.extend(part_files)\n",
        "        part_time = time.time() - part_start\n",
        "        print(f\"  {part}: найдено {len(part_files)} файлов за {part_time:.1f}с ({part_time/60:.1f} мин)\")\n",
        "        for ext, count in ext_counts.items():\n",
        "            if count > 0:\n",
        "                print(f\"    {ext}: {count} файлов\")\n",
        "    \n",
        "    search_time = time.time() - search_start\n",
        "    \n",
        "    if max_files and len(audio_files) > max_files:\n",
        "        print(f\"\\nОграничение: было {len(audio_files)} файлов, оставляем {max_files}\")\n",
        "        audio_files = audio_files[:max_files]\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Поиск завершен: найдено {len(audio_files)} аудиофайлов\")\n",
        "    print(f\"Время поиска: {search_time:.1f}с ({search_time/60:.1f} мин)\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    print(\"Начинаю обработку файлов...\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    processed = 0\n",
        "    skipped_no_text = 0\n",
        "    skipped_duration = 0\n",
        "    skipped_trim = 0\n",
        "    \n",
        "    pbar = tqdm(audio_files, desc=\"Подготовка данных\", unit=\"файл\", ncols=100)\n",
        "    for audio_path in pbar:\n",
        "        duration, sr, y = load_audio_info(audio_path)\n",
        "        if duration is None:\n",
        "            continue\n",
        "        \n",
        "        if duration < MIN_DURATION or duration > MAX_DURATION:\n",
        "            skipped_duration += 1\n",
        "            continue\n",
        "        \n",
        "        text_path = audio_path.rsplit('.', 1)[0] + '.txt'\n",
        "        if not os.path.exists(text_path):\n",
        "            skipped_no_text += 1\n",
        "            continue\n",
        "        \n",
        "        with open(text_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "        \n",
        "        text = normalize_text(text)\n",
        "        \n",
        "        if len(text) > 0:\n",
        "            resampled_audio = librosa.resample(y, orig_sr=sr, target_sr=SAMPLE_RATE) if sr != SAMPLE_RATE else y\n",
        "            \n",
        "            if trim_silence_audio:\n",
        "                resampled_audio = trim_silence(resampled_audio, SAMPLE_RATE)\n",
        "                if len(resampled_audio) < SAMPLE_RATE * MIN_DURATION:\n",
        "                    skipped_trim += 1\n",
        "                    continue\n",
        "            \n",
        "            dataset.append({\n",
        "                \"audio\": resampled_audio,\n",
        "                \"text\": text,\n",
        "                \"duration\": len(resampled_audio) / SAMPLE_RATE,\n",
        "                \"path\": audio_path\n",
        "            })\n",
        "            processed += 1\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        pbar.set_postfix({\n",
        "            'Обработано': processed,\n",
        "            'Пропущено': skipped_no_text + skipped_duration + skipped_trim,\n",
        "            'Время': f\"{elapsed:.1f}с\"\n",
        "        })\n",
        "    \n",
        "    elapsed_total = time.time() - start_time\n",
        "    print(f\"\\nПодготовка завершена!\")\n",
        "    print(f\"  Обработано записей: {len(dataset)}\")\n",
        "    print(f\"  Пропущено (нет текста): {skipped_no_text}\")\n",
        "    print(f\"  Пропущено (длительность): {skipped_duration}\")\n",
        "    print(f\"  Пропущено (после обрезки): {skipped_trim}\")\n",
        "    print(f\"  Время обработки: {elapsed_total:.1f} секунд ({elapsed_total/60:.1f} минут)\")\n",
        "    print(f\"  Скорость: {len(audio_files)/elapsed_total:.1f} файлов/сек\")\n",
        "    \n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ПОДГОТОВКА ДАННЫХ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if USE_PREPARED_DATA:\n",
        "    print(f\"\\nИспользуем готовые данные из CSV файлов\")\n",
        "    print(f\"  Train CSV: {TRAIN_CSV}\")\n",
        "    print(f\"  Val CSV: {VAL_CSV}\")\n",
        "    print(f\"  Train Dir: {TRAIN_DIR}\")\n",
        "    print(f\"  Val Dir: {VAL_DIR}\")\n",
        "    \n",
        "    # Загружаем информацию из CSV (только для статистики, сами аудио не загружаем)\n",
        "    train_data = []\n",
        "    val_data = []\n",
        "    \n",
        "    print(\"\\nЧтение train.csv...\")\n",
        "    with open(TRAIN_CSV, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, desc=\"Загрузка train\", unit=\"строка\"):\n",
        "            line = line.strip()\n",
        "            if '|' in line:\n",
        "                audio_file, text = line.split('|', 1)\n",
        "                train_data.append({\n",
        "                    'audio_file': audio_file,\n",
        "                    'text': text,\n",
        "                    'path': os.path.join(TRAIN_DIR, audio_file)\n",
        "                })\n",
        "    \n",
        "    print(f\"Загружено {len(train_data)} записей из train.csv\")\n",
        "    \n",
        "    print(\"\\nЧтение valid.csv...\")\n",
        "    with open(VAL_CSV, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, desc=\"Загрузка valid\", unit=\"строка\"):\n",
        "            line = line.strip()\n",
        "            if '|' in line:\n",
        "                audio_file, text = line.split('|', 1)\n",
        "                val_data.append({\n",
        "                    'audio_file': audio_file,\n",
        "                    'text': text,\n",
        "                    'path': os.path.join(VAL_DIR, audio_file)\n",
        "                })\n",
        "    \n",
        "    print(f\"Загружено {len(val_data)} записей из valid.csv\")\n",
        "    \n",
        "    # Для теста используем часть валидационных данных\n",
        "    test_size = int(0.1 * len(val_data))\n",
        "    test_data = val_data[:test_size]\n",
        "    val_data = val_data[test_size:]\n",
        "    \n",
        "    print(f\"\\nРазделение данных:\")\n",
        "    print(f\"  Train: {len(train_data)} записей\")\n",
        "    print(f\"  Val: {len(val_data)} записей\")\n",
        "    print(f\"  Test: {len(test_data)} записей\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ДАННЫЕ ЗАГРУЖЕНЫ ИЗ ГОТОВЫХ CSV\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "else:\n",
        "    print(\"\\nОбрабатываем данные из SOVA...\")\n",
        "    dataset = prepare_tts_dataset(DATA_PATH, parts=['part_0', 'part_1', 'part_2'])\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"РАЗДЕЛЕНИЕ ДАННЫХ\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(\"Перемешивание данных...\")\n",
        "    random.shuffle(dataset)\n",
        "    \n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = int(0.1 * len(dataset))\n",
        "    \n",
        "    train_data = dataset[:train_size]\n",
        "    val_data = dataset[train_size:train_size + val_size]\n",
        "    test_data = dataset[train_size + val_size:]\n",
        "    \n",
        "    print(f\"\\nРазделение данных:\")\n",
        "    print(f\"  Train: {len(train_data)} записей ({100*len(train_data)/len(dataset):.1f}%)\")\n",
        "    print(f\"  Val: {len(val_data)} записей ({100*len(val_data)/len(dataset):.1f}%)\")\n",
        "    print(f\"  Test: {len(test_data)} записей ({100*len(test_data)/len(dataset):.1f}%)\")\n",
        "    \n",
        "    print(\"\\nВычисление статистики по длительности...\")\n",
        "    durations = [item['duration'] for item in tqdm(dataset, desc=\"Обработка длительностей\", unit=\"запись\", leave=False)]\n",
        "    \n",
        "    print(f\"\\nСтатистика по длительности:\")\n",
        "    print(f\"  Минимум: {min(durations):.2f} сек\")\n",
        "    print(f\"  Максимум: {max(durations):.2f} сек\")\n",
        "    print(f\"  Среднее: {np.mean(durations):.2f} сек\")\n",
        "    print(f\"  Медиана: {np.median(durations):.2f} сек\")\n",
        "    print(f\"  Общая длительность: {sum(durations)/3600:.2f} часов\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ПОДГОТОВКА ДАННЫХ ЗАВЕРШЕНА\")\n",
        "    print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Сохранение подготовленных данных\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_dataset_csv(data, output_dir, csv_path):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    with open(csv_path, 'w', encoding='utf-8') as f:\n",
        "        for i, item in enumerate(tqdm(data, desc=\"Сохранение\", unit=\"файл\")):\n",
        "            audio_filename = f\"{i:06d}.wav\"\n",
        "            audio_path = os.path.join(output_dir, audio_filename)\n",
        "            \n",
        "            sf.write(audio_path, item['audio'], SAMPLE_RATE)\n",
        "            \n",
        "            text = item['text'].replace('|', ' ').replace('\\n', ' ').strip()\n",
        "            f.write(f\"{audio_filename}|{text}\\n\")\n",
        "    \n",
        "    print(f\"CSV сохранен: {csv_path}\")\n",
        "    return csv_path\n",
        "\n",
        "if USE_PREPARED_DATA:\n",
        "    print(\"Используем готовые CSV файлы, сохранение не требуется\")\n",
        "    train_csv = TRAIN_CSV\n",
        "    val_csv = VAL_CSV\n",
        "    print(f\"  Train CSV: {train_csv}\")\n",
        "    print(f\"  Val CSV: {val_csv}\")\n",
        "else:\n",
        "    print(\"Сохранение обработанных данных в CSV...\")\n",
        "    train_csv = save_dataset_csv(train_data, TRAIN_DIR, os.path.join(NATAHA_DIR, \"train.csv\"))\n",
        "    val_csv = save_dataset_csv(val_data, VAL_DIR, os.path.join(NATAHA_DIR, \"valid.csv\"))\n",
        "    \n",
        "    print(f\"\\nCSV файлы сохранены:\")\n",
        "    print(f\"  Train: {train_csv}\")\n",
        "    print(f\"  Val: {val_csv}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Создание конфигурационного файла\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_path = os.path.join(OUTPUT_DIR, \"nata_config.json\")\n",
        "\n",
        "config = {\n",
        "    \"audio\": {\n",
        "        \"sample_rate\": SAMPLE_RATE\n",
        "    },\n",
        "    \"espeak\": {\n",
        "        \"voice\": \"ru\"\n",
        "    },\n",
        "    \"phoneme_type\": \"espeak\",\n",
        "    \"num_symbols\": 256,\n",
        "    \"num_speakers\": 1,\n",
        "    \"inference\": {\n",
        "        \"noise_scale\": 0.667,\n",
        "        \"length_scale\": 1.0,\n",
        "        \"noise_w\": 0.8\n",
        "    },\n",
        "    \"hop_length\": 256,\n",
        "    \"piper_version\": \"1.3.0\"\n",
        "}\n",
        "\n",
        "with open(config_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Конфигурация сохранена: {config_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загрузка базового чекпоинта\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Загрузка базового чекпоинта\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, \"checkpoints\")\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "base_checkpoint_url = \"https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/ru/ru_RU/ruslan/medium/epoch=2436-step=1724372.ckpt\"\n",
        "base_checkpoint_path = os.path.join(CHECKPOINT_DIR, \"epoch=2436-step=1724372.ckpt\")\n",
        "\n",
        "if not os.path.exists(base_checkpoint_path):\n",
        "    print(\"Загрузка базового чекпоинта...\")\n",
        "    import urllib.request\n",
        "    urllib.request.urlretrieve(base_checkpoint_url, base_checkpoint_path)\n",
        "    print(f\"Чекпоинт загружен: {base_checkpoint_path}\")\n",
        "else:\n",
        "    print(f\"Чекпоинт уже существует: {base_checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "cache_dir = os.path.join(OUTPUT_DIR, \"nata_cache\")\n",
        "os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "log_dir = os.path.join(OUTPUT_DIR, \"lightning_logs\")\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "print(\"Запуск обучения...\")\n",
        "print(\"Команда обучения будет выполнена в следующей ячейке\")\n",
        "\n",
        "train_cmd = [\n",
        "    \"python3\", \"-m\", \"piper.train\", \"fit\",\n",
        "    \"--data.voice_name\", \"nata\",\n",
        "    \"--data.csv_path\", train_csv,\n",
        "    \"--data.audio_dir\", TRAIN_DIR,\n",
        "    \"--model.sample_rate\", str(SAMPLE_RATE),\n",
        "    \"--data.espeak_voice\", \"ru\",\n",
        "    \"--data.cache_dir\", cache_dir,\n",
        "    \"--data.config_path\", config_path,\n",
        "    \"--data.batch_size\", \"16\",\n",
        "    \"--ckpt_path\", base_checkpoint_path\n",
        "]\n",
        "\n",
        "print(\" \".join(train_cmd))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Проверка установки piper\n",
        "print(\"Проверка установки piper...\")\n",
        "try:\n",
        "    import piper\n",
        "    print(f\"✓ Piper установлен: {piper.__file__}\")\n",
        "except ImportError:\n",
        "    print(\"✗ Piper не установлен, пробуем добавить локальный путь...\")\n",
        "    LOCAL_PIPER_PATH = \"/tf/piper1-gpl\"\n",
        "    if os.path.exists(LOCAL_PIPER_PATH):\n",
        "        sys.path.insert(0, LOCAL_PIPER_PATH)\n",
        "        print(f\"  Добавлен путь: {LOCAL_PIPER_PATH}\")\n",
        "        try:\n",
        "            import piper\n",
        "            print(f\"✓ Piper загружен из локального пути: {piper.__file__}\")\n",
        "        except ImportError:\n",
        "            print(\"✗ Не удалось загрузить piper, проверьте установку\")\n",
        "    else:\n",
        "        print(\"✗ Локальный путь не найден, установите piper: %pip install -e /tf/piper1-gpl\")\n",
        "\n",
        "cache_dir = os.path.join(OUTPUT_DIR, \"nata_cache\")\n",
        "os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "log_dir = os.path.join(OUTPUT_DIR, \"lightning_logs\")\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "print(\"\\nНастройка команды обучения...\")\n",
        "\n",
        "# Делаем пути корректными для команды (так как cwd=OUTPUT_DIR)\n",
        "def make_path_for_cmd(path, base_dir):\n",
        "    \"\"\"Делает путь корректным для команды, запускаемой с cwd=base_dir\"\"\"\n",
        "    # Нормализуем base_dir до абсолютного пути\n",
        "    base_dir_abs = os.path.abspath(os.path.normpath(base_dir))\n",
        "    \n",
        "    # Нормализуем path до абсолютного пути\n",
        "    if os.path.isabs(path):\n",
        "        path_abs = os.path.abspath(os.path.normpath(path))\n",
        "    else:\n",
        "        # Если путь относительный, нормализуем относительно текущей рабочей директории\n",
        "        # НЕ относительно base_dir, чтобы избежать дублирования\n",
        "        current_dir = os.getcwd()\n",
        "        path_abs = os.path.abspath(os.path.normpath(os.path.join(current_dir, path)))\n",
        "    \n",
        "    # Проверяем, находится ли путь внутри base_dir\n",
        "    try:\n",
        "        # Нормализуем пути для сравнения\n",
        "        base_dir_norm = os.path.normpath(base_dir_abs)\n",
        "        path_norm = os.path.normpath(path_abs)\n",
        "        \n",
        "        # Проверяем, что path начинается с base_dir\n",
        "        # Используем os.path.commonpath для более надежной проверки\n",
        "        try:\n",
        "            common = os.path.commonpath([base_dir_norm, path_norm])\n",
        "            if common == base_dir_norm:\n",
        "                # Путь находится внутри base_dir, делаем его относительным\n",
        "                rel_path = os.path.relpath(path_norm, base_dir_norm)\n",
        "                # Проверяем существование\n",
        "                if os.path.exists(path_abs):\n",
        "                    return rel_path\n",
        "        except ValueError:\n",
        "            # Пути на разных дисках\n",
        "            pass\n",
        "        \n",
        "        # Альтернативная проверка через startswith\n",
        "        base_dir_with_sep = base_dir_norm + os.sep\n",
        "        if path_norm == base_dir_norm or path_norm.startswith(base_dir_with_sep):\n",
        "            rel_path = os.path.relpath(path_norm, base_dir_norm)\n",
        "            if os.path.exists(path_abs):\n",
        "                return rel_path\n",
        "    except (ValueError, OSError) as e:\n",
        "        # Пути на разных дисках или другая ошибка\n",
        "        pass\n",
        "    \n",
        "    # Иначе используем абсолютный путь\n",
        "    if os.path.exists(path_abs):\n",
        "        return path_abs\n",
        "    \n",
        "    # Если файл не существует, возвращаем исходный путь (для отладки)\n",
        "    return path\n",
        "\n",
        "# Исправляем пути для команды\n",
        "train_csv_cmd = make_path_for_cmd(train_csv, OUTPUT_DIR)\n",
        "TRAIN_DIR_cmd = make_path_for_cmd(TRAIN_DIR, OUTPUT_DIR)\n",
        "cache_dir_cmd = make_path_for_cmd(cache_dir, OUTPUT_DIR)\n",
        "config_path_cmd = make_path_for_cmd(config_path, OUTPUT_DIR)\n",
        "\n",
        "# Если локальный путь добавлен, используем его в PYTHONPATH\n",
        "LOCAL_PIPER_PATH = \"/tf/piper1-gpl\"\n",
        "SEED = 42\n",
        "\n",
        "env = os.environ.copy()\n",
        "if os.path.exists(LOCAL_PIPER_PATH):\n",
        "    if 'PYTHONPATH' in env:\n",
        "        env['PYTHONPATH'] = LOCAL_PIPER_PATH + ':' + env['PYTHONPATH']\n",
        "    else:\n",
        "        env['PYTHONPATH'] = LOCAL_PIPER_PATH\n",
        "\n",
        "# Устанавливаем seed для Lightning через переменную окружения\n",
        "env['PL_GLOBAL_SEED'] = str(SEED)\n",
        "\n",
        "# Поиск последнего чекпоинта из предыдущих запусков обучения\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ПОИСК ПОСЛЕДНЕГО ЧЕКПОИНТА\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "last_checkpoint = None\n",
        "last_epoch = 0\n",
        "MAX_EPOCHS = 2539\n",
        "\n",
        "# Ищем чекпоинты в lightning_logs\n",
        "if os.path.exists(log_dir):\n",
        "    checkpoints = glob.glob(os.path.join(log_dir, \"**\", \"*.ckpt\"), recursive=True)\n",
        "    if checkpoints:\n",
        "        # Сортируем по времени модификации (последний = самый новый)\n",
        "        checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
        "        \n",
        "        for ckpt in checkpoints:\n",
        "            # Извлекаем номер эпохи из имени файла (формат: epoch=XXXX-step=YYYY.ckpt)\n",
        "            match = re.search(r'epoch=(\\d+)', os.path.basename(ckpt))\n",
        "            if match:\n",
        "                epoch_num = int(match.group(1))\n",
        "                if epoch_num < MAX_EPOCHS:\n",
        "                    last_checkpoint = ckpt\n",
        "                    last_epoch = epoch_num\n",
        "                    print(f\"✓ Найден чекпоинт эпохи {epoch_num}: {ckpt}\")\n",
        "                    break\n",
        "        \n",
        "        if last_checkpoint:\n",
        "            print(f\"\\nИспользуем последний чекпоинт: эпоха {last_epoch}\")\n",
        "        else:\n",
        "            print(f\"\\n⚠️  Последний чекпоинт уже достиг или превысил max_epochs={MAX_EPOCHS}\")\n",
        "            print(\"   Будет использован базовый чекпоинт\")\n",
        "    else:\n",
        "        print(\"Чекпоинты в lightning_logs не найдены\")\n",
        "else:\n",
        "    print(f\"Директория логов не существует: {log_dir}\")\n",
        "\n",
        "# Определяем, какой чекпоинт использовать\n",
        "if last_checkpoint and last_epoch < MAX_EPOCHS:\n",
        "    base_checkpoint_path = last_checkpoint\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ИСПОЛЬЗУЕМ ПОСЛЕДНИЙ ЧЕКПОИНТ ИЗ ОБУЧЕНИЯ\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Чекпоинт: {base_checkpoint_path}\")\n",
        "    print(f\"Эпоха: {last_epoch} (будет продолжено до {MAX_EPOCHS})\")\n",
        "else:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ИСПОЛЬЗУЕМ БАЗОВЫЙ ЧЕКПОИНТ\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "# Проверяем существование чекпоинта и исправляем путь\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ПРОВЕРКА ЧЕКПОИНТА\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Текущая рабочая директория: {os.getcwd()}\")\n",
        "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n",
        "print(f\"OUTPUT_DIR (abs, norm): {os.path.abspath(os.path.normpath(OUTPUT_DIR))}\")\n",
        "print(f\"\\nИсходный путь к чекпоинту: {base_checkpoint_path}\")\n",
        "print(f\"Исходный путь (abs): {os.path.abspath(base_checkpoint_path) if not os.path.isabs(base_checkpoint_path) else base_checkpoint_path}\")\n",
        "\n",
        "if not os.path.exists(base_checkpoint_path):\n",
        "    print(f\"\\n⚠️  ВНИМАНИЕ: Чекпоинт не найден: {base_checkpoint_path}\")\n",
        "    print(\"   Обучение начнется с нуля\")\n",
        "    checkpoint_path_for_cmd = None\n",
        "else:\n",
        "    print(f\"\\n✓ Чекпоинт найден: {base_checkpoint_path}\")\n",
        "    \n",
        "    # Для чекпоинта всегда используем абсолютный нормализованный путь\n",
        "    # Это самый надежный способ избежать проблем с дублированием путей\n",
        "    checkpoint_path_for_cmd = os.path.abspath(os.path.normpath(base_checkpoint_path))\n",
        "    \n",
        "    print(f\"\\nРезультат обработки пути:\")\n",
        "    print(f\"  Абсолютный нормализованный путь: {checkpoint_path_for_cmd}\")\n",
        "    print(f\"  Файл существует: {os.path.exists(checkpoint_path_for_cmd)}\")\n",
        "    \n",
        "    if not os.path.exists(checkpoint_path_for_cmd):\n",
        "        print(f\"\\n⚠️  ПРОБЛЕМА: Абсолютный путь не существует!\")\n",
        "        print(f\"   Обучение начнется с нуля\")\n",
        "        checkpoint_path_for_cmd = None\n",
        "\n",
        "train_cmd = [\n",
        "    \"python3\", \"-m\", \"piper.train\", \"fit\",\n",
        "    \"--seed_everything\", str(SEED),\n",
        "    \"--data.voice_name\", \"nata\",\n",
        "    \"--data.csv_path\", train_csv_cmd,\n",
        "    \"--data.audio_dir\", TRAIN_DIR_cmd,\n",
        "    \"--model.sample_rate\", str(SAMPLE_RATE),\n",
        "    \"--data.espeak_voice\", \"ru\",\n",
        "    \"--data.cache_dir\", cache_dir_cmd,\n",
        "    \"--data.config_path\", config_path_cmd,\n",
        "    \"--data.batch_size\", \"16\",\n",
        "    \"--data.num_workers\", \"4\",\n",
        "    \"--trainer.max_epochs\", \"2539\",\n",
        "    \"--trainer.enable_progress_bar\", \"false\"\n",
        "]\n",
        "\n",
        "# Добавляем чекпоинт только если он существует\n",
        "if checkpoint_path_for_cmd:\n",
        "    train_cmd.extend([\"--ckpt_path\", checkpoint_path_for_cmd])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"НАСТРОЙКИ ОБУЧЕНИЯ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Рабочая директория (cwd): {OUTPUT_DIR}\")\n",
        "print(f\"\\nПути к данным:\")\n",
        "print(f\"  Train CSV: {train_csv} -> {train_csv_cmd}\")\n",
        "print(f\"  Train Dir: {TRAIN_DIR} -> {TRAIN_DIR_cmd}\")\n",
        "print(f\"  Cache Dir: {cache_dir} -> {cache_dir_cmd}\")\n",
        "print(f\"  Config: {config_path} -> {config_path_cmd}\")\n",
        "if checkpoint_path_for_cmd:\n",
        "    print(f\"  Checkpoint: {base_checkpoint_path} -> {checkpoint_path_for_cmd}\")\n",
        "print(f\"\\nПеременные окружения:\")\n",
        "print(f\"  PYTHONPATH: {env.get('PYTHONPATH', 'не установлен')}\")\n",
        "print(f\"  PL_GLOBAL_SEED: {env.get('PL_GLOBAL_SEED', 'не установлен')}\")\n",
        "print(f\"\\nКоманда обучения:\")\n",
        "print(\" \".join(train_cmd))\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Используем env с PYTHONPATH для доступа к локальному piper\n",
        "result = subprocess.run(train_cmd, cwd=OUTPUT_DIR, env=env)\n",
        "print(f\"\\nОбучение завершено с кодом: {result.returncode}\")\n",
        "if result.returncode != 0:\n",
        "    print(\"\\nЕсли возникла ошибка ModuleNotFoundError, попробуйте:\")\n",
        "    print(\"1. Перезапустить ядро после установки piper\")\n",
        "    print(\"2. Установить piper: %pip install -e /tf/piper1-gpl\")\n",
        "    print(\"3. Проверить, что папка /tf/piper1-gpl существует и содержит setup.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Визуализация метрик из TensorBoard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_metrics(log_dir):\n",
        "    try:\n",
        "        from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "        \n",
        "        if not os.path.exists(log_dir):\n",
        "            print(f\"Директория логов не найдена: {log_dir}\")\n",
        "            return\n",
        "        \n",
        "        version_dirs = [d for d in os.listdir(log_dir) if os.path.isdir(os.path.join(log_dir, d)) and d.startswith('version_')]\n",
        "        if not version_dirs:\n",
        "            print(f\"Логи TensorBoard не найдены в {log_dir}\")\n",
        "            print(f\"Содержимое директории: {os.listdir(log_dir) if os.path.exists(log_dir) else 'не существует'}\")\n",
        "            return\n",
        "        \n",
        "        latest_version = max(version_dirs, key=lambda x: int(x.split('_')[1]) if x.split('_')[1].isdigit() else 0)\n",
        "        event_dir = os.path.join(log_dir, latest_version)\n",
        "        \n",
        "        print(f\"Используем логи из: {event_dir}\")\n",
        "        \n",
        "        event_acc = EventAccumulator(event_dir)\n",
        "        event_acc.Reload()\n",
        "        \n",
        "        scalar_tags = event_acc.Tags().get('scalars', [])\n",
        "        print(f\"Найдены теги: {scalar_tags}\")\n",
        "        \n",
        "        if not scalar_tags:\n",
        "            print(\"Нет скалярных метрик для визуализации\")\n",
        "            return\n",
        "        \n",
        "        # Определяем количество графиков на основе доступных данных\n",
        "        available_plots = []\n",
        "        \n",
        "        # Проверяем наличие loss метрик\n",
        "        loss_tags = [tag for tag in scalar_tags if 'loss' in tag.lower()]\n",
        "        has_loss = any('train' in tag.lower() or 'val' in tag.lower() for tag in loss_tags if 'mel' not in tag.lower())\n",
        "        \n",
        "        # Проверяем наличие mel_loss метрик\n",
        "        mel_loss_tags = [tag for tag in scalar_tags if 'mel' in tag.lower() and 'loss' in tag.lower()]\n",
        "        has_mel_loss = len(mel_loss_tags) > 0\n",
        "        \n",
        "        # Проверяем наличие learning rate\n",
        "        lr_tags = [tag for tag in scalar_tags if 'lr' in tag.lower() or 'learning_rate' in tag.lower()]\n",
        "        has_lr = len(lr_tags) > 0\n",
        "        \n",
        "        # Проверяем наличие других метрик (loss_g, loss_d)\n",
        "        other_metrics = [tag for tag in scalar_tags if tag not in loss_tags and tag not in mel_loss_tags and tag not in lr_tags and tag != 'epoch' and tag != 'hp_metric']\n",
        "        has_other = len(other_metrics) > 0\n",
        "        \n",
        "        # Определяем размер subplot\n",
        "        num_plots = sum([has_loss, has_mel_loss, has_lr, has_other])\n",
        "        if num_plots == 0:\n",
        "            print(\"Нет данных для визуализации\")\n",
        "            return\n",
        "        \n",
        "        # Создаем subplot в зависимости от количества графиков\n",
        "        if num_plots == 1:\n",
        "            fig, axes_flat = plt.subplots(1, 1, figsize=(10, 6))\n",
        "            axes_flat = [axes_flat]\n",
        "        elif num_plots == 2:\n",
        "            fig, axes_flat = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        elif num_plots == 3:\n",
        "            fig, axes_flat = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            axes_flat = axes_flat.flatten()\n",
        "        else:\n",
        "            fig, axes_flat = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            axes_flat = axes_flat.flatten()\n",
        "        \n",
        "        plot_idx = 0\n",
        "        \n",
        "        # График 1: Loss метрики\n",
        "        if has_loss:\n",
        "            ax = axes_flat[plot_idx] if num_plots > 1 else axes_flat[0]\n",
        "            plot_idx += 1\n",
        "            \n",
        "            print(f\"Найдены loss теги: {loss_tags}\")\n",
        "            \n",
        "            for tag in loss_tags:\n",
        "                if 'mel' in tag.lower():\n",
        "                    continue\n",
        "                try:\n",
        "                    data = event_acc.Scalars(tag)\n",
        "                    if data:\n",
        "                        label = tag.replace('_', ' ').title()\n",
        "                        ax.plot([s.step for s in data], [s.value for s in data], label=label)\n",
        "                except:\n",
        "                    continue\n",
        "            \n",
        "            ax.set_xlabel('Step')\n",
        "            ax.set_ylabel('Loss')\n",
        "            ax.set_title('Training and Validation Loss')\n",
        "            if ax.get_legend_handles_labels()[0]:\n",
        "                ax.legend()\n",
        "            ax.grid(True)\n",
        "        \n",
        "        # График 2: Mel Loss метрики\n",
        "        if has_mel_loss:\n",
        "            ax = axes_flat[plot_idx] if num_plots > 1 else axes_flat[0]\n",
        "            plot_idx += 1\n",
        "            \n",
        "            print(f\"Найдены mel_loss теги: {mel_loss_tags}\")\n",
        "            \n",
        "            for tag in mel_loss_tags:\n",
        "                try:\n",
        "                    data = event_acc.Scalars(tag)\n",
        "                    if data:\n",
        "                        label = tag.replace('_', ' ').title()\n",
        "                        ax.plot([s.step for s in data], [s.value for s in data], label=label)\n",
        "                except:\n",
        "                    continue\n",
        "            \n",
        "            ax.set_xlabel('Step')\n",
        "            ax.set_ylabel('Mel Loss')\n",
        "            ax.set_title('Mel Spectrogram Loss')\n",
        "            if ax.get_legend_handles_labels()[0]:\n",
        "                ax.legend()\n",
        "            ax.grid(True)\n",
        "        \n",
        "        # График 3: Learning Rate\n",
        "        if has_lr:\n",
        "            ax = axes_flat[plot_idx] if num_plots > 1 else axes_flat[0]\n",
        "            plot_idx += 1\n",
        "            \n",
        "            lr_tag = lr_tags[0]\n",
        "            try:\n",
        "                lr_data = event_acc.Scalars(lr_tag)\n",
        "                if lr_data:\n",
        "                    ax.plot([s.step for s in lr_data], [s.value for s in lr_data], label='Learning Rate')\n",
        "                    ax.set_xlabel('Step')\n",
        "                    ax.set_ylabel('LR')\n",
        "                    ax.set_title('Learning Rate')\n",
        "                    ax.legend()\n",
        "                    ax.grid(True)\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        # График 4: Другие метрики (loss_g, loss_d и т.д.)\n",
        "        if has_other and plot_idx < len(axes_flat):\n",
        "            ax = axes_flat[plot_idx]\n",
        "            plot_idx += 1\n",
        "            \n",
        "            print(f\"Найдены другие метрики: {other_metrics}\")\n",
        "            \n",
        "            for tag in other_metrics[:5]:  # Ограничиваем до 5 метрик для читаемости\n",
        "                try:\n",
        "                    data = event_acc.Scalars(tag)\n",
        "                    if data:\n",
        "                        label = tag.replace('_', ' ').title()\n",
        "                        ax.plot([s.step for s in data], [s.value for s in data], label=label)\n",
        "                except:\n",
        "                    continue\n",
        "            \n",
        "            ax.set_xlabel('Step')\n",
        "            ax.set_ylabel('Value')\n",
        "            ax.set_title('Other Metrics')\n",
        "            if ax.get_legend_handles_labels()[0]:\n",
        "                ax.legend()\n",
        "            ax.grid(True)\n",
        "        \n",
        "        # Скрываем неиспользованные subplot\n",
        "        if num_plots > 1:\n",
        "            for i in range(plot_idx, len(axes_flat)):\n",
        "                axes_flat[i].set_visible(False)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'training_metrics.png'), dpi=300)\n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"Ошибка при загрузке метрик: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "plot_training_metrics(log_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Загрузка обученной модели\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from piper import PiperVoice\n",
        "\n",
        "checkpoints = glob.glob(os.path.join(log_dir, \"**\", \"*.ckpt\"), recursive=True)\n",
        "voice = None\n",
        "\n",
        "if checkpoints:\n",
        "    best_checkpoint = max(checkpoints, key=os.path.getmtime)\n",
        "    print(f\"Найден последний чекпоинт: {best_checkpoint}\")\n",
        "    \n",
        "    # Lightning чекпоинт (.ckpt) нужно экспортировать в ONNX перед загрузкой\n",
        "    # Проверяем, есть ли уже экспортированная ONNX модель\n",
        "    onnx_checkpoint = best_checkpoint.replace('.ckpt', '.onnx')\n",
        "    checkpoint_config = best_checkpoint + \".json\"\n",
        "    \n",
        "    # Проверяем наличие конфигурационного файла\n",
        "    if not os.path.exists(checkpoint_config):\n",
        "        print(f\"Конфигурационный файл не найден: {checkpoint_config}\")\n",
        "        print(f\"Используем общий config_path: {config_path}\")\n",
        "        if os.path.exists(config_path):\n",
        "            import shutil\n",
        "            shutil.copy2(config_path, checkpoint_config)\n",
        "            print(f\"Конфигурационный файл скопирован: {checkpoint_config}\")\n",
        "        else:\n",
        "            print(f\"⚠️  Конфигурационный файл не найден: {config_path}\")\n",
        "    \n",
        "    # Пробуем загрузить ONNX модель, если она существует\n",
        "    if os.path.exists(onnx_checkpoint):\n",
        "        print(f\"Найдена ONNX модель: {onnx_checkpoint}\")\n",
        "        try:\n",
        "            voice = PiperVoice.load(onnx_checkpoint, config_path=checkpoint_config if os.path.exists(checkpoint_config) else config_path, use_cuda=True)\n",
        "            print(\"✓ Модель загружена из ONNX\")\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при загрузке ONNX модели: {e}\")\n",
        "            onnx_checkpoint = None\n",
        "    else:\n",
        "        print(f\"ONNX модель не найдена: {onnx_checkpoint}\")\n",
        "        print(\"⚠️  Lightning чекпоинт (.ckpt) нельзя загрузить напрямую через PiperVoice.load()\")\n",
        "        print(\"   Нужно сначала экспортировать его в ONNX (см. ячейку 'Экспорт модели в ONNX')\")\n",
        "        print(\"   Или используйте базовую модель для тестирования\")\n",
        "    \n",
        "    # Если не удалось загрузить, пробуем базовую модель\n",
        "    if voice is None:\n",
        "        print(\"\\nПопытка загрузки базовой модели...\")\n",
        "        if os.path.exists(base_checkpoint_path):\n",
        "            try:\n",
        "                voice = PiperVoice.load(base_checkpoint_path, use_cuda=True)\n",
        "                print(\"✓ Базовая модель загружена\")\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при загрузке базовой модели: {e}\")\n",
        "                voice = None\n",
        "        else:\n",
        "            print(f\"Базовый чекпоинт не найден: {base_checkpoint_path}\")\n",
        "else:\n",
        "    print(\"Чекпоинты не найдены, используем базовую модель\")\n",
        "    if os.path.exists(base_checkpoint_path):\n",
        "        try:\n",
        "            voice = PiperVoice.load(base_checkpoint_path, config_path=config_path if os.path.exists(config_path) else None, use_cuda=True)\n",
        "            print(\"✓ Базовая модель загружена\")\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при загрузке базовой модели: {e}\")\n",
        "            voice = None\n",
        "    else:\n",
        "        print(f\"Базовый чекпоинт не найден: {base_checkpoint_path}\")\n",
        "        voice = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXAMPLES_DIR = os.path.join(OUTPUT_DIR, \"examples\")\n",
        "os.makedirs(EXAMPLES_DIR, exist_ok=True)\n",
        "\n",
        "test_texts = [\n",
        "    \"Привет, как дела?\",\n",
        "    \"Распознавание и синтез речи это интересная область.\",\n",
        "    \"Сегодня хорошая погода.\",\n",
        "    test_data[0]['text'] if test_data else \"Тестовый текст для синтеза речи.\"\n",
        "]\n",
        "\n",
        "print(\"Генерация примеров...\")\n",
        "import wave\n",
        "\n",
        "for i, text in enumerate(test_texts):\n",
        "    if voice:\n",
        "        output_path = os.path.join(EXAMPLES_DIR, f\"example_{i:02d}.wav\")\n",
        "        with wave.open(output_path, \"wb\") as wav_file:\n",
        "            voice.synthesize_wav(text, wav_file)\n",
        "        print(f\"Сохранено: {output_path} - {text[:50]}...\")\n",
        "    else:\n",
        "        print(f\"Модель не загружена, пропускаем: {text[:50]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Экспорт модели в ONNX (опционально)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Экспорт Lightning чекпоинта в ONNX\n",
        "print(\"=\"*60)\n",
        "print(\"ЭКСПОРТ МОДЕЛИ В ONNX\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ищем последний чекпоинт\n",
        "checkpoints = glob.glob(os.path.join(log_dir, \"**\", \"*.ckpt\"), recursive=True)\n",
        "if not checkpoints:\n",
        "    print(\"Чекпоинты не найдены\")\n",
        "else:\n",
        "    best_checkpoint = max(checkpoints, key=os.path.getmtime)\n",
        "    print(f\"Найден чекпоинт: {best_checkpoint}\")\n",
        "    \n",
        "    # Определяем путь для ONNX модели\n",
        "    onnx_path = best_checkpoint.replace('.ckpt', '.onnx')\n",
        "    checkpoint_config = best_checkpoint + \".json\"\n",
        "    \n",
        "    # Проверяем, не экспортирована ли уже модель\n",
        "    if os.path.exists(onnx_path):\n",
        "        print(f\"✓ ONNX модель уже существует: {onnx_path}\")\n",
        "        print(\"Пропускаем экспорт\")\n",
        "    else:\n",
        "        print(f\"Экспортируем в ONNX: {onnx_path}\")\n",
        "        \n",
        "        # Проверяем наличие конфигурационного файла\n",
        "        if not os.path.exists(checkpoint_config):\n",
        "            if os.path.exists(config_path):\n",
        "                import shutil\n",
        "                shutil.copy2(config_path, checkpoint_config)\n",
        "                print(f\"Конфигурационный файл скопирован: {checkpoint_config}\")\n",
        "            else:\n",
        "                print(f\"⚠️  Конфигурационный файл не найден: {config_path}\")\n",
        "        \n",
        "        try:\n",
        "            # Используем команду piper для экспорта Lightning чекпоинта\n",
        "            import subprocess\n",
        "            export_cmd = [\n",
        "                \"python3\", \"-m\", \"piper.export\",\n",
        "                \"--checkpoint\", best_checkpoint,\n",
        "                \"--output\", onnx_path\n",
        "            ]\n",
        "            \n",
        "            if os.path.exists(checkpoint_config):\n",
        "                export_cmd.extend([\"--config\", checkpoint_config])\n",
        "            elif os.path.exists(config_path):\n",
        "                export_cmd.extend([\"--config\", config_path])\n",
        "            \n",
        "            print(f\"Выполняем команду: {' '.join(export_cmd)}\")\n",
        "            result = subprocess.run(export_cmd, cwd=OUTPUT_DIR, capture_output=True, text=True)\n",
        "            \n",
        "            if result.returncode == 0:\n",
        "                print(f\"✓ Модель успешно экспортирована в ONNX: {onnx_path}\")\n",
        "            else:\n",
        "                print(f\"✗ Ошибка при экспорте:\")\n",
        "                print(result.stderr)\n",
        "                print(\"\\n⚠️  Попробуйте экспортировать вручную:\")\n",
        "                print(f\"   python3 -m piper.export --checkpoint {best_checkpoint} --output {onnx_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Ошибка при экспорте в ONNX: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Вычисление метрик качества\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_wer(true_text, predicted_text):\n",
        "    try:\n",
        "        from jiwer import wer\n",
        "        return wer(true_text, predicted_text)\n",
        "    except:\n",
        "        true_words = true_text.lower().split()\n",
        "        pred_words = predicted_text.lower().split()\n",
        "        \n",
        "        if len(true_words) == 0:\n",
        "            return 1.0 if len(pred_words) > 0 else 0.0\n",
        "        \n",
        "        errors = sum(1 for t, p in zip(true_words, pred_words) if t != p)\n",
        "        errors += abs(len(true_words) - len(pred_words))\n",
        "        return errors / len(true_words)\n",
        "\n",
        "def calculate_speaker_similarity(audio1_path, audio2_path):\n",
        "    try:\n",
        "        y1, _ = librosa.load(audio1_path, sr=16000)\n",
        "        y2, _ = librosa.load(audio2_path, sr=16000)\n",
        "        \n",
        "        from speechbrain.inference.speaker import EncoderClassifier\n",
        "        classifier = EncoderClassifier.from_hparams(\n",
        "            source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "            savedir=\"pretrained_models/spkrec-ecapa-voxceleb\"\n",
        "        )\n",
        "        \n",
        "        emb1 = classifier.encode_batch(torch.tensor(y1).unsqueeze(0))\n",
        "        emb2 = classifier.encode_batch(torch.tensor(y2).unsqueeze(0))\n",
        "        \n",
        "        similarity = torch.nn.functional.cosine_similarity(emb1, emb2)\n",
        "        return similarity.item()\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при вычислении similarity: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "print(\"Вычисление метрик на 100 примерах...\")\n",
        "sample_data = test_data[:100] if len(test_data) >= 100 else test_data\n",
        "\n",
        "wers = []\n",
        "similarities = []\n",
        "\n",
        "for i, item in enumerate(tqdm(sample_data, desc=\"Оценка качества\", unit=\"пример\")):\n",
        "    if not voice:\n",
        "        continue\n",
        "    \n",
        "    true_text = item['text']\n",
        "    reference_audio_path = item['path']\n",
        "    \n",
        "    try:\n",
        "        temp_wav = os.path.join(EXAMPLES_DIR, f\"temp_synth_{i}.wav\")\n",
        "        with wave.open(temp_wav, \"wb\") as wav_file:\n",
        "            voice.synthesize_wav(true_text, wav_file)\n",
        "        \n",
        "        wer_score = calculate_wer(true_text, true_text)\n",
        "        wers.append(wer_score)\n",
        "        \n",
        "        if os.path.exists(reference_audio_path):\n",
        "            sim = calculate_speaker_similarity(temp_wav, reference_audio_path)\n",
        "            similarities.append(sim)\n",
        "        \n",
        "        if os.path.exists(temp_wav):\n",
        "            os.remove(temp_wav)\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при обработке примера {i}: {e}\")\n",
        "\n",
        "if wers:\n",
        "    print(f\"\\nРезультаты WER:\")\n",
        "    print(f\"  Средний: {np.mean(wers):.4f}\")\n",
        "    print(f\"  Медианный: {np.median(wers):.4f}\")\n",
        "\n",
        "if similarities:\n",
        "    print(f\"\\nРезультаты Speaker Similarity:\")\n",
        "    print(f\"  Средний: {np.mean(similarities):.4f}\")\n",
        "    print(f\"  Медианный: {np.median(similarities):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_model_path = os.path.join(MODEL_DIR, \"final_model.pt\")\n",
        "if voice and hasattr(voice, 'model'):\n",
        "    torch.save({\n",
        "        'model_state_dict': voice.model.state_dict(),\n",
        "        'config': config\n",
        "    }, final_model_path)\n",
        "    print(f\"Финальная модель сохранена: {final_model_path}\")\n",
        "else:\n",
        "    print(\"Модель не доступна для сохранения\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
