{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Домашнее задание №1: Распознавание речи\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install nvidia-ml-py\n",
        "%pip install --upgrade jupyter ipywidgets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Импорты\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Для работы с аудио\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "# NeMo\n",
        "import nemo.collections.asr as nemo_asr\n",
        "from nemo.core.config import hydra_runner\n",
        "from nemo.utils import logging\n",
        "\n",
        "# Для токенизации\n",
        "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Настройка для воспроизводимости\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Подготовка данных\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пути к датасетам\n",
        "SOVA_PATH = \"/share/audio_data/sova/ytub/raid/nanosemantics/nextcloud/sova_done\"\n",
        "SBER_GOLOS_PATH = \"/share/audio_data/sber-golos/tar/train\"\n",
        "\n",
        "# Выходные директории\n",
        "OUTPUT_DIR = \"./asr_data\"\n",
        "MANIFEST_DIR = os.path.join(OUTPUT_DIR, \"manifests\")\n",
        "TOKENIZER_DIR = os.path.join(OUTPUT_DIR, \"tokenizer\")\n",
        "\n",
        "os.makedirs(MANIFEST_DIR, exist_ok=True)\n",
        "os.makedirs(TOKENIZER_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_audio_info(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=None)\n",
        "        duration = len(y) / sr\n",
        "        return duration, sr\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке {audio_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^а-яёa-z0-9\\s]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "def process_sova_dataset(base_path, max_files=None):\n",
        "    manifest = []\n",
        "    audio_extensions = ['*.wav', '*.mp3', '*.flac', '*.ogg']\n",
        "    audio_files = []\n",
        "    for ext in audio_extensions:\n",
        "        audio_files.extend(glob.glob(os.path.join(base_path, '**', ext), recursive=True))\n",
        "    \n",
        "    if max_files:\n",
        "        audio_files = audio_files[:max_files]\n",
        "    \n",
        "    print(f\"Найдено {len(audio_files)} аудиофайлов в СОВА\")\n",
        "    print(\"Начинаю обработку...\")\n",
        "    \n",
        "    for audio_path in tqdm(audio_files, desc=\"Обработка СОВА\", unit=\"файл\"):\n",
        "        duration, sr = load_audio_info(audio_path)\n",
        "        if duration is None:\n",
        "            continue\n",
        "        \n",
        "        text_path = audio_path.rsplit('.', 1)[0] + '.txt'\n",
        "        if not os.path.exists(text_path):\n",
        "            continue\n",
        "        \n",
        "        with open(text_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "        \n",
        "        text = normalize_text(text)\n",
        "        \n",
        "        if len(text) > 0 and duration > 0.5:\n",
        "            manifest.append({\n",
        "                \"audio_filepath\": audio_path,\n",
        "                \"duration\": duration,\n",
        "                \"text\": text\n",
        "            })\n",
        "    \n",
        "    print(f\"Обработано {len(manifest)} записей из {len(audio_files)} файлов\")\n",
        "    return manifest\n",
        "\n",
        "def process_sber_golos_dataset(base_path, max_files=None):\n",
        "    manifest = []\n",
        "    manifest_path = os.path.join(base_path, 'manifest.json')\n",
        "    \n",
        "    if os.path.exists(manifest_path):\n",
        "        with open(manifest_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            \n",
        "        print(f\"Найдено {len(data)} записей в манифесте СберГолос\")\n",
        "        print(\"Начинаю обработку...\")\n",
        "        \n",
        "        for item in tqdm(data, desc=\"Обработка СберГолос\", unit=\"запись\"):\n",
        "            audio_path = item.get('audio_filepath') or item.get('audio')\n",
        "            if not audio_path:\n",
        "                continue\n",
        "            \n",
        "            if not os.path.isabs(audio_path):\n",
        "                audio_path = os.path.join(base_path, audio_path)\n",
        "            \n",
        "            duration, sr = load_audio_info(audio_path)\n",
        "            if duration is None:\n",
        "                continue\n",
        "            \n",
        "            text = item.get('text') or item.get('transcript', '')\n",
        "            text = normalize_text(text)\n",
        "            \n",
        "            if len(text) > 0 and duration > 0.5:\n",
        "                manifest.append({\n",
        "                    \"audio_filepath\": audio_path,\n",
        "                    \"duration\": duration,\n",
        "                    \"text\": text\n",
        "                })\n",
        "        \n",
        "        print(f\"Обработано {len(manifest)} записей из {len(data)} в манифесте\")\n",
        "    else:\n",
        "        audio_extensions = ['*.wav', '*.mp3', '*.flac', '*.ogg']\n",
        "        audio_files = []\n",
        "        for ext in audio_extensions:\n",
        "            audio_files.extend(glob.glob(os.path.join(base_path, '**', ext), recursive=True))\n",
        "        \n",
        "        if max_files:\n",
        "            audio_files = audio_files[:max_files]\n",
        "        \n",
        "        print(f\"Найдено {len(audio_files)} аудиофайлов в СберГолос\")\n",
        "        print(\"Начинаю обработку...\")\n",
        "        \n",
        "        for audio_path in tqdm(audio_files, desc=\"Обработка СберГолос\", unit=\"файл\"):\n",
        "            duration, sr = load_audio_info(audio_path)\n",
        "            if duration is None:\n",
        "                continue\n",
        "            \n",
        "            text_path = audio_path.rsplit('.', 1)[0] + '.txt'\n",
        "            if os.path.exists(text_path):\n",
        "                with open(text_path, 'r', encoding='utf-8') as f:\n",
        "                    text = f.read().strip()\n",
        "                text = normalize_text(text)\n",
        "                \n",
        "                if len(text) > 0 and duration > 0.5:\n",
        "                    manifest.append({\n",
        "                        \"audio_filepath\": audio_path,\n",
        "                        \"duration\": duration,\n",
        "                        \"text\": text\n",
        "                    })\n",
        "        \n",
        "        print(f\"Обработано {len(manifest)} записей из {len(audio_files)} файлов\")\n",
        "    \n",
        "    return manifest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Загрузка данных из СОВА...\")\n",
        "sova_manifest = process_sova_dataset(SOVA_PATH)\n",
        "\n",
        "print(\"\\nЗагрузка данных из СберГолос...\")\n",
        "sber_manifest = process_sber_golos_dataset(SBER_GOLOS_PATH)\n",
        "\n",
        "all_manifest = sova_manifest + sber_manifest\n",
        "\n",
        "print(f\"\\nВсего записей: {len(all_manifest)}\")\n",
        "print(f\"Из СОВА: {len(sova_manifest)}\")\n",
        "print(f\"Из СберГолос: {len(sber_manifest)}\")\n",
        "\n",
        "durations = [item['duration'] for item in all_manifest]\n",
        "print(f\"\\nСтатистика по длительности:\")\n",
        "print(f\"  Минимум: {min(durations):.2f} сек\")\n",
        "print(f\"  Максимум: {max(durations):.2f} сек\")\n",
        "print(f\"  Среднее: {np.mean(durations):.2f} сек\")\n",
        "print(f\"  Медиана: {np.median(durations):.2f} сек\")\n",
        "print(f\"  Общая длительность: {sum(durations)/3600:.2f} часов\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random.shuffle(all_manifest)\n",
        "\n",
        "train_size = int(0.8 * len(all_manifest))\n",
        "val_size = int(0.1 * len(all_manifest))\n",
        "\n",
        "train_manifest = all_manifest[:train_size]\n",
        "val_manifest = all_manifest[train_size:train_size + val_size]\n",
        "test_manifest = all_manifest[train_size + val_size:]\n",
        "\n",
        "print(f\"Разделение данных:\")\n",
        "print(f\"  Train: {len(train_manifest)} записей\")\n",
        "print(f\"  Val: {len(val_manifest)} записей\")\n",
        "print(f\"  Test: {len(test_manifest)} записей\")\n",
        "\n",
        "def save_manifest(manifest, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for item in manifest:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "save_manifest(train_manifest, os.path.join(MANIFEST_DIR, 'train_manifest.json'))\n",
        "save_manifest(val_manifest, os.path.join(MANIFEST_DIR, 'val_manifest.json'))\n",
        "save_manifest(test_manifest, os.path.join(MANIFEST_DIR, 'test_manifest.json'))\n",
        "\n",
        "print(\"\\nМанифесты сохранены!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Обучение токенизатора\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_texts = [item['text'] for item in train_manifest]\n",
        "\n",
        "text_file = os.path.join(TOKENIZER_DIR, 'train_texts.txt')\n",
        "with open(text_file, 'w', encoding='utf-8') as f:\n",
        "    for text in all_texts:\n",
        "        f.write(text + '\\n')\n",
        "\n",
        "print(f\"Собрано {len(all_texts)} текстов для обучения токенизатора\")\n",
        "print(f\"Общая длина текста: {sum(len(t) for t in all_texts)} символов\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 369\n",
        "\n",
        "tokenizer_model = os.path.join(TOKENIZER_DIR, 'tokenizer.model')\n",
        "\n",
        "SentencePieceTrainer.train(\n",
        "    input=text_file,\n",
        "    model_prefix=tokenizer_model.replace('.model', ''),\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    model_type='bpe',\n",
        "    character_coverage=0.9995,\n",
        "    max_sentencepiece_length=16,\n",
        "    split_by_whitespace=True,\n",
        "    byte_fallback=True,\n",
        "    normalization_rule_name='nmt_nfkc_cf',\n",
        "    user_defined_symbols=['<pad>', '<unk>', '<s>', '</s>']\n",
        ")\n",
        "\n",
        "print(f\"Токенизатор обучен! Размер словаря: {VOCAB_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = SentencePieceProcessor()\n",
        "tokenizer.load(tokenizer_model)\n",
        "\n",
        "print(f\"Размер словаря токенизатора: {tokenizer.get_piece_size()}\")\n",
        "\n",
        "test_texts = [\n",
        "    \"привет как дела\",\n",
        "    \"распознавание речи это интересно\",\n",
        "    all_texts[0] if all_texts else \"тестовый текст\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    tokens = tokenizer.encode(text, out_type=str)\n",
        "    decoded = tokenizer.decode(tokenizer.encode(text))\n",
        "    print(f\"\\nТекст: {text}\")\n",
        "    print(f\"Токены ({len(tokens)}): {tokens[:10]}...\" if len(tokens) > 10 else f\"Токены ({len(tokens)}): {tokens}\")\n",
        "    print(f\"Декодировано: {decoded}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Обучение модели\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    asr_model = nemo_asr.models.EncDecCTCModel.from_pretrained(\n",
        "        model_name=\"QuartzNet15x5Base-En\"\n",
        "    )\n",
        "    print(\"Загружена модель QuartzNet15x5Base-En\")\n",
        "except Exception as e:\n",
        "    print(f\"Не удалось загрузить предобученную модель: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "asr_model._cfg.train_ds.manifest_filepath = os.path.join(MANIFEST_DIR, 'train_manifest.json')\n",
        "asr_model._cfg.validation_ds.manifest_filepath = os.path.join(MANIFEST_DIR, 'val_manifest.json')\n",
        "\n",
        "asr_model._cfg.train_ds.sample_rate = 16000\n",
        "asr_model._cfg.validation_ds.sample_rate = 16000\n",
        "asr_model._cfg.train_ds.batch_size = 16\n",
        "asr_model._cfg.validation_ds.batch_size = 16\n",
        "\n",
        "print(\"Конфигурация модели обновлена\")\n",
        "print(f\"Train manifest: {asr_model._cfg.train_ds.manifest_filepath}\")\n",
        "print(f\"Val manifest: {asr_model._cfg.validation_ds.manifest_filepath}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Настройка Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "checkpoint_dir = os.path.join(OUTPUT_DIR, 'checkpoints')\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "logger = TensorBoardLogger(\n",
        "    save_dir=OUTPUT_DIR,\n",
        "    name='asr_training'\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=checkpoint_dir,\n",
        "    filename='asr-{epoch:02d}-{val_loss:.2f}',\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_top_k=3,\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    patience=5,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=50,\n",
        "    accelerator='gpu',\n",
        "    devices=1,\n",
        "    logger=logger,\n",
        "    callbacks=[checkpoint_callback, early_stopping],\n",
        "    log_every_n_steps=10,\n",
        "    val_check_interval=0.5,\n",
        "    gradient_clip_val=1.0,\n",
        "    accumulate_grad_batches=1\n",
        ")\n",
        "\n",
        "print(\"Trainer настроен\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Запуск обучения\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.fit(asr_model)\n",
        "\n",
        "asr_model.save_to(\"./asr_model.nemo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Анализ результатов\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "asr_model = nemo_asr.models.EncDecCTCModel.restore_from(\n",
        "    restore_path=\"./asr_model.nemo\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Визуализация метрик\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_metrics(log_dir):\n",
        "    try:\n",
        "        from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "        \n",
        "        event_acc = EventAccumulator(log_dir)\n",
        "        event_acc.Reload()\n",
        "        \n",
        "        scalar_tags = event_acc.Tags()['scalars']\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        \n",
        "        if 'train_loss' in scalar_tags:\n",
        "            train_loss = event_acc.Scalars('train_loss')\n",
        "            axes[0, 0].plot([s.step for s in train_loss], [s.value for s in train_loss], label='Train Loss')\n",
        "        \n",
        "        if 'val_loss' in scalar_tags:\n",
        "            val_loss = event_acc.Scalars('val_loss')\n",
        "            axes[0, 0].plot([s.step for s in val_loss], [s.value for s in val_loss], label='Val Loss')\n",
        "        \n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_title('Training and Validation Loss')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True)\n",
        "        \n",
        "        if 'val_wer' in scalar_tags:\n",
        "            val_wer = event_acc.Scalars('val_wer')\n",
        "            axes[0, 1].plot([s.step for s in val_wer], [s.value for s in val_wer], label='Val WER')\n",
        "            axes[0, 1].set_xlabel('Step')\n",
        "            axes[0, 1].set_ylabel('WER')\n",
        "            axes[0, 1].set_title('Word Error Rate')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'training_metrics.png'), dpi=300)\n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке метрик: {e}\")\n",
        "\n",
        "plot_training_metrics(logger.log_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Тестирование модели\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model, test_manifest, num_samples=10):\n",
        "    samples = random.sample(test_manifest, min(num_samples, len(test_manifest)))\n",
        "    results = []\n",
        "    \n",
        "    for sample in samples:\n",
        "        audio_path = sample['audio_filepath']\n",
        "        true_text = sample['text']\n",
        "        \n",
        "        try:\n",
        "            predicted_text = model.transcribe([audio_path])[0]\n",
        "            \n",
        "            results.append({\n",
        "                'audio': audio_path,\n",
        "                'true': true_text,\n",
        "                'predicted': predicted_text\n",
        "            })\n",
        "            \n",
        "            print(f\"\\nАудио: {os.path.basename(audio_path)}\")\n",
        "            print(f\"Истинный текст: {true_text}\")\n",
        "            print(f\"Распознанный текст: {predicted_text}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обработке {audio_path}: {e}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "test_results = test_model(asr_model, test_manifest, num_samples=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Вычисление метрик качества\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_wer(true_text, predicted_text):\n",
        "    try:\n",
        "        from jiwer import wer\n",
        "        return wer(true_text, predicted_text)\n",
        "    except:\n",
        "        true_words = true_text.lower().split()\n",
        "        pred_words = predicted_text.lower().split()\n",
        "        \n",
        "        if len(true_words) == 0:\n",
        "            return 1.0 if len(pred_words) > 0 else 0.0\n",
        "        \n",
        "        errors = sum(1 for t, p in zip(true_words, pred_words) if t != p)\n",
        "        errors += abs(len(true_words) - len(pred_words))\n",
        "        return errors / len(true_words)\n",
        "\n",
        "def calculate_metrics(model, test_manifest):\n",
        "    wers = []\n",
        "    \n",
        "    print(f\"Вычисление метрик на {len(test_manifest)} примерах...\")\n",
        "    \n",
        "    for i, sample in enumerate(test_manifest):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Обработано {i}/{len(test_manifest)}\")\n",
        "        \n",
        "        audio_path = sample['audio_filepath']\n",
        "        true_text = sample['text']\n",
        "        \n",
        "        try:\n",
        "            predicted_text = model.transcribe([audio_path])[0]\n",
        "            wer_score = calculate_wer(true_text, predicted_text)\n",
        "            wers.append(wer_score)\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обработке {audio_path}: {e}\")\n",
        "    \n",
        "    avg_wer = np.mean(wers) if wers else 0\n",
        "    \n",
        "    print(f\"\\nРезультаты:\")\n",
        "    print(f\"  Средний WER: {avg_wer:.4f}\")\n",
        "    print(f\"  Медианный WER: {np.median(wers):.4f}\")\n",
        "    print(f\"  Минимальный WER: {np.min(wers):.4f}\")\n",
        "    print(f\"  Максимальный WER: {np.max(wers):.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'avg_wer': avg_wer,\n",
        "        'median_wer': np.median(wers),\n",
        "        'min_wer': np.min(wers),\n",
        "        'max_wer': np.max(wers),\n",
        "        'all_wers': wers\n",
        "    }\n",
        "\n",
        "metrics = calculate_metrics(asr_model, test_manifest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
