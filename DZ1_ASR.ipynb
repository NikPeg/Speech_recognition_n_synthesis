{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Домашнее задание №1: Распознавание речи\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Установка зависимостей\n",
        "%pip install nvidia-ml-py\n",
        "%pip install --upgrade jupyter ipywidgets\n",
        "\n",
        "# Проверка установки nvidia-ml-py\n",
        "try:\n",
        "    import nvidia_ml_py3 as nvml\n",
        "    print(\"✓ nvidia-ml-py успешно установлен\")\n",
        "except ImportError:\n",
        "    print(\"⚠️  nvidia-ml-py не установлен, но это не критично\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Импорты\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Для работы с аудио\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "# NeMo\n",
        "import nemo.collections.asr as nemo_asr\n",
        "from nemo.core.config import hydra_runner\n",
        "from nemo.utils import logging\n",
        "\n",
        "# Для токенизации\n",
        "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "\n",
        "# Настройка для воспроизводимости\n",
        "SEED = 42\n",
        "\n",
        "# Устанавливаем seed для всех генераторов случайных чисел\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Для воспроизводимости на GPU\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Устанавливаем seed для Lightning через переменную окружения\n",
        "os.environ['PL_GLOBAL_SEED'] = str(SEED)\n",
        "\n",
        "print(f\"Seed установлен: {SEED}\")\n",
        "print(f\"PL_GLOBAL_SEED: {os.environ.get('PL_GLOBAL_SEED')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Подготовка данных\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пути к датасетам\n",
        "SOVA_PATH = \"/share/audio_data/sova/ytub/raid/nanosemantics/nextcloud/sova_done\"\n",
        "SBER_GOLOS_PATH = \"/share/audio_data/sber-golos/tar/train\"\n",
        "\n",
        "# Обрабатывать только указанные части из СОВА (для ускорения)\n",
        "# Можно добавить больше частей: ['part_0', 'part_1', 'part_2', 'part_3', ...]\n",
        "SOVA_PARTS = ['part_0', 'part_1', 'part_2']\n",
        "\n",
        "# Выходные директории\n",
        "OUTPUT_DIR = \"./asr_data\"\n",
        "MANIFEST_DIR = os.path.join(OUTPUT_DIR, \"manifests\")\n",
        "TOKENIZER_DIR = os.path.join(OUTPUT_DIR, \"tokenizer\")\n",
        "\n",
        "os.makedirs(MANIFEST_DIR, exist_ok=True)\n",
        "os.makedirs(TOKENIZER_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_audio_info(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=None)\n",
        "        duration = len(y) / sr\n",
        "        return duration, sr\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке {audio_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^а-яёa-z0-9\\s]', '', text)\n",
        "    text = text.strip()\n",
        "    return text if len(text) > 0 else None\n",
        "\n",
        "def process_sova_dataset(base_path, parts=None, max_files=None):\n",
        "    manifest = []\n",
        "    audio_extensions = ['.wav', '.mp3', '.flac', '.ogg']\n",
        "    audio_files = []\n",
        "    \n",
        "    print(\"Поиск аудиофайлов в СОВА...\")\n",
        "    search_start = time.time()\n",
        "    \n",
        "    if parts:\n",
        "        print(f\"Обработка частей: {parts}\")\n",
        "        for part in parts:\n",
        "            part_path = os.path.join(base_path, part)\n",
        "            if not os.path.exists(part_path):\n",
        "                print(f\"Предупреждение: часть {part} не найдена\")\n",
        "                continue\n",
        "            \n",
        "            print(f\"\\nОбработка {part}...\")\n",
        "            part_start = time.time()\n",
        "            \n",
        "            # Собираем все поддиректории для оценки прогресса\n",
        "            all_dirs = []\n",
        "            print(\"  Сканирование структуры директорий...\")\n",
        "            for root, dirs, files in os.walk(part_path):\n",
        "                all_dirs.append(root)\n",
        "            \n",
        "            print(f\"  Найдено {len(all_dirs)} поддиректорий\")\n",
        "            \n",
        "            # Поиск файлов с прогресс-баром\n",
        "            part_files = []\n",
        "            ext_counts = {ext: 0 for ext in audio_extensions}\n",
        "            \n",
        "            pbar_search = tqdm(all_dirs, desc=f\"Поиск в {part}\", unit=\"дир\", ncols=100, leave=False)\n",
        "            for root in pbar_search:\n",
        "                try:\n",
        "                    for file in os.listdir(root):\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        if os.path.isfile(file_path):\n",
        "                            file_ext = os.path.splitext(file)[1].lower()\n",
        "                            if file_ext in audio_extensions:\n",
        "                                part_files.append(file_path)\n",
        "                                ext_counts[file_ext] += 1\n",
        "                                pbar_search.set_postfix({\n",
        "                                    'найдено': len(part_files),\n",
        "                                    'wav': ext_counts['.wav'],\n",
        "                                    'mp3': ext_counts['.mp3']\n",
        "                                })\n",
        "                except (PermissionError, OSError) as e:\n",
        "                    continue\n",
        "            \n",
        "            audio_files.extend(part_files)\n",
        "            part_time = time.time() - part_start\n",
        "            print(f\"  {part}: найдено {len(part_files)} файлов за {part_time:.1f}с ({part_time/60:.1f} мин)\")\n",
        "    else:\n",
        "        # Обрабатываем все части (старый метод для совместимости)\n",
        "        for ext in ['*.wav', '*.mp3', '*.flac', '*.ogg']:\n",
        "            audio_files.extend(glob.glob(os.path.join(base_path, '**', ext), recursive=True))\n",
        "    \n",
        "    if max_files:\n",
        "        audio_files = audio_files[:max_files]\n",
        "    \n",
        "    search_time = time.time() - search_start\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Поиск завершен: найдено {len(audio_files)} аудиофайлов\")\n",
        "    print(f\"Время поиска: {search_time:.1f}с ({search_time/60:.1f} мин)\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    print(\"Начинаю обработку файлов...\")\n",
        "    start_time = time.time()\n",
        "    processed = 0\n",
        "    skipped_no_text = 0\n",
        "    skipped_duration = 0\n",
        "    \n",
        "    pbar = tqdm(audio_files, desc=\"Обработка СОВА\", unit=\"файл\", ncols=100)\n",
        "    for audio_path in pbar:\n",
        "        duration, sr = load_audio_info(audio_path)\n",
        "        if duration is None:\n",
        "            continue\n",
        "        \n",
        "        if duration < 0.5:\n",
        "            skipped_duration += 1\n",
        "            continue\n",
        "        \n",
        "        text_path = audio_path.rsplit('.', 1)[0] + '.txt'\n",
        "        if not os.path.exists(text_path):\n",
        "            skipped_no_text += 1\n",
        "            continue\n",
        "        \n",
        "        with open(text_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "        \n",
        "        text = normalize_text(text)\n",
        "        \n",
        "        if text and len(text) > 0:\n",
        "            manifest.append({\n",
        "                \"audio_filepath\": audio_path,\n",
        "                \"duration\": duration,\n",
        "                \"text\": text\n",
        "            })\n",
        "            processed += 1\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        pbar.set_postfix({\n",
        "            'Обработано': processed,\n",
        "            'Пропущено': skipped_no_text + skipped_duration,\n",
        "            'Время': f\"{elapsed:.1f}с\"\n",
        "        })\n",
        "    \n",
        "    elapsed_total = time.time() - start_time\n",
        "    print(f\"\\nОбработка завершена!\")\n",
        "    print(f\"  Обработано записей: {len(manifest)}\")\n",
        "    print(f\"  Пропущено (нет текста): {skipped_no_text}\")\n",
        "    print(f\"  Пропущено (длительность): {skipped_duration}\")\n",
        "    print(f\"  Время обработки: {elapsed_total:.1f} секунд ({elapsed_total/60:.1f} минут)\")\n",
        "    print(f\"  Скорость: {len(audio_files)/elapsed_total:.1f} файлов/сек\")\n",
        "    \n",
        "    return manifest\n",
        "\n",
        "def process_sber_golos_dataset(base_path, max_files=None):\n",
        "    manifest = []\n",
        "    manifest_path = os.path.join(base_path, 'manifest.json')\n",
        "    \n",
        "    if os.path.exists(manifest_path):\n",
        "        with open(manifest_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            \n",
        "        print(f\"Найдено {len(data)} записей в манифесте СберГолос\")\n",
        "        print(\"Начинаю обработку...\")\n",
        "        \n",
        "        for item in tqdm(data, desc=\"Обработка СберГолос\", unit=\"запись\"):\n",
        "            audio_path = item.get('audio_filepath') or item.get('audio')\n",
        "            if not audio_path:\n",
        "                continue\n",
        "            \n",
        "            if not os.path.isabs(audio_path):\n",
        "                audio_path = os.path.join(base_path, audio_path)\n",
        "            \n",
        "            duration, sr = load_audio_info(audio_path)\n",
        "            if duration is None:\n",
        "                continue\n",
        "            \n",
        "            text = item.get('text') or item.get('transcript', '')\n",
        "            text = normalize_text(text)\n",
        "            \n",
        "            if text and len(text) > 0 and duration > 0.5:\n",
        "                manifest.append({\n",
        "                    \"audio_filepath\": audio_path,\n",
        "                    \"duration\": duration,\n",
        "                    \"text\": text\n",
        "                })\n",
        "        \n",
        "        print(f\"Обработано {len(manifest)} записей из {len(data)} в манифесте\")\n",
        "    else:\n",
        "        audio_extensions = ['*.wav', '*.mp3', '*.flac', '*.ogg']\n",
        "        audio_files = []\n",
        "        for ext in audio_extensions:\n",
        "            audio_files.extend(glob.glob(os.path.join(base_path, '**', ext), recursive=True))\n",
        "        \n",
        "        if max_files:\n",
        "            audio_files = audio_files[:max_files]\n",
        "        \n",
        "        print(f\"Найдено {len(audio_files)} аудиофайлов в СберГолос\")\n",
        "        print(\"Начинаю обработку...\")\n",
        "        \n",
        "        for audio_path in tqdm(audio_files, desc=\"Обработка СберГолос\", unit=\"файл\"):\n",
        "            duration, sr = load_audio_info(audio_path)\n",
        "            if duration is None:\n",
        "                continue\n",
        "            \n",
        "            text_path = audio_path.rsplit('.', 1)[0] + '.txt'\n",
        "            if os.path.exists(text_path):\n",
        "                with open(text_path, 'r', encoding='utf-8') as f:\n",
        "                    text = f.read().strip()\n",
        "                text = normalize_text(text)\n",
        "                \n",
        "                if text and len(text) > 0 and duration > 0.5:\n",
        "                    manifest.append({\n",
        "                        \"audio_filepath\": audio_path,\n",
        "                        \"duration\": duration,\n",
        "                        \"text\": text\n",
        "                    })\n",
        "        \n",
        "        print(f\"Обработано {len(manifest)} записей из {len(audio_files)} файлов\")\n",
        "    \n",
        "    return manifest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ПОДГОТОВКА ДАННЫХ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nЗагрузка данных из СОВА...\")\n",
        "sova_manifest = process_sova_dataset(SOVA_PATH, parts=SOVA_PARTS)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Загрузка данных из СберГолос...\")\n",
        "print(\"=\" * 60)\n",
        "sber_manifest = process_sber_golos_dataset(SBER_GOLOS_PATH)\n",
        "\n",
        "all_manifest = sova_manifest + sber_manifest\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"ИТОГИ ЗАГРУЗКИ ДАННЫХ\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Всего записей: {len(all_manifest)}\")\n",
        "print(f\"Из СОВА: {len(sova_manifest)} ({100*len(sova_manifest)/len(all_manifest):.1f}%)\")\n",
        "print(f\"Из СберГолос: {len(sber_manifest)} ({100*len(sber_manifest)/len(all_manifest):.1f}%)\")\n",
        "\n",
        "print(\"\\nВычисление статистики по длительности...\")\n",
        "durations = [item['duration'] for item in tqdm(all_manifest, desc=\"Обработка длительностей\", unit=\"запись\", leave=False)]\n",
        "\n",
        "print(f\"\\nСтатистика по длительности:\")\n",
        "print(f\"  Минимум: {min(durations):.2f} сек\")\n",
        "print(f\"  Максимум: {max(durations):.2f} сек\")\n",
        "print(f\"  Среднее: {np.mean(durations):.2f} сек\")\n",
        "print(f\"  Медиана: {np.median(durations):.2f} сек\")\n",
        "print(f\"  Общая длительность: {sum(durations)/3600:.2f} часов\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ЗАГРУЗКА ДАННЫХ ЗАВЕРШЕНА\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"РАЗДЕЛЕНИЕ ДАННЫХ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"Перемешивание данных...\")\n",
        "random.shuffle(all_manifest)\n",
        "\n",
        "train_size = int(0.8 * len(all_manifest))\n",
        "val_size = int(0.1 * len(all_manifest))\n",
        "\n",
        "train_manifest = all_manifest[:train_size]\n",
        "val_manifest = all_manifest[train_size:train_size + val_size]\n",
        "test_manifest = all_manifest[train_size + val_size:]\n",
        "\n",
        "print(f\"\\nРазделение данных:\")\n",
        "print(f\"  Train: {len(train_manifest)} записей ({100*len(train_manifest)/len(all_manifest):.1f}%)\")\n",
        "print(f\"  Val: {len(val_manifest)} записей ({100*len(val_manifest)/len(all_manifest):.1f}%)\")\n",
        "print(f\"  Test: {len(test_manifest)} записей ({100*len(test_manifest)/len(all_manifest):.1f}%)\")\n",
        "\n",
        "def save_manifest(manifest, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for item in tqdm(manifest, desc=f\"Сохранение {os.path.basename(path)}\", unit=\"запись\", leave=False):\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(\"\\nСохранение манифестов...\")\n",
        "save_manifest(train_manifest, os.path.join(MANIFEST_DIR, 'train_manifest.json'))\n",
        "save_manifest(val_manifest, os.path.join(MANIFEST_DIR, 'val_manifest.json'))\n",
        "save_manifest(test_manifest, os.path.join(MANIFEST_DIR, 'test_manifest.json'))\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Манифесты сохранены!\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Обучение токенизатора\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_texts = [item['text'] for item in train_manifest]\n",
        "\n",
        "text_file = os.path.join(TOKENIZER_DIR, 'train_texts.txt')\n",
        "with open(text_file, 'w', encoding='utf-8') as f:\n",
        "    for text in all_texts:\n",
        "        f.write(text + '\\n')\n",
        "\n",
        "print(f\"Собрано {len(all_texts)} текстов для обучения токенизатора\")\n",
        "print(f\"Общая длина текста: {sum(len(t) for t in all_texts)} символов\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 369\n",
        "\n",
        "tokenizer_model = os.path.join(TOKENIZER_DIR, 'tokenizer.model')\n",
        "\n",
        "SentencePieceTrainer.train(\n",
        "    input=text_file,\n",
        "    model_prefix=tokenizer_model.replace('.model', ''),\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    model_type='bpe',\n",
        "    character_coverage=0.9995,\n",
        "    max_sentencepiece_length=16,\n",
        "    split_by_whitespace=True,\n",
        "    byte_fallback=True,\n",
        "    normalization_rule_name='nmt_nfkc_cf',\n",
        "    user_defined_symbols=['<pad>', '<unk>', '<s>', '</s>']\n",
        ")\n",
        "\n",
        "print(f\"Токенизатор обучен! Размер словаря: {VOCAB_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = SentencePieceProcessor()\n",
        "tokenizer.load(tokenizer_model)\n",
        "\n",
        "print(f\"Размер словаря токенизатора: {tokenizer.get_piece_size()}\")\n",
        "\n",
        "test_texts = [\n",
        "    \"привет как дела\",\n",
        "    \"распознавание речи это интересно\",\n",
        "    all_texts[0] if all_texts else \"тестовый текст\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    tokens = tokenizer.encode(text, out_type=str)\n",
        "    decoded = tokenizer.decode(tokenizer.encode(text))\n",
        "    print(f\"\\nТекст: {text}\")\n",
        "    print(f\"Токены ({len(tokens)}): {tokens[:10]}...\" if len(tokens) > 10 else f\"Токены ({len(tokens)}): {tokens}\")\n",
        "    print(f\"Декодировано: {decoded}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Обучение модели\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    asr_model = nemo_asr.models.EncDecCTCModel.from_pretrained(\n",
        "        model_name=\"QuartzNet15x5Base-En\"\n",
        "    )\n",
        "    print(\"Загружена модель QuartzNet15x5Base-En\")\n",
        "except Exception as e:\n",
        "    print(f\"Не удалось загрузить предобученную модель: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "asr_model._cfg.train_ds.manifest_filepath = os.path.join(MANIFEST_DIR, 'train_manifest.json')\n",
        "asr_model._cfg.validation_ds.manifest_filepath = os.path.join(MANIFEST_DIR, 'val_manifest.json')\n",
        "\n",
        "asr_model._cfg.train_ds.sample_rate = 16000\n",
        "asr_model._cfg.validation_ds.sample_rate = 16000\n",
        "asr_model._cfg.train_ds.batch_size = 16\n",
        "asr_model._cfg.validation_ds.batch_size = 16\n",
        "\n",
        "print(\"Конфигурация модели обновлена\")\n",
        "print(f\"Train manifest: {asr_model._cfg.train_ds.manifest_filepath}\")\n",
        "print(f\"Val manifest: {asr_model._cfg.validation_ds.manifest_filepath}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Настройка Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "checkpoint_dir = os.path.join(OUTPUT_DIR, 'checkpoints')\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "logger = TensorBoardLogger(\n",
        "    save_dir=OUTPUT_DIR,\n",
        "    name='asr_training'\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=checkpoint_dir,\n",
        "    filename='asr-{epoch:02d}-{val_loss:.2f}',\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_top_k=3,\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    patience=5,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=50,\n",
        "    accelerator='gpu',\n",
        "    devices=1,\n",
        "    logger=logger,\n",
        "    callbacks=[checkpoint_callback, early_stopping],\n",
        "    log_every_n_steps=10,\n",
        "    val_check_interval=0.5,\n",
        "    gradient_clip_val=1.0,\n",
        "    accumulate_grad_batches=1,\n",
        "    deterministic=True,\n",
        "    benchmark=False\n",
        ")\n",
        "\n",
        "print(\"Trainer настроен\")\n",
        "print(f\"Seed: {SEED}\")\n",
        "print(f\"Deterministic: {trainer.deterministic}\")\n",
        "print(f\"Benchmark: {trainer.benchmark}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Запуск обучения\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.fit(asr_model)\n",
        "\n",
        "asr_model.save_to(\"./asr_model.nemo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Анализ результатов\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "asr_model = nemo_asr.models.EncDecCTCModel.restore_from(\n",
        "    restore_path=\"./asr_model.nemo\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Визуализация метрик\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_metrics(log_dir):\n",
        "    try:\n",
        "        from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "        \n",
        "        # Ищем логи в поддиректориях (lightning_logs/version_*/)\n",
        "        version_dirs = []\n",
        "        if os.path.exists(log_dir):\n",
        "            for item in os.listdir(log_dir):\n",
        "                item_path = os.path.join(log_dir, item)\n",
        "                if os.path.isdir(item_path) and (item.startswith('version_') or 'events.out.tfevents' in os.listdir(item_path) if os.path.isdir(item_path) else False):\n",
        "                    version_dirs.append(item_path)\n",
        "        \n",
        "        if not version_dirs:\n",
        "            # Пробуем использовать log_dir напрямую\n",
        "            version_dirs = [log_dir]\n",
        "        \n",
        "        latest_version = max(version_dirs) if version_dirs else log_dir\n",
        "        event_dir = latest_version\n",
        "        \n",
        "        event_acc = EventAccumulator(event_dir)\n",
        "        event_acc.Reload()\n",
        "        \n",
        "        scalar_tags = event_acc.Tags()['scalars']\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        \n",
        "        # Loss\n",
        "        if 'train/loss' in scalar_tags or 'train_loss' in scalar_tags:\n",
        "            tag = 'train/loss' if 'train/loss' in scalar_tags else 'train_loss'\n",
        "            train_loss = event_acc.Scalars(tag)\n",
        "            axes[0, 0].plot([s.step for s in train_loss], [s.value for s in train_loss], label='Train Loss')\n",
        "        \n",
        "        if 'val/loss' in scalar_tags or 'val_loss' in scalar_tags:\n",
        "            tag = 'val/loss' if 'val/loss' in scalar_tags else 'val_loss'\n",
        "            val_loss = event_acc.Scalars(tag)\n",
        "            axes[0, 0].plot([s.step for s in val_loss], [s.value for s in val_loss], label='Val Loss')\n",
        "        \n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_title('Training and Validation Loss')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True)\n",
        "        \n",
        "        # WER\n",
        "        if 'val/wer' in scalar_tags or 'val_wer' in scalar_tags:\n",
        "            tag = 'val/wer' if 'val/wer' in scalar_tags else 'val_wer'\n",
        "            val_wer = event_acc.Scalars(tag)\n",
        "            axes[0, 1].plot([s.step for s in val_wer], [s.value for s in val_wer], label='Val WER', color='green')\n",
        "            axes[0, 1].set_xlabel('Step')\n",
        "            axes[0, 1].set_ylabel('WER')\n",
        "            axes[0, 1].set_title('Word Error Rate')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True)\n",
        "        \n",
        "        # Learning Rate (если есть)\n",
        "        if 'train/learning_rate' in scalar_tags or 'learning_rate' in scalar_tags:\n",
        "            tag = 'train/learning_rate' if 'train/learning_rate' in scalar_tags else 'learning_rate'\n",
        "            lr = event_acc.Scalars(tag)\n",
        "            axes[1, 0].plot([s.step for s in lr], [s.value for s in lr], label='Learning Rate', color='orange')\n",
        "            axes[1, 0].set_xlabel('Step')\n",
        "            axes[1, 0].set_ylabel('LR')\n",
        "            axes[1, 0].set_title('Learning Rate')\n",
        "            axes[1, 0].legend()\n",
        "            axes[1, 0].grid(True)\n",
        "            axes[1, 0].set_yscale('log')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'training_metrics.png'), dpi=300)\n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке метрик: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "plot_training_metrics(logger.log_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Тестирование модели\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model, test_manifest, num_samples=10):\n",
        "    samples = random.sample(test_manifest, min(num_samples, len(test_manifest)))\n",
        "    results = []\n",
        "    \n",
        "    for sample in samples:\n",
        "        audio_path = sample['audio_filepath']\n",
        "        true_text = sample['text']\n",
        "        \n",
        "        try:\n",
        "            predicted_text = model.transcribe([audio_path])[0]\n",
        "            \n",
        "            results.append({\n",
        "                'audio': audio_path,\n",
        "                'true': true_text,\n",
        "                'predicted': predicted_text\n",
        "            })\n",
        "            \n",
        "            print(f\"\\nАудио: {os.path.basename(audio_path)}\")\n",
        "            print(f\"Истинный текст: {true_text}\")\n",
        "            print(f\"Распознанный текст: {predicted_text}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обработке {audio_path}: {e}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "test_results = test_model(asr_model, test_manifest, num_samples=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Вычисление метрик качества\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_wer(true_text, predicted_text):\n",
        "    try:\n",
        "        from jiwer import wer\n",
        "        return wer(true_text, predicted_text)\n",
        "    except:\n",
        "        true_words = true_text.lower().split()\n",
        "        pred_words = predicted_text.lower().split()\n",
        "        \n",
        "        if len(true_words) == 0:\n",
        "            return 1.0 if len(pred_words) > 0 else 0.0\n",
        "        \n",
        "        errors = sum(1 for t, p in zip(true_words, pred_words) if t != p)\n",
        "        errors += abs(len(true_words) - len(pred_words))\n",
        "        return errors / len(true_words)\n",
        "\n",
        "def calculate_metrics(model, test_manifest):\n",
        "    wers = []\n",
        "    errors = 0\n",
        "    \n",
        "    print(f\"Вычисление метрик на {len(test_manifest)} примерах...\")\n",
        "    \n",
        "    for sample in tqdm(test_manifest, desc=\"Оценка качества\", unit=\"пример\", ncols=100):\n",
        "        audio_path = sample['audio_filepath']\n",
        "        true_text = sample['text']\n",
        "        \n",
        "        try:\n",
        "            predicted_text = model.transcribe([audio_path])[0]\n",
        "            wer_score = calculate_wer(true_text, predicted_text)\n",
        "            wers.append(wer_score)\n",
        "        except Exception as e:\n",
        "            errors += 1\n",
        "            if errors <= 5:  # Показываем только первые 5 ошибок\n",
        "                print(f\"\\nОшибка при обработке {os.path.basename(audio_path)}: {e}\")\n",
        "    \n",
        "    if not wers:\n",
        "        print(\"✗ Не удалось обработать ни одного примера\")\n",
        "        return {\n",
        "            'avg_wer': 1.0,\n",
        "            'median_wer': 1.0,\n",
        "            'min_wer': 1.0,\n",
        "            'max_wer': 1.0,\n",
        "            'all_wers': []\n",
        "        }\n",
        "    \n",
        "    avg_wer = np.mean(wers)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"РЕЗУЛЬТАТЫ ОЦЕНКИ\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Обработано успешно: {len(wers)}/{len(test_manifest)}\")\n",
        "    if errors > 0:\n",
        "        print(f\"Ошибок: {errors}\")\n",
        "    print(f\"\\nМетрики WER:\")\n",
        "    print(f\"  Средний WER: {avg_wer:.4f}\")\n",
        "    print(f\"  Медианный WER: {np.median(wers):.4f}\")\n",
        "    print(f\"  Минимальный WER: {np.min(wers):.4f}\")\n",
        "    print(f\"  Максимальный WER: {np.max(wers):.4f}\")\n",
        "    print(f\"  Стандартное отклонение: {np.std(wers):.4f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    return {\n",
        "        'avg_wer': avg_wer,\n",
        "        'median_wer': np.median(wers),\n",
        "        'min_wer': np.min(wers),\n",
        "        'max_wer': np.max(wers),\n",
        "        'std_wer': np.std(wers),\n",
        "        'all_wers': wers,\n",
        "        'errors': errors\n",
        "    }\n",
        "\n",
        "metrics = calculate_metrics(asr_model, test_manifest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
