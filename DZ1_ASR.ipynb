{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ ‚Ññ1: –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ (ASR)\n",
    "\n",
    "## –ß–∞—Å—Ç—å 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "\n",
    "**–¶–µ–ª—å:** –û—Å–≤–æ–∏—Ç—å —ç—Ç–∞–ø—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è ASR.\n",
    "\n",
    "**–ó–∞–¥–∞—á–∏:**\n",
    "1. –†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (372(-3) = 369 —Ç–æ–∫–µ–Ω–æ–≤)\n",
    "2. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –°–û–í–ê –∏ –°–±–µ—Ä–ì–æ–ª–æ—Å\n",
    "3. –°–æ–∑–¥–∞—Ç—å –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –¥–ª—è NeMo\n",
    "\n",
    "## –ß–∞—Å—Ç—å 2: –û–±—É—á–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏ ASR\n",
    "\n",
    "**–¶–µ–ª—å:** –ó–∞–∫—Ä–µ–ø–∏—Ç—å –Ω–∞–≤—ã–∫–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è ASR.\n",
    "\n",
    "**–ó–∞–¥–∞—á–∏:**\n",
    "1. –û–±—É—á–∏—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –≤ –ø–∞–∫–µ—Ç–µ NeMo\n",
    "2. –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "3. –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è, –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ (loss, accuracy, WER)\n",
    "4. –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "5. –ü—Ä–æ–≤–µ—Å—Ç–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "6. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ .nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ NeMo –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "%pip install nemo_toolkit[all] -q\n",
    "%pip install setuptools wheel -q\n",
    "%pip install tensorboard -q\n",
    "%pip install jiwer -q  # –î–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è WER\n",
    "%pip install tokenizers -q  # –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "%pip install nvidia-ml-py -q  # –î–ª—è –∑–∞–º–µ–Ω—ã —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ pynvml\n",
    "\n",
    "print(\"‚úì –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "# NeMo –∏–º–ø–æ—Ä—Ç—ã\n",
    "try:\n",
    "    import nemo\n",
    "    import nemo.collections.asr as nemo_asr\n",
    "    from nemo.collections.asr.models import ASRModel\n",
    "    from nemo.core.config import hydra_runner\n",
    "    from nemo.utils import logging\n",
    "    print(\"‚úì NeMo —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ NeMo: {e}\")\n",
    "    print(\"   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ NeMo —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: pip install nemo_toolkit[all]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ['PL_GLOBAL_SEED'] = str(SEED)\n",
    "\n",
    "print(f\"‚úì Seed —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {SEED}\")\n",
    "print(f\"‚úì PL_GLOBAL_SEED: {os.environ.get('PL_GLOBAL_SEED')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 1: –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "\n",
    "### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: **372(-3) = 369 —Ç–æ–∫–µ–Ω–æ–≤**\n",
    "- –°—Ç—Ä–∞—Ç–µ–≥–∏—è: BPE (Byte Pair Encoding) –∏–ª–∏ Unigram Language Model\n",
    "- –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–±—É—á–µ–Ω –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –°–û–í–ê –∏ –°–±–µ—Ä–ì–æ–ª–æ—Å\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "# –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "TARGET_VOCAB_SIZE = 369  # 372 - 3\n",
    "\n",
    "print(f\"–¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {TARGET_VOCAB_SIZE} —Ç–æ–∫–µ–Ω–æ–≤\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_for_asr(text: str) -> str:\n",
    "    \"\"\"–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è ASR.\"\"\"\n",
    "    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    text = text.lower().strip()\n",
    "    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = re.sub(r'[^–∞-—è—ëa-z0-9\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_texts_from_manifests(manifest_paths: List[str]) -> List[str]:\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –∏–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤ NeMo.\"\"\"\n",
    "    texts = []\n",
    "    for manifest_path in manifest_paths:\n",
    "        if not os.path.exists(manifest_path):\n",
    "            print(f\"‚ö†Ô∏è  –ú–∞–Ω–∏—Ñ–µ—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {manifest_path}\")\n",
    "            continue\n",
    "        \n",
    "        with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    text = data.get('text', '')\n",
    "                    if text:\n",
    "                        texts.append(normalize_text_for_asr(text))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    return texts\n",
    "\n",
    "print(\"‚úì –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤ –≥–æ—Ç–æ–≤—ã\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_tokenizer(texts: List[str], vocab_size: int = TARGET_VOCAB_SIZE, output_path: Optional[str] = None) -> Tokenizer:\n",
    "    \"\"\"–û–±—É—á–∞–µ—Ç BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö.\"\"\"\n",
    "    # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\"]\n",
    "    )\n",
    "    \n",
    "    # –û–±—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "    print(f\"–û–±—É—á–µ–Ω–∏–µ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ {len(texts)} —Ç–µ–∫—Å—Ç–∞—Ö...\")\n",
    "    tokenizer.train_from_iterator(texts, trainer)\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "    if output_path:\n",
    "        tokenizer.save(output_path)\n",
    "        print(f\"‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "print(\"‚úì –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≥–æ—Ç–æ–≤–∞\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_info(audio_path: str) -> Tuple[Optional[float], Optional[int], Optional[np.ndarray]]:\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞—É–¥–∏–æ—Ñ–∞–π–ª–µ.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        duration = len(y) / sr\n",
    "        return duration, sr, y\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {audio_path}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def prepare_manifest_from_dataset(\n",
    "    dataset_path: str,\n",
    "    output_manifest: str,\n",
    "    dataset_name: str = \"dataset\",\n",
    "    max_files: Optional[int] = None,\n",
    "    audio_extensions: List[str] = ['.wav', '.mp3', '.flac', '.ogg']\n",
    ") -> int:\n",
    "    \"\"\"–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –º–∞–Ω–∏—Ñ–µ—Å—Ç NeMo –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞.\"\"\"\n",
    "    print(f\"\\n–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_name}\")\n",
    "    print(f\"–ü—É—Ç—å: {dataset_path}\")\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"‚ö†Ô∏è  –ü—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {dataset_path}\")\n",
    "        return 0\n",
    "    \n",
    "    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª—ã\n",
    "    audio_files = []\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in audio_extensions):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    \n",
    "    if max_files:\n",
    "        audio_files = audio_files[:max_files]\n",
    "    \n",
    "    print(f\"–ù–∞–π–¥–µ–Ω–æ {len(audio_files)} –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤\")\n",
    "    \n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã\n",
    "    manifest_data = []\n",
    "    processed = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    for audio_file in tqdm(audio_files, desc=f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ {dataset_name}\"):\n",
    "        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª\n",
    "        base_name = os.path.splitext(audio_file)[0]\n",
    "        text_file = base_name + '.txt'\n",
    "        \n",
    "        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã\n",
    "        if not os.path.exists(text_file):\n",
    "            parent_dir = os.path.dirname(audio_file)\n",
    "            text_file = os.path.join(parent_dir, os.path.basename(base_name) + '.txt')\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ\n",
    "        duration, sr, audio = load_audio_info(audio_file)\n",
    "        if duration is None or duration < MIN_DURATION or duration > MAX_DURATION:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç\n",
    "        text = \"\"\n",
    "        if os.path.exists(text_file):\n",
    "            try:\n",
    "                with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read().strip()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª\n",
    "        if not text:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç\n",
    "        text = normalize_text_for_asr(text)\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç\n",
    "        manifest_data.append({\n",
    "            \"audio_filepath\": os.path.abspath(audio_file),\n",
    "            \"duration\": duration,\n",
    "            \"text\": text\n",
    "        })\n",
    "        processed += 1\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç\n",
    "    with open(output_manifest, 'w', encoding='utf-8') as f:\n",
    "        for item in manifest_data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}, –ø—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}\")\n",
    "    print(f\"‚úì –ú–∞–Ω–∏—Ñ–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_manifest}\")\n",
    "    \n",
    "    return processed\n",
    "\n",
    "print(\"‚úì –§—É–Ω–∫—Ü–∏–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≥–æ—Ç–æ–≤—ã\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_DIR –¥–µ–ª–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–º, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø—É—Ç–µ–π\n",
    "OUTPUT_DIR_REL = \"./asr_data\"\n",
    "OUTPUT_DIR = os.path.abspath(os.path.normpath(OUTPUT_DIR_REL))\n",
    "\n",
    "# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö\n",
    "MANIFEST_DIR = os.path.join(OUTPUT_DIR, \"manifests\")\n",
    "TOKENIZER_DIR = os.path.join(OUTPUT_DIR, \"tokenizer\")\n",
    "MODEL_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
    "TEST_DIR = os.path.join(OUTPUT_DIR, \"test\")\n",
    "EXAMPLES_DIR = os.path.join(OUTPUT_DIR, \"examples\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "for dir_path in [OUTPUT_DIR, MANIFEST_DIR, TOKENIZER_DIR, MODEL_DIR, TEST_DIR, EXAMPLES_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n",
    "print(f\"–¢–µ–∫—É—â–∞—è —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}\")\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—É–¥–∏–æ\n",
    "SAMPLE_RATE = 16000  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è ASR\n",
    "MIN_DURATION = 0.5\n",
    "MAX_DURATION = 30.0\n",
    "\n",
    "# ============================================================================\n",
    "# –ü–û–ò–°–ö –ì–û–¢–û–í–´–• –ú–ê–ù–ò–§–ï–°–¢–û–í (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"–ü–û–ò–°–ö –ì–û–¢–û–í–´–• –ú–ê–ù–ò–§–ï–°–¢–û–í\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# –í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è –≥–æ—Ç–æ–≤—ã—Ö –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤\n",
    "POSSIBLE_MANIFEST_LOCATIONS = [\n",
    "    # –ì–æ—Ç–æ–≤—ã–µ train/val –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã\n",
    "    {\n",
    "        'train': '/tf/sishmuratov/manifests/train.json',\n",
    "        'val': '/tf/sishmuratov/manifests/val.json',\n",
    "        'name': 'sishmuratov/manifests'\n",
    "    },\n",
    "    # –û—Ç–¥–µ–ª—å–Ω—ã–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –°–û–í–ê –∏ –°–±–µ—Ä–ì–æ–ª–æ—Å\n",
    "    {\n",
    "        'sova': '/tf/nspeganov/ASR/sova_manifest1.json',\n",
    "        'golos': '/tf/nspeganov/ASR/golos_manifest.json',\n",
    "        'name': 'nspeganov/ASR'\n",
    "    },\n",
    "    {\n",
    "        'sova': '/tf/minazarko/asr/sem3ft/sova_manifest1.json',\n",
    "        'golos': '/tf/minazarko/asr/golos_manifest.json',\n",
    "        'name': 'minazarko/asr'\n",
    "    }\n",
    "]\n",
    "\n",
    "# –ò—â–µ–º –≥–æ—Ç–æ–≤—ã–µ train/val –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã\n",
    "train_manifest = None\n",
    "val_manifest = None\n",
    "\n",
    "for location in POSSIBLE_MANIFEST_LOCATIONS:\n",
    "    if 'train' in location and 'val' in location:\n",
    "        if os.path.exists(location['train']) and os.path.exists(location['val']):\n",
    "            train_manifest = location['train']\n",
    "            val_manifest = location['val']\n",
    "            print(f\"‚úì –ù–∞–π–¥–µ–Ω—ã –≥–æ—Ç–æ–≤—ã–µ train/val –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –≤: {location['name']}\")\n",
    "            print(f\"  Train: {train_manifest}\")\n",
    "            print(f\"  Val: {val_manifest}\")\n",
    "            break\n",
    "\n",
    "# –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≥–æ—Ç–æ–≤—ã–µ train/val, –∏—â–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –°–û–í–ê –∏ –°–±–µ—Ä–ì–æ–ª–æ—Å\n",
    "sova_manifest_found = None\n",
    "golos_manifest_found = None\n",
    "\n",
    "if not train_manifest or not val_manifest:\n",
    "    print(\"\\n–ò—â–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –°–û–í–ê –∏ –°–±–µ—Ä–ì–æ–ª–æ—Å...\")\n",
    "    for location in POSSIBLE_MANIFEST_LOCATIONS:\n",
    "        if 'sova' in location and 'golos' in location:\n",
    "            if os.path.exists(location['sova']) and os.path.exists(location['golos']):\n",
    "                sova_manifest_found = location['sova']\n",
    "                golos_manifest_found = location['golos']\n",
    "                print(f\"‚úì –ù–∞–π–¥–µ–Ω—ã –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –≤: {location['name']}\")\n",
    "                print(f\"  –°–û–í–ê: {sova_manifest_found}\")\n",
    "                print(f\"  –°–±–µ—Ä–ì–æ–ª–æ—Å: {golos_manifest_found}\")\n",
    "                break\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã)\n",
    "# –õ–æ–∫–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)\n",
    "LOCAL_SOVA_PATH = \"/tf/nspeganov/sova_data\"\n",
    "LOCAL_SBER_PATH = \"/tf/nspeganov/sber_golos_data\"\n",
    "\n",
    "# Share –ø—É—Ç–∏ (fallback)\n",
    "SOVA_PATH = \"/share/audio_data/sova/ytub/raid/nanosemantics/nextcloud/sova_done\"\n",
    "SBER_GOLOS_PATH = \"/share/audio_data/sber-golos/tar/train\"\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –ø—É—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (–µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è)\n",
    "if os.path.exists(LOCAL_SOVA_PATH):\n",
    "    SOVA_PATH = LOCAL_SOVA_PATH\n",
    "    print(f\"\\n‚úì –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –°–û–í–ê –Ω–∞–π–¥–µ–Ω: {SOVA_PATH}\")\n",
    "elif os.path.exists(SOVA_PATH):\n",
    "    print(f\"\\n‚ö†Ô∏è  –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –°–û–í–ê –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º share: {SOVA_PATH}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  –ü—É—Ç—å –°–û–í–ê –Ω–µ –Ω–∞–π–¥–µ–Ω: {SOVA_PATH}\")\n",
    "\n",
    "if os.path.exists(LOCAL_SBER_PATH):\n",
    "    SBER_GOLOS_PATH = LOCAL_SBER_PATH\n",
    "    print(f\"‚úì –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –°–±–µ—Ä–ì–æ–ª–æ—Å –Ω–∞–π–¥–µ–Ω: {SBER_GOLOS_PATH}\")\n",
    "elif os.path.exists(SBER_GOLOS_PATH):\n",
    "    print(f\"‚ö†Ô∏è  –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –°–±–µ—Ä–ì–æ–ª–æ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º share: {SBER_GOLOS_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  –ü—É—Ç—å –°–±–µ—Ä–ì–æ–ª–æ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {SBER_GOLOS_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# –ü–û–î–ì–û–¢–û–í–ö–ê –ú–ê–ù–ò–§–ï–°–¢–û–í\n",
    "# ============================================================================\n",
    "\n",
    "# –ï—Å–ª–∏ —É–∂–µ –Ω–∞—à–ª–∏ –≥–æ—Ç–æ–≤—ã–µ train/val –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö\n",
    "if train_manifest and val_manifest and os.path.exists(train_manifest) and os.path.exists(val_manifest):\n",
    "    print(\"\\n‚úì –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–µ train/val –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É\")\n",
    "    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π\n",
    "    with open(train_manifest, 'r', encoding='utf-8') as f:\n",
    "        train_count = sum(1 for line in f if line.strip())\n",
    "    with open(val_manifest, 'r', encoding='utf-8') as f:\n",
    "        val_count = sum(1 for line in f if line.strip())\n",
    "    print(f\"  Train: {train_count} –∑–∞–ø–∏—Å–µ–π\")\n",
    "    print(f\"  Val: {val_count} –∑–∞–ø–∏—Å–µ–π\")\n",
    "else:\n",
    "    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –°–û–í–ê –∏ –°–±–µ—Ä–ì–æ–ª–æ—Å, –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö\n",
    "    if sova_manifest_found and golos_manifest_found:\n",
    "        print(\"\\n‚úì –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –°–û–í–ê –∏ –°–±–µ—Ä–ì–æ–ª–æ—Å\")\n",
    "        sova_manifest = sova_manifest_found\n",
    "        sber_manifest = golos_manifest_found\n",
    "    else:\n",
    "        # –°–æ–∑–¥–∞–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –∑–∞–Ω–æ–≤–æ\n",
    "        print(\"\\n–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤ –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "        sova_manifest = os.path.join(MANIFEST_DIR, \"sova_manifest.json\")\n",
    "        sber_manifest = os.path.join(MANIFEST_DIR, \"sber_golos_manifest.json\")\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ª–∏ —É–∂–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã\n",
    "        if os.path.exists(sova_manifest) and os.path.exists(sber_manifest):\n",
    "            print(\"‚úì –ú–∞–Ω–∏—Ñ–µ—Å—Ç—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ OUTPUT_DIR, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö\")\n",
    "        else:\n",
    "            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã\n",
    "            print(\"–°–æ–∑–¥–∞–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤ –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "            \n",
    "            # –°–û–í–ê –¥–∞—Ç–∞—Å–µ—Ç\n",
    "            if os.path.exists(SOVA_PATH):\n",
    "                prepare_manifest_from_dataset(\n",
    "                    SOVA_PATH,\n",
    "                    sova_manifest,\n",
    "                    dataset_name=\"–°–û–í–ê\",\n",
    "                    max_files=10000  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
    "                )\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  –°–û–í–ê –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {SOVA_PATH}\")\n",
    "            \n",
    "            # –°–±–µ—Ä–ì–æ–ª–æ—Å –¥–∞—Ç–∞—Å–µ—Ç\n",
    "            if os.path.exists(SBER_GOLOS_PATH):\n",
    "                prepare_manifest_from_dataset(\n",
    "                    SBER_GOLOS_PATH,\n",
    "                    sber_manifest,\n",
    "                    dataset_name=\"–°–±–µ—Ä–ì–æ–ª–æ—Å\",\n",
    "                    max_files=10000  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
    "                )\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  –°–±–µ—Ä–ì–æ–ª–æ—Å –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {SBER_GOLOS_PATH}\")\n",
    "    \n",
    "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –∏ —Å–æ–∑–¥–∞–µ–º train/val\n",
    "    train_manifest = os.path.join(MANIFEST_DIR, \"train_manifest.json\")\n",
    "    val_manifest = os.path.join(MANIFEST_DIR, \"val_manifest.json\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å train/val –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã\n",
    "    if os.path.exists(train_manifest) and os.path.exists(val_manifest):\n",
    "        print(\"\\n‚úì Train/val –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö\")\n",
    "        with open(train_manifest, 'r', encoding='utf-8') as f:\n",
    "            train_count = sum(1 for line in f if line.strip())\n",
    "        with open(val_manifest, 'r', encoding='utf-8') as f:\n",
    "            val_count = sum(1 for line in f if line.strip())\n",
    "        print(f\"  Train: {train_count} –∑–∞–ø–∏—Å–µ–π\")\n",
    "        print(f\"  Val: {val_count} –∑–∞–ø–∏—Å–µ–π\")\n",
    "    elif 'sova_manifest' in locals() and os.path.exists(sova_manifest) and os.path.exists(sber_manifest):\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val\n",
    "        print(\"\\n–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ train/val...\")\n",
    "        all_data = []\n",
    "        \n",
    "        for manifest_path in [sova_manifest, sber_manifest]:\n",
    "            if os.path.exists(manifest_path):\n",
    "                with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            all_data.append(json.loads(line.strip()))\n",
    "                        except:\n",
    "                            continue\n",
    "        \n",
    "        if all_data:\n",
    "            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º\n",
    "            random.shuffle(all_data)\n",
    "            \n",
    "            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val (90/10)\n",
    "            split_idx = int(len(all_data) * 0.9)\n",
    "            train_data = all_data[:split_idx]\n",
    "            val_data = all_data[split_idx:]\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º\n",
    "            with open(train_manifest, 'w', encoding='utf-8') as f:\n",
    "                for item in train_data:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "            \n",
    "            with open(val_manifest, 'w', encoding='utf-8') as f:\n",
    "                for item in val_data:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "            \n",
    "            print(f\"\\n‚úì –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã —Å–æ–∑–¥–∞–Ω—ã:\")\n",
    "            print(f\"  Train: {len(train_data)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "            print(f\"  Val: {len(val_data)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω—ã –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è train/val\")\n",
    "        print(\"   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç –∏–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã\")\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞\n",
    "if not train_manifest or not os.path.exists(train_manifest):\n",
    "    train_manifest = os.path.join(MANIFEST_DIR, \"train_manifest.json\")\n",
    "if not val_manifest or not os.path.exists(val_manifest):\n",
    "    val_manifest = os.path.join(MANIFEST_DIR, \"val_manifest.json\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"–ò–¢–û–ì–û–í–´–ï –ü–£–¢–ò –ö –ú–ê–ù–ò–§–ï–°–¢–ê–ú\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Train –º–∞–Ω–∏—Ñ–µ—Å—Ç: {train_manifest}\")\n",
    "print(f\"Val –º–∞–Ω–∏—Ñ–µ—Å—Ç: {val_manifest}\")\n",
    "if os.path.exists(train_manifest) and os.path.exists(val_manifest):\n",
    "    print(\"‚úì –û–±–∞ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –∏ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  –û–¥–∏–Ω –∏–ª–∏ –æ–±–∞ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "tokenizer_path = os.path.join(TOKENIZER_DIR, \"custom_tokenizer.json\")\n",
    "\n",
    "if os.path.exists(tokenizer_path):\n",
    "    print(\"‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É–∂–µ –æ–±—É—á–µ–Ω, –∑–∞–≥—Ä—É–∂–∞–µ–º...\")\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "else:\n",
    "    print(\"–û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–≤\n",
    "    texts = load_texts_from_manifests([train_manifest, val_manifest])\n",
    "    print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\")\n",
    "    \n",
    "    # –û–±—É—á–∞–µ–º BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "    tokenizer = train_bpe_tokenizer(\n",
    "        texts,\n",
    "        vocab_size=TARGET_VOCAB_SIZE,\n",
    "        output_path=tokenizer_path\n",
    "    )\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è\n",
    "vocab = tokenizer.get_vocab()\n",
    "actual_vocab_size = len(vocab)\n",
    "print(f\"\\n–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {actual_vocab_size} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"–¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä: {TARGET_VOCAB_SIZE} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "\n",
    "if actual_vocab_size == TARGET_VOCAB_SIZE:\n",
    "    print(\"‚úì –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º (—Ä–∞–∑–Ω–∏—Ü–∞: {actual_vocab_size - TARGET_VOCAB_SIZE})\")\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "test_text = \"–ø—Ä–∏–≤–µ—Ç –∫–∞–∫ –¥–µ–ª–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded.ids)\n",
    "\n",
    "print(f\"\\n–¢–µ—Å—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:\")\n",
    "print(f\"  –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {test_text}\")\n",
    "print(f\"  –¢–æ–∫–µ–Ω—ã: {encoded.tokens}\")\n",
    "print(f\"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(encoded.ids)}\")\n",
    "print(f\"  –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ NeMo ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –∏–∑ TensorBoard (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ DZ2)\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "def plot_asr_training_metrics(log_dir):\n",
    "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è ASR –º–æ–¥–µ–ª–∏ –∏–∑ TensorBoard.\"\"\"\n",
    "    print(f\"–ü–æ–∏—Å–∫ –ª–æ–≥–æ–≤ TensorBoard...\")\n",
    "    print(f\"–û—Å–Ω–æ–≤–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {log_dir}\")\n",
    "    \n",
    "    # –°–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª–æ–≥–æ–≤\n",
    "    possible_log_dirs = [\n",
    "        log_dir,  # –ü—Ä—è–º–æ –≤ MODEL_DIR\n",
    "        os.path.join(log_dir, \"lightning_logs\"),  # MODEL_DIR/lightning_logs\n",
    "        os.path.join(OUTPUT_DIR, \"lightning_logs\"),  # OUTPUT_DIR/lightning_logs\n",
    "        \"/tf/lightning_logs\",  # –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è /tf\n",
    "        os.path.join(os.path.dirname(OUTPUT_DIR), \"lightning_logs\"),  # –†–æ–¥–∏—Ç–µ–ª—å—Å–∫–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è\n",
    "    ]\n",
    "    \n",
    "    version_dirs = []\n",
    "    found_location = None\n",
    "    \n",
    "    # –ò—â–µ–º version_* –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤–æ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö\n",
    "    for search_dir in possible_log_dirs:\n",
    "        if os.path.exists(search_dir):\n",
    "            print(f\"  –ü—Ä–æ–≤–µ—Ä—è–µ–º: {search_dir}\")\n",
    "            # –ò—â–µ–º version_* –Ω–∞–ø—Ä—è–º—É—é\n",
    "            found_versions = glob.glob(os.path.join(search_dir, \"version_*\"))\n",
    "            if found_versions:\n",
    "                version_dirs.extend(found_versions)\n",
    "                if not found_location:\n",
    "                    found_location = search_dir\n",
    "                    print(f\"  ‚úì –ù–∞–π–¥–µ–Ω—ã –ª–æ–≥–∏ –≤: {search_dir}\")\n",
    "            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "            for root, dirs, files in os.walk(search_dir):\n",
    "                for d in dirs:\n",
    "                    if d.startswith(\"version_\"):\n",
    "                        version_path = os.path.join(root, d)\n",
    "                        if version_path not in version_dirs:\n",
    "                            version_dirs.append(version_path)\n",
    "                            if not found_location:\n",
    "                                found_location = root\n",
    "                                print(f\"  ‚úì –ù–∞–π–¥–µ–Ω—ã –ª–æ–≥–∏ –≤: {root}\")\n",
    "    \n",
    "    if not version_dirs:\n",
    "        print(f\"\\n‚ö†Ô∏è  –õ–æ–≥–∏ TensorBoard –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∏ –≤ –æ–¥–Ω–æ–º –∏–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç:\")\n",
    "        for search_dir in possible_log_dirs:\n",
    "            exists = \"‚úì\" if os.path.exists(search_dir) else \"‚úó\"\n",
    "            print(f\"  {exists} {search_dir}\")\n",
    "        print(f\"\\nüí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\")\n",
    "        print(f\"  1. –û–±—É—á–µ–Ω–∏–µ –µ—â–µ –Ω–µ –Ω–∞—á–∞–ª–æ—Å—å\")\n",
    "        print(f\"  2. –û–±—É—á–µ–Ω–∏–µ –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ –ø–µ—Ä–≤—É—é —ç–ø–æ—Ö—É\")\n",
    "        print(f\"  3. –õ–æ–≥–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –¥—Ä—É–≥–æ–º –º–µ—Å—Ç–µ\")\n",
    "        print(f\"\\n   –ï—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ:\")\n",
    "        print(f\"   - {log_dir}\")\n",
    "        print(f\"   - {OUTPUT_DIR}\")\n",
    "        print(f\"   - /tf/lightning_logs\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n‚úì –ù–∞–π–¥–µ–Ω–æ {len(version_dirs)} –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å –ª–æ–≥–∞–º–∏\")\n",
    "    \n",
    "    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é (–ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏)\n",
    "    latest_version = max(version_dirs, key=os.path.getmtime)\n",
    "    print(f\"\\n–ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–≥–∏ –∏–∑: {latest_version}\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ event —Ñ–∞–π–ª–æ–≤\n",
    "    event_files = glob.glob(os.path.join(latest_version, \"events.out.tfevents.*\"))\n",
    "    if not event_files:\n",
    "        print(f\"‚ö†Ô∏è  –í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {latest_version} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã event —Ñ–∞–π–ª—ã TensorBoard\")\n",
    "        print(f\"   –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ—Å—å –∏–ª–∏ –µ—â–µ –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ –ø–µ—Ä–≤—É—é —ç–ø–æ—Ö—É.\")\n",
    "        return\n",
    "    print(f\"‚úì –ù–∞–π–¥–µ–Ω–æ {len(event_files)} event —Ñ–∞–π–ª–æ–≤\")\n",
    "    \n",
    "    try:\n",
    "        event_acc = EventAccumulator(latest_version)\n",
    "        event_acc.Reload()\n",
    "        \n",
    "        scalar_tags = event_acc.Tags().get('scalars', [])\n",
    "        print(f\"–ù–∞–π–¥–µ–Ω—ã —Ç–µ–≥–∏: {scalar_tags}\")\n",
    "        \n",
    "        if not scalar_tags:\n",
    "            print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\")\n",
    "            return\n",
    "        \n",
    "        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–µ–≥–∏\n",
    "        filtered_tags = [t for t in scalar_tags if 'hp_metric' not in t.lower() and 'hparams' not in t.lower()]\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è ASR\n",
    "        loss_tags = [tag for tag in filtered_tags if 'loss' in tag.lower()]\n",
    "        wer_tags = [tag for tag in filtered_tags if 'wer' in tag.lower()]\n",
    "        accuracy_tags = [tag for tag in filtered_tags if 'accuracy' in tag.lower() or 'acc' in tag.lower()]\n",
    "        lr_tags = [tag for tag in filtered_tags if 'lr' in tag.lower() or 'learning_rate' in tag.lower()]\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "        num_plots = sum([len(loss_tags) > 0, len(wer_tags) > 0, len(accuracy_tags) > 0, len(lr_tags) > 0])\n",
    "        \n",
    "        if num_plots == 0:\n",
    "            print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\")\n",
    "            return\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º subplot\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        plot_idx = 0\n",
    "        \n",
    "        # –ì—Ä–∞—Ñ–∏–∫ 1: Loss\n",
    "        if loss_tags:\n",
    "            ax = axes[plot_idx]\n",
    "            for tag in loss_tags:\n",
    "                try:\n",
    "                    data = event_acc.Scalars(tag)\n",
    "                    if data:\n",
    "                        label = tag.replace('_', ' ').title()\n",
    "                        ax.plot([s.step for s in data], [s.value for s in data], label=label)\n",
    "                except:\n",
    "                    continue\n",
    "            ax.set_xlabel('Step')\n",
    "            ax.set_ylabel('Loss')\n",
    "            ax.set_title('Training and Validation Loss')\n",
    "            if ax.get_legend_handles_labels()[0]:\n",
    "                ax.legend()\n",
    "            ax.grid(True)\n",
    "            plot_idx += 1\n",
    "        \n",
    "        # –ì—Ä–∞—Ñ–∏–∫ 2: WER\n",
    "        if wer_tags:\n",
    "            ax = axes[plot_idx]\n",
    "            for tag in wer_tags:\n",
    "                try:\n",
    "                    data = event_acc.Scalars(tag)\n",
    "                    if data:\n",
    "                        label = tag.replace('_', ' ').title()\n",
    "                        ax.plot([s.step for s in data], [s.value for s in data], label=label)\n",
    "                except:\n",
    "                    continue\n",
    "            ax.set_xlabel('Step')\n",
    "            ax.set_ylabel('WER')\n",
    "            ax.set_title('Word Error Rate')\n",
    "            if ax.get_legend_handles_labels()[0]:\n",
    "                ax.legend()\n",
    "            ax.grid(True)\n",
    "            plot_idx += 1\n",
    "        \n",
    "        # –ì—Ä–∞—Ñ–∏–∫ 3: Accuracy\n",
    "        if accuracy_tags:\n",
    "            ax = axes[plot_idx]\n",
    "            for tag in accuracy_tags:\n",
    "                try:\n",
    "                    data = event_acc.Scalars(tag)\n",
    "                    if data:\n",
    "                        label = tag.replace('_', ' ').title()\n",
    "                        ax.plot([s.step for s in data], [s.value for s in data], label=label)\n",
    "                except:\n",
    "                    continue\n",
    "            ax.set_xlabel('Step')\n",
    "            ax.set_ylabel('Accuracy')\n",
    "            ax.set_title('Accuracy')\n",
    "            if ax.get_legend_handles_labels()[0]:\n",
    "                ax.legend()\n",
    "            ax.grid(True)\n",
    "            plot_idx += 1\n",
    "        \n",
    "        # –ì—Ä–∞—Ñ–∏–∫ 4: Learning Rate\n",
    "        if lr_tags:\n",
    "            ax = axes[plot_idx]\n",
    "            for tag in lr_tags:\n",
    "                try:\n",
    "                    data = event_acc.Scalars(tag)\n",
    "                    if data:\n",
    "                        label = tag.replace('_', ' ').title()\n",
    "                        ax.plot([s.step for s in data], [s.value for s in data], label=label)\n",
    "                except:\n",
    "                    continue\n",
    "            ax.set_xlabel('Step')\n",
    "            ax.set_ylabel('LR')\n",
    "            ax.set_title('Learning Rate')\n",
    "            if ax.get_legend_handles_labels()[0]:\n",
    "                ax.legend()\n",
    "            ax.grid(True)\n",
    "            plot_idx += 1\n",
    "        \n",
    "        # –°–∫—Ä—ã–≤–∞–µ–º –Ω–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ subplot\n",
    "        for i in range(plot_idx, len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, 'asr_training_metrics.png'), dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–µ—Ç—Ä–∏–∫: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "print(\"=\"*60)\n",
    "print(\"–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ú–ï–¢–†–ò–ö –û–ë–£–ß–ï–ù–ò–Ø\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ï—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –µ—â–µ –Ω–µ –Ω–∞—á–∞–ª–æ—Å—å –∏–ª–∏ –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ –ø–µ—Ä–≤—É—é —ç–ø–æ—Ö—É,\")\n",
    "print(\"   –ª–æ–≥–∏ TensorBoard –º–æ–≥—É—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å–Ω–∞—á–∞–ª–∞.\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "plot_asr_training_metrics(MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞\n",
    "print(\"=\"*60)\n",
    "print(\"–ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checkpoints = glob.glob(os.path.join(MODEL_DIR, \"checkpoints\", \"*.ckpt\"))\n",
    "\n",
    "if checkpoints:\n",
    "    best_checkpoint = max(checkpoints, key=os.path.getmtime)\n",
    "    print(f\"\\n‚úì –ù–∞–π–¥–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {best_checkpoint}\")\n",
    "    print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞...\")\n",
    "    \n",
    "    try:\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞\n",
    "        asr_model = nemo_asr.models.EncDecCTCModel.load_from_checkpoint(best_checkpoint)\n",
    "        asr_model.eval()\n",
    "        print(\"‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}\")\n",
    "        print(\"–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –æ–Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  –ß–µ–∫–ø–æ–∏–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n",
    "    print(\"–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –æ–Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞)\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n",
    "if 'asr_model' not in globals() or asr_model is None:\n",
    "    print(\"\\n‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!\")\n",
    "    print(\"   –ó–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫—É '–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏' –∏–ª–∏ '–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏' —Å–Ω–∞—á–∞–ª–∞\")\n",
    "    raise RuntimeError(\"–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º.\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏\n",
    "def transcribe_audio(audio_path: str, verbose: bool = False) -> str:\n",
    "    \"\"\"–†–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ä–µ—á—å –∏–∑ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞.\"\"\"\n",
    "    if asr_model is None:\n",
    "        if verbose:\n",
    "            print(f\"  ‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "        return \"\"\n",
    "    \n",
    "    if not os.path.exists(audio_path):\n",
    "        if verbose:\n",
    "            print(f\"  ‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}\")\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        if verbose:\n",
    "            file_size = os.path.getsize(audio_path) / 1024  # KB\n",
    "            print(f\"  üìÅ –§–∞–π–ª: {os.path.basename(audio_path)} ({file_size:.1f} KB)\")\n",
    "            print(f\"  üîÑ –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ...\")\n",
    "        \n",
    "        transcription = asr_model.transcribe([audio_path])\n",
    "        \n",
    "        result = transcription[0] if transcription and len(transcription) > 0 else \"\"\n",
    "        \n",
    "        if verbose:\n",
    "            if result:\n",
    "                print(f\"  ‚úì –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {result[:50]}...\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\")\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"  ‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        return \"\"\n",
    "\n",
    "print(\"\\n‚úì –§—É–Ω–∫—Ü–∏—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞\")\n",
    "print(f\"‚úì –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "from jiwer import wer, cer\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def fix_audio_paths_in_manifest(manifest_path: str, search_dirs: List[str] = None) -> int:\n",
    "    \"\"\"–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø—É—Ç–∏ –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞–º –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ–≤–µ—Ä–Ω—ã–µ.\"\"\"\n",
    "    if search_dirs is None:\n",
    "        search_dirs = [\n",
    "            OUTPUT_DIR,\n",
    "            \"/tf/nspeganov/sova_data\",\n",
    "            \"/tf/nspeganov/sber_golos_data\",\n",
    "            \"/share/audio_data/sova\",\n",
    "            \"/share/audio_data/sber-golos\",\n",
    "        ]\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç\n",
    "    manifest_data = []\n",
    "    with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                manifest_data.append(json.loads(line.strip()))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    fixed_count = 0\n",
    "    \n",
    "    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—É—Ç–∏\n",
    "    for item in manifest_data:\n",
    "        audio_path = item.get('audio_filepath', '')\n",
    "        if not audio_path or not os.path.exists(audio_path):\n",
    "            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª –ø–æ –∏–º–µ–Ω–∏\n",
    "            basename = os.path.basename(audio_path)\n",
    "            if basename:\n",
    "                for search_dir in search_dirs:\n",
    "                    if os.path.exists(search_dir):\n",
    "                        found = glob.glob(os.path.join(search_dir, \"**\", basename), recursive=True)\n",
    "                        if found:\n",
    "                            item['audio_filepath'] = os.path.abspath(found[0])\n",
    "                            fixed_count += 1\n",
    "                            break\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –º–∞–Ω–∏—Ñ–µ—Å—Ç\n",
    "    if fixed_count > 0:\n",
    "        backup_path = manifest_path + \".backup\"\n",
    "        if not os.path.exists(backup_path):\n",
    "            import shutil\n",
    "            shutil.copy2(manifest_path, backup_path)\n",
    "        \n",
    "        with open(manifest_path, 'w', encoding='utf-8') as f:\n",
    "            for item in manifest_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"‚úì –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {fixed_count} –ø—É—Ç–µ–π –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ\")\n",
    "        print(f\"  –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {backup_path}\")\n",
    "    \n",
    "    return fixed_count\n",
    "\n",
    "def evaluate_model(manifest_path: str, num_examples: int = 10):\n",
    "    \"\"\"–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –∏–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞.\"\"\"\n",
    "    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ\n",
    "    print(\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Ç–µ–π –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ...\")\n",
    "    fixed = fix_audio_paths_in_manifest(manifest_path)\n",
    "    if fixed > 0:\n",
    "        print(f\"‚úì –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {fixed} –ø—É—Ç–µ–π\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "    test_data = []\n",
    "    with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                test_data.append(json.loads(line.strip()))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if not test_data:\n",
    "        print(\"‚ö†Ô∏è  –ú–∞–Ω–∏—Ñ–µ—Å—Ç –ø—É—Å—Ç –∏–ª–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—á–∏—Ç–∞–Ω\")\n",
    "        return [], []\n",
    "    \n",
    "    # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã\n",
    "    sample_data = random.sample(test_data, min(num_examples, len(test_data)))\n",
    "    \n",
    "    wers = []\n",
    "    cers = []\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –ù–ê {len(sample_data)} –ü–†–ò–ú–ï–†–ê–•\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—É—Ç–µ–π –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º\n",
    "    print(\"üìã –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—É—Ç–µ–π –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞–º (–ø–µ—Ä–≤—ã–µ 3 –ø—Ä–∏–º–µ—Ä–∞):\")\n",
    "    for i, item in enumerate(sample_data[:3]):\n",
    "        audio_path = item.get('audio_filepath', '')\n",
    "        print(f\"  –ü—Ä–∏–º–µ—Ä {i+1}:\")\n",
    "        print(f\"    –ü—É—Ç—å –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ: {audio_path}\")\n",
    "        print(f\"    –°—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(audio_path) if audio_path else False}\")\n",
    "        if audio_path and not os.path.exists(audio_path):\n",
    "            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª\n",
    "            basename = os.path.basename(audio_path)\n",
    "            print(f\"    –ò–º—è —Ñ–∞–π–ª–∞: {basename}\")\n",
    "            # –ò—â–µ–º –≤ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö\n",
    "            possible_dirs = [\n",
    "                os.path.dirname(audio_path),\n",
    "                OUTPUT_DIR,\n",
    "                \"/tf/nspeganov/sova_data\",\n",
    "                \"/tf/nspeganov/sber_golos_data\",\n",
    "                \"/share/audio_data/sova\",\n",
    "                \"/share/audio_data/sber-golos\",\n",
    "            ]\n",
    "            for search_dir in possible_dirs:\n",
    "                if os.path.exists(search_dir):\n",
    "                    found = glob.glob(os.path.join(search_dir, \"**\", basename), recursive=True)\n",
    "                    if found:\n",
    "                        print(f\"    ‚úì –ù–∞–π–¥–µ–Ω –≤: {found[0]}\")\n",
    "                        break\n",
    "    \n",
    "    print(f\"\\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ...\\n\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏\n",
    "    stats = {\n",
    "        'processed': 0,\n",
    "        'found': 0,\n",
    "        'transcribed': 0,\n",
    "        'errors': 0,\n",
    "    }\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π\n",
    "    pbar = tqdm(sample_data, desc=\"–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ\", unit=\"—Ñ–∞–π–ª\")\n",
    "    \n",
    "    for i, item in enumerate(pbar):\n",
    "        step_start = time.time()\n",
    "        audio_path = item.get('audio_filepath', '')\n",
    "        true_text = item.get('text', '')\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞\n",
    "        pbar.set_postfix({\n",
    "            '–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ': stats['processed'],\n",
    "            '–ù–∞–π–¥–µ–Ω–æ': stats['found'],\n",
    "            '–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ': stats['transcribed'],\n",
    "            '–û—à–∏–±–æ–∫': stats['errors']\n",
    "        })\n",
    "        \n",
    "        stats['processed'] += 1\n",
    "        \n",
    "        if not audio_path:\n",
    "            stats['errors'] += 1\n",
    "            if i < 3:\n",
    "                pbar.write(f\"‚ö†Ô∏è  [{i+1}/{len(sample_data)}] –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É\")\n",
    "            continue\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞\n",
    "        if not os.path.exists(audio_path):\n",
    "            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª –ø–æ –∏–º–µ–Ω–∏\n",
    "            basename = os.path.basename(audio_path)\n",
    "            if basename:\n",
    "                # –ò—â–µ–º –≤ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö\n",
    "                found_files = []\n",
    "                search_dirs = [\n",
    "                    OUTPUT_DIR,\n",
    "                    \"/tf/nspeganov/sova_data\",\n",
    "                    \"/tf/nspeganov/sber_golos_data\",\n",
    "                    \"/share/audio_data/sova\",\n",
    "                    \"/share/audio_data/sber-golos\",\n",
    "                ]\n",
    "                for search_dir in search_dirs:\n",
    "                    if os.path.exists(search_dir):\n",
    "                        found = glob.glob(os.path.join(search_dir, \"**\", basename), recursive=True)\n",
    "                        if found:\n",
    "                            found_files.extend(found)\n",
    "                            break\n",
    "                \n",
    "                if found_files:\n",
    "                    audio_path = found_files[0]\n",
    "                    stats['found'] += 1\n",
    "                    if i < 3:\n",
    "                        pbar.write(f\"‚úì [{i+1}/{len(sample_data)}] –§–∞–π–ª –Ω–∞–π–¥–µ–Ω –ø–æ –∏–º–µ–Ω–∏: {os.path.basename(audio_path)}\")\n",
    "                else:\n",
    "                    stats['errors'] += 1\n",
    "                    if i < 3:\n",
    "                        pbar.write(f\"‚úó [{i+1}/{len(sample_data)}] –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {basename}\")\n",
    "                    continue\n",
    "            else:\n",
    "                stats['errors'] += 1\n",
    "                if i < 3:\n",
    "                    pbar.write(f\"‚úó [{i+1}/{len(sample_data)}] –ù–µ–≤–µ—Ä–Ω—ã–π –ø—É—Ç—å: {audio_path[:50]}...\")\n",
    "                continue\n",
    "        else:\n",
    "            stats['found'] += 1\n",
    "        \n",
    "        # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º\n",
    "        transcribe_start = time.time()\n",
    "        # –í–∫–ª—é—á–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–µ –ª–æ–≥–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "        verbose_log = (i < 3)\n",
    "        if verbose_log:\n",
    "            pbar.write(f\"  üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {i+1}/{len(sample_data)}: {os.path.basename(audio_path)}\")\n",
    "        predicted_text = transcribe_audio(audio_path, verbose=verbose_log)\n",
    "        transcribe_time = time.time() - transcribe_start\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "        if true_text and predicted_text:\n",
    "            w = wer(true_text, predicted_text)\n",
    "            c = cer(true_text, predicted_text)\n",
    "            wers.append(w)\n",
    "            cers.append(c)\n",
    "            stats['transcribed'] += 1\n",
    "            \n",
    "            # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "            if i < 5:\n",
    "                pbar.write(f\"\\nüìù –ü—Ä–∏–º–µ—Ä {i+1}/{len(sample_data)}:\")\n",
    "                pbar.write(f\"  –ò—Å—Ç–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {true_text[:60]}...\")\n",
    "                pbar.write(f\"  –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π: {predicted_text[:60]}...\")\n",
    "                pbar.write(f\"  WER: {w:.3f}, CER: {c:.3f} (–≤—Ä–µ–º—è: {transcribe_time:.2f}—Å)\")\n",
    "                \n",
    "                # –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∞—É–¥–∏–æ\n",
    "                try:\n",
    "                    display(Audio(audio_path, autoplay=False))\n",
    "                except:\n",
    "                    pass\n",
    "        elif not predicted_text:\n",
    "            stats['errors'] += 1\n",
    "            if i < 3:\n",
    "                pbar.write(f\"‚ö†Ô∏è  [{i+1}/{len(sample_data)}] –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∂–¥—ã–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "        if (i + 1) % 5 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time = elapsed / (i + 1)\n",
    "            remaining = avg_time * (len(sample_data) - i - 1)\n",
    "            pbar.write(f\"‚è±Ô∏è  –ü—Ä–æ–≥—Ä–µ—Å—Å: {i+1}/{len(sample_data)} | \"\n",
    "                      f\"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.2f}—Å/—Ñ–∞–π–ª | \"\n",
    "                      f\"–û—Å—Ç–∞–ª–æ—Å—å: ~{remaining:.0f}—Å\")\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.1f} —Å–µ–∫—É–Ω–¥\")\n",
    "    print(f\"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {stats['processed']}, –Ω–∞–π–¥–µ–Ω–æ {stats['found']}, \"\n",
    "          f\"—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ {stats['transcribed']}, –æ—à–∏–±–æ–∫ {stats['errors']}\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    if wers:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"–°—Ä–µ–¥–Ω–∏–π WER: {np.mean(wers):.3f} ¬± {np.std(wers):.3f}\")\n",
    "        print(f\"–ú–µ–¥–∏–∞–Ω–Ω—ã–π WER: {np.median(wers):.3f}\")\n",
    "        print(f\"–°—Ä–µ–¥–Ω–∏–π CER: {np.mean(cers):.3f} ¬± {np.std(cers):.3f}\")\n",
    "        print(f\"–ú–µ–¥–∏–∞–Ω–Ω—ã–π CER: {np.median(cers):.3f}\")\n",
    "        print(f\"\\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ: {len(wers)}/{len(sample_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏\")\n",
    "        print(\"\\n–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\")\n",
    "        print(\"  1. –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç\")\n",
    "        print(\"  2. –§—É–Ω–∫—Ü–∏—è transcribe_audio –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\")\n",
    "        print(\"  3. –ê—É–¥–∏–æ—Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã\")\n",
    "        print(\"  4. –¢–µ–∫—Å—Ç—ã –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ –ø—É—Å—Ç—ã–µ\")\n",
    "        \n",
    "        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞\n",
    "        print(\"\\n–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:\")\n",
    "        print(f\"  - –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {'asr_model' in globals() and asr_model is not None}\")\n",
    "        if 'asr_model' in globals() and asr_model is not None:\n",
    "            print(f\"  - –¢–∏–ø –º–æ–¥–µ–ª–∏: {type(asr_model)}\")\n",
    "        print(f\"  - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(sample_data)}\")\n",
    "        found_files = sum(1 for item in sample_data if os.path.exists(item.get('audio_filepath', '')))\n",
    "        print(f\"  - –ù–∞–π–¥–µ–Ω–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤: {found_files}/{len(sample_data)}\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –ø—É—Ç–µ–π\n",
    "        print(f\"\\n–ü—Ä–∏–º–µ—Ä—ã –ø—É—Ç–µ–π –∏–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞:\")\n",
    "        for i, item in enumerate(sample_data[:5]):\n",
    "            audio_path = item.get('audio_filepath', '')\n",
    "            exists = os.path.exists(audio_path) if audio_path else False\n",
    "            status = \"‚úì\" if exists else \"‚úó\"\n",
    "            print(f\"  {status} {i+1}: {audio_path[:80]}...\")\n",
    "            if not exists and audio_path:\n",
    "                basename = os.path.basename(audio_path)\n",
    "                print(f\"      –ò–º—è —Ñ–∞–π–ª–∞: {basename}\")\n",
    "        \n",
    "        # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\n",
    "        if sample_data:\n",
    "            test_item = sample_data[0]\n",
    "            test_audio = test_item.get('audio_filepath', '')\n",
    "            if test_audio:\n",
    "                if os.path.exists(test_audio):\n",
    "                    print(f\"\\n–¢–µ—Å—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω–∞ –ø–µ—Ä–≤–æ–º —Ñ–∞–π–ª–µ: {test_audio}\")\n",
    "                    test_result = transcribe_audio(test_audio)\n",
    "                    print(f\"  –†–µ–∑—É–ª—å—Ç–∞—Ç: '{test_result}'\")\n",
    "                    print(f\"  –î–ª–∏–Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {len(test_result)}\")\n",
    "                    if not test_result:\n",
    "                        print(\"  ‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—É—Å—Ç–æ–π - –º–æ–¥–µ–ª—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç –∞—É–¥–∏–æ\")\n",
    "                else:\n",
    "                    print(f\"\\n‚ö†Ô∏è  –ü–µ—Ä–≤—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {test_audio}\")\n",
    "                    print(f\"   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –ø—É—Ç–∏ –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö\")\n",
    "    \n",
    "    return wers, cers\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "wers, cers = evaluate_model(val_manifest, num_examples=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ .nemo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ .nemo\n",
    "final_model_path = os.path.join(MODEL_DIR, \"final_asr_model.nemo\")\n",
    "\n",
    "try:\n",
    "    asr_model.save_to(final_model_path)\n",
    "    print(f\"‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}\")\n",
    "    print(f\"  –†–∞–∑–º–µ—Ä: {os.path.getsize(final_model_path) / (1024*1024):.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"–§–ê–ô–õ–´ –î–õ–Ø –°–î–ê–ß–ò –ó–ê–î–ê–ù–ò–Ø\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nüì¶ –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–ª–æ–∂–∏—Ç—å:\")\n",
    "print(f\"\\n1. –ú–æ–¥–µ–ª—å (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ):\")\n",
    "print(f\"   {final_model_path}\")\n",
    "print(f\"\\n2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä:\")\n",
    "print(f\"   {tokenizer_path}\")\n",
    "print(f\"\\n3. –ú–∞–Ω–∏—Ñ–µ—Å—Ç—ã:\")\n",
    "print(f\"   {train_manifest}\")\n",
    "print(f\"   {val_manifest}\")\n",
    "print(f\"\\n4. –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "print(f\"   {os.path.join(OUTPUT_DIR, 'asr_training_metrics.png')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"–ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò ASR\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\nüìä –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\")\n",
    "print(f\"- –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {actual_vocab_size} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"- –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä: {TARGET_VOCAB_SIZE} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "if 'wers' in globals() and wers:\n",
    "    print(f\"- –°—Ä–µ–¥–Ω–∏–π WER: {np.mean(wers):.3f}\")\n",
    "    print(f\"- –°—Ä–µ–¥–Ω–∏–π CER: {np.mean(cers):.3f}\")\n",
    "\n",
    "print(\"\\nüí° –í—ã–≤–æ–¥—ã:\")\n",
    "print(\"–ú–æ–¥–µ–ª—å ASR —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –°–û–í–ê –∏ –°–±–µ—Ä–ì–æ–ª–æ—Å.\")\n",
    "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∑–∞–¥–∞–Ω–∏—è (369 —Ç–æ–∫–µ–Ω–æ–≤).\")\n",
    "print(\"–ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä—É—Å—Å–∫–æ–π —Ä–µ—á–∏.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ NeMo ASR –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ\n",
    "print(\"=\"*60)\n",
    "print(\"–ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò NeMo ASR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏\n",
    "print(\"\\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π NeMo...\")\n",
    "try:\n",
    "    available_models = nemo_asr.models.EncDecCTCModel.list_available_models()\n",
    "    print(f\"‚úì –ù–∞–π–¥–µ–Ω–æ {len(available_models)} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\")\n",
    "    \n",
    "    # –ò—â–µ–º —Ä—É—Å—Å–∫–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    ru_models = [m for m in available_models if 'ru' in m.lower() or 'russian' in m.lower()]\n",
    "    if ru_models:\n",
    "        print(f\"\\n–†—É—Å—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ ({len(ru_models)}):\")\n",
    "        for model in ru_models[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10\n",
    "            print(f\"  - {model}\")\n",
    "        if len(ru_models) > 10:\n",
    "            print(f\"  ... –∏ –µ—â–µ {len(ru_models) - 10} –º–æ–¥–µ–ª–µ–π\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  –†—É—Å—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ:\")\n",
    "        for model in available_models[:20]:\n",
    "            print(f\"  - {model}\")\n",
    "        if len(available_models) > 20:\n",
    "            print(f\"  ... –∏ –µ—â–µ {len(available_models) - 20} –º–æ–¥–µ–ª–µ–π\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: {e}\")\n",
    "\n",
    "# –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å (–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"–ü–û–ü–´–¢–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ü–†–ï–î–û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_names_to_try = [\n",
    "    \"stt_ru_conformer_ctc_large\",\n",
    "    \"stt_ru_conformer_ctc_medium\",\n",
    "    \"stt_ru_conformer_ctc_small\",\n",
    "    \"stt_ru_quartznet15x5\",\n",
    "    \"stt_ru_quartznet15x5_ctc\",\n",
    "    \"nvidia/stt_ru_conformer_ctc_large\",\n",
    "    \"nvidia/stt_ru_quartznet15x5\",\n",
    "]\n",
    "\n",
    "asr_model = None\n",
    "loaded_model_name = None\n",
    "\n",
    "for model_name in model_names_to_try:\n",
    "    try:\n",
    "        print(f\"\\n–ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å: {model_name}\")\n",
    "        asr_model = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=model_name)\n",
    "        loaded_model_name = model_name\n",
    "        print(f\"‚úì –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó –ù–µ —É–¥–∞–ª–æ—Å—å: {str(e)[:100]}...\")\n",
    "        continue\n",
    "\n",
    "# –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "if asr_model is None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ö†Ô∏è  –ù–ï –£–î–ê–õ–û–°–¨ –ó–ê–ì–†–£–ó–ò–¢–¨ –ü–†–ï–î–û–ë–£–ß–ï–ù–ù–£–Æ –ú–û–î–ï–õ–¨\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n–í–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã:\")\n",
    "    print(\"1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ —Å–ø–∏—Å–∫–∞ –≤—ã—à–µ\")\n",
    "    print(\"2. –°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è (—Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ)\")\n",
    "    print(\"3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π\")\n",
    "    print(\"\\n–ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è...\")\n",
    "    \n",
    "    try:\n",
    "        from omegaconf import OmegaConf\n",
    "        asr_config = {\n",
    "            \"model\": {\n",
    "                \"sample_rate\": SAMPLE_RATE,\n",
    "                \"train_ds\": {\n",
    "                    \"manifest_filepath\": train_manifest,\n",
    "                    \"sample_rate\": SAMPLE_RATE,\n",
    "                    \"batch_size\": 16,\n",
    "                    \"shuffle\": True,\n",
    "                    \"num_workers\": 4,\n",
    "                },\n",
    "                \"validation_ds\": {\n",
    "                    \"manifest_filepath\": val_manifest,\n",
    "                    \"sample_rate\": SAMPLE_RATE,\n",
    "                    \"batch_size\": 16,\n",
    "                    \"shuffle\": False,\n",
    "                    \"num_workers\": 4,\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "        asr_model = nemo_asr.models.EncDecCTCModel(cfg=OmegaConf.create(asr_config[\"model\"]))\n",
    "        print(\"‚úì –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º\n",
    "if asr_model:\n",
    "    asr_model._cfg.train_ds.manifest_filepath = train_manifest\n",
    "    asr_model._cfg.validation_ds.manifest_filepath = val_manifest\n",
    "    print(\"\\n‚úì –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã\")\n",
    "    print(f\"  Train: {train_manifest}\")\n",
    "    print(f\"  Val: {val_manifest}\")\n",
    "    print(\"\\n‚ö†Ô∏è  NeMo —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.\")\n",
    "    print(\"   –ù–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "print(\"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...\")\n",
    "print(f\"Train –º–∞–Ω–∏—Ñ–µ—Å—Ç: {train_manifest}\")\n",
    "print(f\"Val –º–∞–Ω–∏—Ñ–µ—Å—Ç: {val_manifest}\")\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(MODEL_DIR, \"checkpoints\"),\n",
    "    filename=\"asr-{epoch:02d}-{val_loss:.2f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=3,\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=16 if torch.cuda.is_available() else 32,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=50,\n",
    "    default_root_dir=MODEL_DIR\n",
    ")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "try:\n",
    "    trainer.fit(asr_model)\n",
    "    print(\"\\n‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
